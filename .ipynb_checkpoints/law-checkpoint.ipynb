{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63d589df",
   "metadata": {},
   "source": [
    "# SET UP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "17c0cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "import argparse\n",
    "import numpy as np\n",
    "import optuna\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from catboost.metrics import RMSEWithUncertainty\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover, LFR, OptimPreproc, Reweighing\n",
    "from imblearn.over_sampling import ADASYN\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97fb517b",
   "metadata": {},
   "outputs": [],
   "source": [
    "privileged_groups = [{'gender': 1}]\n",
    "unprivileged_groups = [{'gender': 0}]\n",
    "\n",
    "df = pd.read_csv(\"./data/law_preprocessed.csv\")\n",
    "\n",
    "# Create a BinaryLabelDataset using the binary labels (gpa_class) and relevant attributes\n",
    "dataset = BinaryLabelDataset(\n",
    "    favorable_label=1,  # 1 indicates \"high GPA\" (favorable outcome)\n",
    "    unfavorable_label=0,  # 0 indicates \"low GPA\" (unfavorable outcome)\n",
    "    df=df,\n",
    "    label_names=['pass_bar'],  # The newly created binary label\n",
    "    protected_attribute_names=['gender']  # The protected attribute (e.g., gender)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "55b1cd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset.features\n",
    "y = dataset.labels.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8adca2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature_names = dataset.feature_names\n",
    "# race_column_index = feature_names.index('race')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1af08a1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "X_val, X_test_us, y_val, y_test_us = train_test_split(X_test, y_test, test_size=0.2, random_state=42, stratify=y_test)\n",
    "X_test = np.concatenate((X_train, X_test_us), axis=0)\n",
    "y_test = np.concatenate((y_train, y_test_us), axis=0)\n",
    "membership = [\"in\"] * X_train.shape[0] + [\"out\"] * X_test_us.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "78026e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test_dataset = np.concatenate((X_test, y_test.reshape(-1,1)), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7b9d2ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# protected_test = test_dataset[:, race_column_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f30f2f38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the test dataset to recreate the BinaryLabelDataset\n",
    "df_test = pd.DataFrame(X_test, columns=dataset.feature_names)\n",
    "df_test['pass_bar'] = y_test  # Add the label\n",
    "# df_test['race'] = protected_test  # Add the protected attribute\n",
    "\n",
    "# Recreate the BinaryLabelDataset for the test set\n",
    "test_dataset = BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=df_test,\n",
    "    label_names=['pass_bar'],  # Label column name\n",
    "    protected_attribute_names=['gender']  # Protected attribute column name\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d7c9e83",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3439f66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optuna objective\n",
    "def objective(trial):\n",
    "    param = {\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1, log=True),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1, 10, log=True),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 1, 1000, log=True),\n",
    "    }\n",
    "\n",
    "    param[\"thread_count\"] = 4\n",
    "    param[\"random_seed\"] = 42\n",
    "\n",
    "    _X_train, _X_valid, _y_train, _y_valid = train_test_split(\n",
    "        X_train, y_train, test_size=0.2, stratify=y_train, random_state=np.random.randint(0, 1000)\n",
    "    )\n",
    "\n",
    "    clf = CatBoostClassifier(**param)\n",
    "    clf.fit(_X_train, _y_train, verbose=0)\n",
    "    _y_pred_test = clf.predict(_X_valid, prediction_type=\"Probability\")[:, 1]\n",
    "    score = roc_auc_score(_y_valid, _y_pred_test)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f23d4743",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 17:49:19,367] A new study created in memory with name: no-name-a43b8a1a-7b7e-4f42-a723-d1dfde5f0325\n",
      "[I 2024-10-24 17:49:19,690] Trial 0 finished with value: 0.8374922142634693 and parameters: {'depth': 10, 'l2_leaf_reg': 1.0964813663024324, 'learning_rate': 0.03893810788124229, 'random_strength': 1.4094998897031572, 'objective': 'CrossEntropy', 'iterations': 2}. Best is trial 0 with value: 0.8374922142634693.\n",
      "[I 2024-10-24 17:49:19,834] Trial 1 finished with value: 0.784575969324198 and parameters: {'depth': 4, 'l2_leaf_reg': 6.204742984539694, 'learning_rate': 0.030770977031366394, 'random_strength': 5.145583066235087, 'objective': 'CrossEntropy', 'iterations': 3}. Best is trial 0 with value: 0.8374922142634693.\n",
      "[I 2024-10-24 17:49:19,996] Trial 2 finished with value: 0.8275872975708503 and parameters: {'depth': 5, 'l2_leaf_reg': 1.8847431443926963, 'learning_rate': 0.04025679682769234, 'random_strength': 1.9037562853120868, 'objective': 'Logloss', 'iterations': 12}. Best is trial 0 with value: 0.8374922142634693.\n",
      "[I 2024-10-24 17:49:20,373] Trial 3 finished with value: 0.8376576611647463 and parameters: {'depth': 1, 'l2_leaf_reg': 3.681125842755607, 'learning_rate': 0.0690879244627939, 'random_strength': 1.0118971083427288, 'objective': 'Logloss', 'iterations': 152}. Best is trial 3 with value: 0.8376576611647463.\n",
      "[I 2024-10-24 17:49:20,554] Trial 4 finished with value: 0.851268101837434 and parameters: {'depth': 3, 'l2_leaf_reg': 1.9678657150720327, 'learning_rate': 0.2514562807042199, 'random_strength': 2.6998594886544165, 'objective': 'CrossEntropy', 'iterations': 4}. Best is trial 4 with value: 0.851268101837434.\n",
      "[I 2024-10-24 17:49:20,897] Trial 5 finished with value: 0.8495941684833386 and parameters: {'depth': 5, 'l2_leaf_reg': 3.0106036603689907, 'learning_rate': 0.014190024230762285, 'random_strength': 2.6963926660103197, 'objective': 'Logloss', 'iterations': 66}. Best is trial 4 with value: 0.851268101837434.\n",
      "[I 2024-10-24 17:49:21,227] Trial 6 finished with value: 0.8629637379321083 and parameters: {'depth': 5, 'l2_leaf_reg': 4.929859102498138, 'learning_rate': 0.9282524686347446, 'random_strength': 3.3685652554705667, 'objective': 'Logloss', 'iterations': 45}. Best is trial 6 with value: 0.8629637379321083.\n",
      "[I 2024-10-24 17:49:21,382] Trial 7 finished with value: 0.8562631384303956 and parameters: {'depth': 7, 'l2_leaf_reg': 1.4262977216081154, 'learning_rate': 0.011137921705413783, 'random_strength': 9.346717304553925, 'objective': 'Logloss', 'iterations': 11}. Best is trial 6 with value: 0.8629637379321083.\n",
      "[I 2024-10-24 17:49:21,689] Trial 8 finished with value: 0.8605963874182497 and parameters: {'depth': 9, 'l2_leaf_reg': 9.560350796983052, 'learning_rate': 0.031898344122611846, 'random_strength': 1.9918561796922236, 'objective': 'CrossEntropy', 'iterations': 31}. Best is trial 6 with value: 0.8629637379321083.\n",
      "[I 2024-10-24 17:49:21,814] Trial 9 finished with value: 0.8125218973839925 and parameters: {'depth': 8, 'l2_leaf_reg': 1.8836204819256048, 'learning_rate': 0.07869961983754052, 'random_strength': 2.6821059681847523, 'objective': 'CrossEntropy', 'iterations': 2}. Best is trial 6 with value: 0.8629637379321083.\n",
      "[I 2024-10-24 17:49:24,549] Trial 10 finished with value: 0.8341005527872937 and parameters: {'depth': 2, 'l2_leaf_reg': 4.8485912604934756, 'learning_rate': 0.8425202498110163, 'random_strength': 4.79123586429205, 'objective': 'Logloss', 'iterations': 948}. Best is trial 6 with value: 0.8629637379321083.\n",
      "[I 2024-10-24 17:49:25,166] Trial 11 finished with value: 0.8695329531298662 and parameters: {'depth': 10, 'l2_leaf_reg': 9.239771159296811, 'learning_rate': 0.26389717820064035, 'random_strength': 4.116166387516645, 'objective': 'CrossEntropy', 'iterations': 60}. Best is trial 11 with value: 0.8695329531298662.\n",
      "[I 2024-10-24 17:49:25,872] Trial 12 finished with value: 0.8567278495795703 and parameters: {'depth': 7, 'l2_leaf_reg': 9.8561176908307, 'learning_rate': 0.8992663713511676, 'random_strength': 4.298524057923545, 'objective': 'Logloss', 'iterations': 145}. Best is trial 11 with value: 0.8695329531298662.\n",
      "[I 2024-10-24 17:49:28,379] Trial 13 finished with value: 0.8202176113360322 and parameters: {'depth': 6, 'l2_leaf_reg': 6.593615911232702, 'learning_rate': 0.3076932894461032, 'random_strength': 7.258854308675536, 'objective': 'CrossEntropy', 'iterations': 565}. Best is trial 11 with value: 0.8695329531298662.\n",
      "[I 2024-10-24 17:49:28,898] Trial 14 finished with value: 0.8866591404546871 and parameters: {'depth': 10, 'l2_leaf_reg': 6.671336025810517, 'learning_rate': 0.3435190000105214, 'random_strength': 3.358024483842096, 'objective': 'CrossEntropy', 'iterations': 42}. Best is trial 14 with value: 0.8866591404546871.\n"
     ]
    }
   ],
   "source": [
    "# Optimize using Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "d59bfbc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detailed objective to train model with best parameters\n",
    "def detailed_objective(trial):\n",
    "    param = {\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1, 10, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 0.01, 1, log=True),\n",
    "        \"random_strength\": trial.suggest_float(\"random_strength\", 1, 10, log=True),\n",
    "        \"objective\": trial.suggest_categorical(\"objective\", [\"Logloss\", \"CrossEntropy\"]),\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 1, 1000, log=True),\n",
    "    }\n",
    "\n",
    "    param[\"thread_count\"] = 4\n",
    "    param[\"random_seed\"] = 42\n",
    "\n",
    "    clf = CatBoostClassifier(**param)\n",
    "    clf.fit(X_train, y_train, verbose=0)\n",
    "\n",
    "    score_val = clf.predict(X_val, prediction_type=\"Probability\")[:, 1]\n",
    "    score_test = clf.predict(X_test, prediction_type=\"Probability\")[:, 1]\n",
    "\n",
    "    return score_val, score_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ec240d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores for validation and test set\n",
    "y_score_val, y_score_test = detailed_objective(study.best_trial)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49598591",
   "metadata": {},
   "source": [
    "# Attack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "bff5e0fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log-odds transformation\n",
    "f_scores = lambda prob, l: (np.log(prob) - np.log(1 - prob)) * (2 * l - 1)\n",
    "y_score_test = f_scores(y_score_test, y_test)\n",
    "y_score_val = f_scores(y_score_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e15fa302",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quantile regression objective\n",
    "def objective_2(trial):\n",
    "    param = {\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-4, 1e4, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1, log=True),\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 1, 1000, log=True),\n",
    "    }\n",
    "\n",
    "    param[\"thread_count\"] = 1\n",
    "    param[\"objective\"] = \"RMSEWithUncertainty\"\n",
    "    param[\"posterior_sampling\"] = True\n",
    "    param[\"random_seed\"] = 42\n",
    "    eval_metric = RMSEWithUncertainty()\n",
    "\n",
    "    _X_train, _X_valid, _y_train, _y_valid = train_test_split(\n",
    "        X_val, y_score_val, test_size=0.2, random_state=np.random.randint(0, 1000), stratify=y_val\n",
    "    )\n",
    "\n",
    "    clf = CatBoostRegressor(**param)\n",
    "    clf.fit(_X_train, _y_train, verbose=0)\n",
    "    _y_pred_valid = clf.predict(_X_valid, prediction_type=\"RawFormulaVal\")\n",
    "    score = eval_metric.eval(label=_y_valid.T, approx=_y_pred_valid.T)\n",
    "\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0acd2f4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 17:49:37,571] A new study created in memory with name: no-name-83eac210-8708-4ae2-b34f-c1f16cef0e29\n",
      "[I 2024-10-24 17:49:38,443] Trial 4 finished with value: 2.105907469012939 and parameters: {'depth': 10, 'l2_leaf_reg': 4.841842748189172, 'learning_rate': 0.0013172457032780333, 'iterations': 1}. Best is trial 4 with value: 2.105907469012939.\n",
      "[I 2024-10-24 17:49:38,604] Trial 2 finished with value: 2.1343538020581674 and parameters: {'depth': 4, 'l2_leaf_reg': 0.0005204166454603278, 'learning_rate': 0.00010512938419523253, 'iterations': 6}. Best is trial 4 with value: 2.105907469012939.\n",
      "[I 2024-10-24 17:49:38,740] Trial 12 finished with value: 1.7644317314254574 and parameters: {'depth': 7, 'l2_leaf_reg': 0.00019754495646021728, 'learning_rate': 0.7136104021750086, 'iterations': 1}. Best is trial 12 with value: 1.7644317314254574.\n",
      "[I 2024-10-24 17:49:38,748] Trial 6 finished with value: 1.6905593692941983 and parameters: {'depth': 8, 'l2_leaf_reg': 12.555676773181865, 'learning_rate': 0.38505643134331424, 'iterations': 5}. Best is trial 6 with value: 1.6905593692941983.\n",
      "[I 2024-10-24 17:49:38,774] Trial 11 finished with value: 1.8368083102940167 and parameters: {'depth': 5, 'l2_leaf_reg': 182.94029906552322, 'learning_rate': 0.5629236002320689, 'iterations': 1}. Best is trial 6 with value: 1.6905593692941983.\n",
      "[I 2024-10-24 17:49:38,775] Trial 5 finished with value: 1.7768420569669863 and parameters: {'depth': 4, 'l2_leaf_reg': 1542.7333438662265, 'learning_rate': 0.10217544113252527, 'iterations': 16}. Best is trial 6 with value: 1.6905593692941983.\n",
      "[I 2024-10-24 17:49:38,820] Trial 8 finished with value: 1.3259924080259607e+70 and parameters: {'depth': 7, 'l2_leaf_reg': 0.003195273270408381, 'learning_rate': 0.8307158442043423, 'iterations': 5}. Best is trial 6 with value: 1.6905593692941983.\n",
      "[I 2024-10-24 17:49:38,837] Trial 10 finished with value: 2.12424344653893 and parameters: {'depth': 9, 'l2_leaf_reg': 40.56883577049722, 'learning_rate': 0.000435411746608732, 'iterations': 2}. Best is trial 6 with value: 1.6905593692941983.\n",
      "[I 2024-10-24 17:49:38,837] Trial 3 finished with value: 1.9549021456891433 and parameters: {'depth': 4, 'l2_leaf_reg': 0.025541938909755344, 'learning_rate': 0.0158925863346807, 'iterations': 17}. Best is trial 6 with value: 1.6905593692941983.\n",
      "[I 2024-10-24 17:49:38,861] Trial 13 finished with value: 1.6912985858487777 and parameters: {'depth': 6, 'l2_leaf_reg': 0.02481102907627826, 'learning_rate': 0.10669322042837477, 'iterations': 7}. Best is trial 6 with value: 1.6905593692941983.\n",
      "[I 2024-10-24 17:49:38,909] Trial 1 finished with value: 1.655808315928706 and parameters: {'depth': 4, 'l2_leaf_reg': 72.06483470427455, 'learning_rate': 0.021591835880072702, 'iterations': 56}. Best is trial 1 with value: 1.655808315928706.\n",
      "[I 2024-10-24 17:49:39,074] Trial 14 finished with value: 1.6183828553724025 and parameters: {'depth': 2, 'l2_leaf_reg': 33.36857012382663, 'learning_rate': 0.3376709889247116, 'iterations': 4}. Best is trial 14 with value: 1.6183828553724025.\n",
      "[I 2024-10-24 17:49:39,697] Trial 0 finished with value: 112.33325310686891 and parameters: {'depth': 6, 'l2_leaf_reg': 0.6134649518357013, 'learning_rate': 0.8195679740148449, 'iterations': 156}. Best is trial 14 with value: 1.6183828553724025.\n",
      "[I 2024-10-24 17:49:39,727] Trial 9 finished with value: 1419.4426223699425 and parameters: {'depth': 4, 'l2_leaf_reg': 0.00018606254714115275, 'learning_rate': 0.07107650215124019, 'iterations': 203}. Best is trial 14 with value: 1.6183828553724025.\n",
      "[I 2024-10-24 17:49:39,880] Trial 7 finished with value: 1.7505801610696095 and parameters: {'depth': 6, 'l2_leaf_reg': 48.66086549891899, 'learning_rate': 0.004278996197070888, 'iterations': 174}. Best is trial 14 with value: 1.6183828553724025.\n"
     ]
    }
   ],
   "source": [
    "# Optimize quantile regression\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective_2, n_trials=15, n_jobs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3a3cfe0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_objective2(trial):\n",
    "    param = {\n",
    "        \"depth\": trial.suggest_int(\"depth\", 1, 10),\n",
    "        \"l2_leaf_reg\": trial.suggest_float(\"l2_leaf_reg\", 1e-4, 1e4, log=True),\n",
    "        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-4, 1, log=True),\n",
    "        \"iterations\": trial.suggest_int(\"iterations\", 1, 1000, log=True),\n",
    "    }\n",
    "\n",
    "    param[\"thread_count\"] = 1\n",
    "    param[\"objective\"] = \"RMSEWithUncertainty\"\n",
    "    param[\"posterior_sampling\"] = True\n",
    "    param[\"random_seed\"] = 42\n",
    "\n",
    "    clf = CatBoostRegressor(**param)\n",
    "    clf.fit(X_val, y_score_val, verbose=0)\n",
    "\n",
    "    conf_test = clf.predict(X_test, prediction_type=\"RawFormulaVal\")\n",
    "    return conf_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0fe9a310",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence values for test set\n",
    "y_conf = detailed_objective2(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "424538b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_pred = {}\n",
    "\n",
    "gaussian_pred[\"score\"] = y_score_test\n",
    "gaussian_pred[\"mu\"] = y_conf[:, 0]\n",
    "gaussian_pred[\"log_sigma\"] = y_conf[:, 1]\n",
    "gaussian_pred[\"membership\"] = membership\n",
    "\n",
    "gaussian_pred = pd.DataFrame(gaussian_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "033fb3a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QMIA Privacy Accuracy: 0.6024521195608622\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Set the threshold as `mu`, and classify based on whether the actual score exceeds `mu`\n",
    "predicted_membership = [\"in\" if score > mu else \"out\" for score, mu in zip(gaussian_pred[\"score\"], gaussian_pred[\"mu\"])]\n",
    "\n",
    "# Step 2: Calculate the accuracy by comparing the predicted membership with the true membership\n",
    "true_membership = gaussian_pred[\"membership\"]\n",
    "\n",
    "\n",
    "true_membership_numeric = [1 if m == \"in\" else 0 for m in true_membership]\n",
    "predicted_membership_numeric = [1 if m == \"in\" else 0 for m in predicted_membership]\n",
    "\n",
    "qmia_accuracy = accuracy_score(true_membership_numeric, predicted_membership_numeric)\n",
    "\n",
    "\n",
    "print(f\"QMIA Privacy Accuracy: {qmia_accuracy}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46789f53",
   "metadata": {},
   "source": [
    "# Subpopulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1ad7dcb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileged group with favorable outcome MIA accuracy: 0.6579858497666717\n",
      "Privileged group with unfavorable outcome MIA accuracy: 0.18543046357615894\n",
      "Unprivileged group with favorable outcome MIA accuracy: 0.5836531082118189\n",
      "Unprivileged group with unfavorable outcome MIA accuracy: 0.15217391304347827\n"
     ]
    }
   ],
   "source": [
    "def calculate_mia_accuracy_for_subpopulation(group_indices, gaussian_pred):\n",
    "    sub_gaussian_pred = gaussian_pred.iloc[group_indices]\n",
    "    predicted_membership = [\"in\" if score > mu else \"out\" for score, mu in zip(sub_gaussian_pred[\"score\"], sub_gaussian_pred[\"mu\"])]\n",
    "    predicted_membership_numeric = [1 if member == \"in\" else 0 for member in predicted_membership]\n",
    "    true_membership_numeric = [1 if member == \"in\" else 0 for member in sub_gaussian_pred[\"membership\"]]\n",
    "    return accuracy_score(true_membership_numeric, predicted_membership_numeric)\n",
    "\n",
    "# Calculate subpopulation indices within test set\n",
    "priv_fav_indices = np.where((test_dataset.protected_attributes.ravel() == 1) & (test_dataset.labels.ravel() == 1))[0]\n",
    "priv_unfav_indices = np.where((test_dataset.protected_attributes.ravel() == 1) & (test_dataset.labels.ravel() == 0))[0]\n",
    "unpriv_fav_indices = np.where((test_dataset.protected_attributes.ravel() == 0) & (test_dataset.labels.ravel() == 1))[0]\n",
    "unpriv_unfav_indices = np.where((test_dataset.protected_attributes.ravel() == 0) & (test_dataset.labels.ravel() == 0))[0]\n",
    "\n",
    "# Calculate MIA accuracy for each subpopulation within the test set\n",
    "priv_fav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(priv_fav_indices, gaussian_pred)\n",
    "priv_unfav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(priv_unfav_indices, gaussian_pred)\n",
    "unpriv_fav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(unpriv_fav_indices, gaussian_pred)\n",
    "unpriv_unfav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(unpriv_unfav_indices, gaussian_pred)\n",
    "\n",
    "# Print MIA accuracy for each subpopulation\n",
    "print(f\"Privileged group with favorable outcome MIA accuracy: {priv_fav_mia_accuracy}\")\n",
    "print(f\"Privileged group with unfavorable outcome MIA accuracy: {priv_unfav_mia_accuracy}\")\n",
    "print(f\"Unprivileged group with favorable outcome MIA accuracy: {unpriv_fav_mia_accuracy}\")\n",
    "print(f\"Unprivileged group with unfavorable outcome MIA accuracy: {unpriv_unfav_mia_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39730201",
   "metadata": {},
   "source": [
    "# Applying syn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bfa7772",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_orig_train, dataset_orig_test = dataset.split([0.5], shuffle=True)\n",
    "dataset_orig_val = dataset_orig_test\n",
    "from aif360.metrics import BinaryLabelDatasetMetric\n",
    "# check fairness on the original data\n",
    "metric_orig_train = BinaryLabelDatasetMetric(dataset_orig_train, \n",
    "                                            unprivileged_groups=unprivileged_groups,\n",
    "                                            privileged_groups=privileged_groups)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "4e85b9fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "f_label = dataset_orig_train.favorable_label\n",
    "uf_label = dataset_orig_train.unfavorable_label\n",
    "base_rate_privileged = metric_orig_train.base_rate(privileged=True)\n",
    "base_rate_unprivileged = metric_orig_train.base_rate(privileged=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "f19ccfcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aif360.metrics import utils\n",
    "# return dataset indices of unprivileged and privileaged groups\n",
    "def group_indices (dataset, unprivileged_groups):\n",
    "    feature_names = dataset.feature_names\n",
    "    cond_vec = utils.compute_boolean_conditioning_vector(dataset.features, feature_names, unprivileged_groups)\n",
    "\n",
    "    # indices of examples in the unprivileged and privileged groups\n",
    "    indices = [i for i, x in enumerate(cond_vec) if x == True]\n",
    "    priv_indices = [i for i, x in enumerate(cond_vec) if x == False]\n",
    "    return indices, priv_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4a977af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample unprivileged favored so that the absolute number \n",
    "# of the unprivileged favored matches with the privilged favored  \n",
    "def synthetic_balance (dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.00):\n",
    "\n",
    "    # make a duplicate copy of the input data\n",
    "    dataset_transf_train = dataset.copy(deepcopy=True)\n",
    "\n",
    "    # indices of examples in the unprivileged and privileged groups\n",
    "    indices, priv_indices = group_indices(dataset, unprivileged_groups)\n",
    "\n",
    "    # subset: unprivileged--unprivileged_dataset and privileged--privileged_dataset \n",
    "    unprivileged_dataset = dataset.subset(indices) # unprivileaged\n",
    "    privileged_dataset = dataset.subset(priv_indices) # privilegaed\n",
    "\n",
    "    # number of unprivileged/privileged with favorable label\n",
    "    n_unpriv_favor = np.count_nonzero(unprivileged_dataset.labels==f_label) # unprivileged with favorable label\n",
    "    n_unpriv_unfavor = np.count_nonzero(unprivileged_dataset.labels!=f_label) # unprivileged with unfavorable label\n",
    "    n_priv_favor = np.count_nonzero(privileged_dataset.labels==f_label) # privileged with favorable label\n",
    "    n_priv_unfavor = np.count_nonzero(privileged_dataset.labels!=f_label) # privileged with unfavorable label\n",
    "\n",
    "    # privileged group has more favored in absolute number than the unprivileged group\n",
    "    if n_unpriv_favor < n_priv_favor:\n",
    "        # inflate unprivileged favored class\n",
    "        n_extra_sample = (n_priv_favor - n_unpriv_favor)*sampling_strategy\n",
    "        if n_extra_sample + n_unpriv_favor >= n_unpriv_unfavor:\n",
    "            inflate_rate = int(((n_extra_sample+n_unpriv_favor)/n_unpriv_unfavor)+1)\n",
    "        else:\n",
    "            inflate_rate = round(((n_extra_sample+n_unpriv_favor)/n_unpriv_unfavor)+1)\n",
    "        _, extra_favored  = balance(unprivileged_dataset, n_extra_sample, inflate_rate, f_label, uf_label)\n",
    "\n",
    "        # inflate unprivileged unfavored class to match the privileged baserate\n",
    "        n_extra_sample = (n_extra_sample + n_unpriv_favor - bp * (n_extra_sample + n_unpriv_favor + n_unpriv_unfavor))/bp\n",
    "        if n_extra_sample + n_unpriv_unfavor >= n_unpriv_favor:\n",
    "            inflate_rate = int(((n_extra_sample+n_unpriv_unfavor)/n_unpriv_favor)+1)\n",
    "        else:\n",
    "            inflate_rate = round(((n_extra_sample+n_unpriv_unfavor)/n_unpriv_favor)+1)\n",
    "        _, extra_unfavored  = balance(unprivileged_dataset, n_extra_sample, inflate_rate, uf_label, f_label)\n",
    "\n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, extra_favored.features, extra_unfavored.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, extra_favored.labels, extra_unfavored.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, extra_favored.instance_weights, extra_unfavored.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, extra_favored.protected_attributes, extra_unfavored.protected_attributes))\n",
    "\n",
    "    return dataset_transf_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0c02d943",
   "metadata": {},
   "outputs": [],
   "source": [
    "# adaptive oversampling for the unprivileged group\n",
    "def synthetic(dataset, unprivileged_groups, bp, bnp, f_label, uf_label, os_mode=2, sampling_strategy=0.50):\n",
    "\n",
    "    # make a duplicate copy of the input data\n",
    "    dataset_transf_train = dataset.copy(deepcopy=True)\n",
    "\n",
    "    # Case 1: privileged group is not relatively favored but has more favored in absolute number than the unprivileged group\n",
    "    # if privileged base rate is less than unprivilegeted base rate\n",
    "    if bp < bnp:\n",
    "        dataset_transf_train = synthetic_balance(dataset, unprivileged_groups, bp, bnp, f_label, uf_label)\n",
    "        return dataset_transf_train\n",
    "\n",
    "    # Case 2: if privileged is favored, i.e. has a higher base rate\n",
    "\n",
    "    # [Method 1] inflate privileged unfavored class\n",
    "    if os_mode == 1:\n",
    "        _, sample_unfavor_priv = synthetic_unfavor_priv (dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.00)\n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, sample_unfavor_priv.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, sample_unfavor_priv.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, sample_unfavor_priv.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, sample_unfavor_priv.protected_attributes))\n",
    "    elif os_mode == 2:\n",
    "    # [Method 2] inflate unprivileged favored class\n",
    "        _, sample_favor_unpriv = synthetic_favor_unpriv (dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.0)\n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, sample_favor_unpriv.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, sample_favor_unpriv.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, sample_favor_unpriv.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, sample_favor_unpriv.protected_attributes))\n",
    "    # [Method 3] combine methods 1 and 2 \n",
    "    elif os_mode == 3:\n",
    "        _, sample_unfavor_priv = synthetic_unfavor_priv (dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.00)\n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, sample_unfavor_priv.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, sample_unfavor_priv.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, sample_unfavor_priv.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, sample_unfavor_priv.protected_attributes))\n",
    "        _, sample_favor_unpriv = synthetic_favor_unpriv (dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.00)\n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, sample_favor_unpriv.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, sample_favor_unpriv.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, sample_favor_unpriv.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, sample_favor_unpriv.protected_attributes))\n",
    "    else:\n",
    "        sys.exit(\"Oversampling mode is missing: 1: oversample unfavorable privileged; 2: oversample favorable unprivileged; 3. both\")\n",
    "\n",
    "    return dataset_transf_train\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "797f11ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# oversample favorable in the unprivileged group\n",
    "def synthetic_favor_unpriv (dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.00):\n",
    "\n",
    "    indices, priv_indices = group_indices (dataset, unprivileged_groups)\n",
    "\n",
    "    # subset: unprivileged--unprivileged_dataset and privileged--privileged_dataset \n",
    "    unprivileged_dataset = dataset.subset(indices) # unprivileaged\n",
    "    privileged_dataset = dataset.subset(priv_indices) # privilegaed\n",
    "\n",
    "    n_unpriv_favor = np.count_nonzero(unprivileged_dataset.labels==f_label) # unprivileged with favorable label\n",
    "    n_unpriv_unfavor = np.count_nonzero(unprivileged_dataset.labels!=f_label) # unprivileged with unfavorable label\n",
    "\n",
    "    n_extra_sample = (bp * len(indices)-n_unpriv_favor) / (1- bp) * sampling_strategy\n",
    "    # unprivileged favorable > unprivileged unfavorable\n",
    "    if n_extra_sample + n_unpriv_favor >= n_unpriv_unfavor:\n",
    "        inflate_rate = int(((n_extra_sample+n_unpriv_favor)/n_unpriv_unfavor)+1)\n",
    "    else:\n",
    "        inflate_rate = round(((n_extra_sample+n_unpriv_favor)/n_unpriv_unfavor)+1)\n",
    "\n",
    "    dataset_transf_refprivileged_train, extra_favored_unpriv  = balance(unprivileged_dataset, n_extra_sample, inflate_rate, f_label, uf_label)\n",
    "\n",
    "    return dataset_transf_refprivileged_train, extra_favored_unpriv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3dff62dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def balance(dataset, n_extra, inflate_rate, f_label, uf_label):\n",
    "\n",
    "    # make a duplicate copy of the input data\n",
    "    dataset_transf_train = dataset.copy(deepcopy=True)\n",
    "\n",
    "    # subsets with favorable labels and unfavorable labels\n",
    "    f_dataset = dataset.subset(np.where(dataset.labels==f_label)[0].tolist())\n",
    "    uf_dataset = dataset.subset(np.where(dataset.labels==uf_label)[0].tolist())\n",
    "\n",
    "    # expand the group with uf_label for oversampling purpose\n",
    "    inflated_uf_features = np.repeat(uf_dataset.features, inflate_rate, axis=0)\n",
    "    sample_features = np.concatenate((f_dataset.features, inflated_uf_features))\n",
    "    inflated_uf_labels = np.repeat(uf_dataset.labels, inflate_rate, axis=0)\n",
    "    sample_labels = np.concatenate((f_dataset.labels, inflated_uf_labels))\n",
    "    \n",
    "\n",
    "    # oversampling favorable samples\n",
    "    # X: inflated dataset with synthetic samples of f_label attached to the end\n",
    "\n",
    "    # Now apply ADASYN oversampling\n",
    "    oversample = ADASYN(sampling_strategy='minority')\n",
    "    X, y = oversample.fit_resample(sample_features, sample_labels)\n",
    "    y = y.reshape(-1,1)\n",
    "\n",
    "    # take samples from dataset with only favorable labels\n",
    "    X = X[np.where(y==f_label)[0].tolist()]  # data with f_label + new samples\n",
    "    y = y[y==f_label]\n",
    "\n",
    "    selected = int(f_dataset.features.shape[0]+n_extra)\n",
    "\n",
    "    X = X[:selected, :]\n",
    "    y = y[:selected]\n",
    "    y = y.reshape(-1,1)\n",
    "    \n",
    "    # print(f\"Type of instance_weights: {type(f_dataset.instance_weights)}\")\n",
    "    # print(f\"Shape of instance_weights: {getattr(f_dataset.instance_weights, 'shape', 'N/A')}\")\n",
    "    # print(f\"Content of instance_weights: {f_dataset.instance_weights}\")\n",
    "\n",
    "    # print(f\"Type of protected_attributes: {type(f_dataset.protected_attributes)}\")\n",
    "    # print(f\"Shape of protected_attributes: {getattr(f_dataset.protected_attributes, 'shape', 'N/A')}\")\n",
    "    # print(f\"Content of protected_attributes: {f_dataset.protected_attributes}\")\n",
    "\n",
    "    # Convert to lists if necessary\n",
    "    instance_weights_list = f_dataset.instance_weights.flatten().tolist() if isinstance(f_dataset.instance_weights, np.ndarray) else f_dataset.instance_weights\n",
    "    protected_attributes_list = f_dataset.protected_attributes.flatten().tolist() if isinstance(f_dataset.protected_attributes, np.ndarray) else f_dataset.protected_attributes\n",
    "\n",
    "    # set weights and protected_attributes for the newly generated samples\n",
    "    inc = X.shape[0]-f_dataset.features.shape[0]\n",
    "    new_weights = [random.choice(instance_weights_list) for _ in range(inc)]\n",
    "    new_attributes = [random.choice(protected_attributes_list) for _ in range(inc)]\n",
    "    \n",
    "    # new_attributes is 1D, reshape it to match the shape (n, 1)\n",
    "    new_attributes = np.array(new_attributes).reshape(-1, 1)\n",
    "\n",
    "    # compose transformed dataset\n",
    "    dataset_transf_train.features = np.concatenate((uf_dataset.features, X))\n",
    "    dataset_transf_train.labels = np.concatenate((uf_dataset.labels, y))\n",
    "    dataset_transf_train.instance_weights = np.concatenate((uf_dataset.instance_weights, f_dataset.instance_weights, new_weights))\n",
    "    dataset_transf_train.protected_attributes = np.concatenate((uf_dataset.protected_attributes, f_dataset.protected_attributes, new_attributes))\n",
    "\n",
    "    # make a duplicate copy of the input data\n",
    "    dataset_extra_train = dataset.copy()\n",
    "\n",
    "    X_ex = X[-int(n_extra):]\n",
    "    y_ex = y[-int(n_extra):]\n",
    "    y_ex = y_ex.reshape(-1,1)\n",
    "    \n",
    "    # set weights and protected_attributes for the newly generated samples\n",
    "    inc = int(n_extra)\n",
    "    new_weights = [random.choice(instance_weights_list) for _ in range(inc)]\n",
    "    new_attributes = [random.choice(protected_attributes_list) for _ in range(inc)]\n",
    "    \n",
    "    # new_attributes is 1D, reshape it to match the shape (n, 1)\n",
    "    new_attributes = np.array(new_attributes).reshape(-1, 1)\n",
    "\n",
    "    # compose extra dataset\n",
    "    dataset_extra_train.features = X_ex\n",
    "    dataset_extra_train.labels = y_ex\n",
    "    dataset_extra_train.instance_weights = new_weights\n",
    "    dataset_extra_train.protected_attributes = new_attributes\n",
    "\n",
    "    # verifying\n",
    "    #print(dataset_transf_train.features.shape)\n",
    "    #print(dataset_transf_train.labels.shape)\n",
    "    #print(dataset_transf_train.instance_weights.shape)\n",
    "    #print(dataset_transf_train.protected_attributes.shape)\n",
    "\n",
    "    # return favor and unfavored oversampling results\n",
    "    return dataset_transf_train, dataset_extra_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d24f716d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_transf_train = synthetic(dataset_orig_train, unprivileged_groups, base_rate_privileged, base_rate_unprivileged, f_label, uf_label, os_mode = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5bee9b12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = dataset_transf_train.features\n",
    "y = dataset_transf_train.labels.ravel()\n",
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "X_val, X_test_us, y_val, y_test_us = train_test_split(X_test, y_test, test_size=0.2, random_state=42, stratify=y_test)\n",
    "X_test = np.concatenate((X_train, X_test_us), axis=0)\n",
    "y_test = np.concatenate((y_train, y_test_us), axis=0)\n",
    "membership = [\"in\"] * X_train.shape[0] + [\"out\"] * X_test_us.shape[0]\n",
    "# Create a DataFrame for the test dataset to recreate the BinaryLabelDataset\n",
    "df_test = pd.DataFrame(X_test, columns=dataset.feature_names)\n",
    "df_test['two_year_recid'] = y_test  # Add the label\n",
    "# df_test['race'] = protected_test  # Add the protected attribute\n",
    "\n",
    "# Recreate the BinaryLabelDataset for the test set\n",
    "test_dataset = BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=df_test,\n",
    "    label_names=['two_year_recid'],  # Label column name\n",
    "    protected_attribute_names=['race']  # Protected attribute column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "aeb612d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 17:51:00,038] A new study created in memory with name: no-name-8620ec78-9263-421f-9785-02e90b18a9fd\n",
      "[I 2024-10-24 17:51:02,400] Trial 0 finished with value: 0.8671918767507003 and parameters: {'depth': 2, 'l2_leaf_reg': 6.812345192740989, 'learning_rate': 0.01732099821758437, 'random_strength': 2.610958873921719, 'objective': 'Logloss', 'iterations': 780}. Best is trial 0 with value: 0.8671918767507003.\n",
      "[I 2024-10-24 17:51:02,496] Trial 1 finished with value: 0.7905112044817928 and parameters: {'depth': 4, 'l2_leaf_reg': 4.985691686101141, 'learning_rate': 0.012561033331743273, 'random_strength': 1.1578588348274919, 'objective': 'Logloss', 'iterations': 1}. Best is trial 0 with value: 0.8671918767507003.\n",
      "[I 2024-10-24 17:51:07,449] Trial 2 finished with value: 0.8072303921568628 and parameters: {'depth': 9, 'l2_leaf_reg': 1.0681159828568825, 'learning_rate': 0.37487084890893435, 'random_strength': 1.0497411719630407, 'objective': 'Logloss', 'iterations': 259}. Best is trial 0 with value: 0.8671918767507003.\n",
      "[I 2024-10-24 17:51:07,643] Trial 3 finished with value: 0.7922881652661065 and parameters: {'depth': 10, 'l2_leaf_reg': 8.89567119555594, 'learning_rate': 0.5424026370156216, 'random_strength': 7.933277997239595, 'objective': 'Logloss', 'iterations': 2}. Best is trial 0 with value: 0.8671918767507003.\n",
      "[I 2024-10-24 17:51:07,773] Trial 4 finished with value: 0.8707282913165266 and parameters: {'depth': 1, 'l2_leaf_reg': 8.108347154115195, 'learning_rate': 0.5618262673940819, 'random_strength': 1.206046233268649, 'objective': 'CrossEntropy', 'iterations': 40}. Best is trial 4 with value: 0.8707282913165266.\n",
      "[I 2024-10-24 17:51:08,974] Trial 5 finished with value: 0.7859943977591036 and parameters: {'depth': 6, 'l2_leaf_reg': 1.605165143242439, 'learning_rate': 0.3134961104641419, 'random_strength': 1.8295906372635211, 'objective': 'CrossEntropy', 'iterations': 219}. Best is trial 4 with value: 0.8707282913165266.\n",
      "[I 2024-10-24 17:51:09,072] Trial 6 finished with value: 0.8187062324929971 and parameters: {'depth': 3, 'l2_leaf_reg': 9.847370676259736, 'learning_rate': 0.8648003138236796, 'random_strength': 8.345332242177347, 'objective': 'CrossEntropy', 'iterations': 5}. Best is trial 4 with value: 0.8707282913165266.\n",
      "[I 2024-10-24 17:51:10,803] Trial 7 finished with value: 0.8746673669467787 and parameters: {'depth': 9, 'l2_leaf_reg': 4.339874900994753, 'learning_rate': 0.2548396525326566, 'random_strength': 2.743101360699752, 'objective': 'Logloss', 'iterations': 121}. Best is trial 7 with value: 0.8746673669467787.\n",
      "[I 2024-10-24 17:51:11,819] Trial 8 finished with value: 0.8883928571428571 and parameters: {'depth': 5, 'l2_leaf_reg': 1.4958345325873867, 'learning_rate': 0.011705049600494366, 'random_strength': 5.133450367499738, 'objective': 'CrossEntropy', 'iterations': 391}. Best is trial 8 with value: 0.8883928571428571.\n",
      "[I 2024-10-24 17:51:13,340] Trial 9 finished with value: 0.7526260504201681 and parameters: {'depth': 5, 'l2_leaf_reg': 3.0753207035515278, 'learning_rate': 0.3851548303813647, 'random_strength': 3.2665685270522373, 'objective': 'CrossEntropy', 'iterations': 622}. Best is trial 8 with value: 0.8883928571428571.\n",
      "[I 2024-10-24 17:51:13,500] Trial 10 finished with value: 0.7764793417366946 and parameters: {'depth': 7, 'l2_leaf_reg': 1.8487535609326529, 'learning_rate': 0.044576003552790504, 'random_strength': 5.1128920828561535, 'objective': 'CrossEntropy', 'iterations': 14}. Best is trial 8 with value: 0.8883928571428571.\n",
      "[I 2024-10-24 17:51:14,091] Trial 11 finished with value: 0.8759803921568629 and parameters: {'depth': 8, 'l2_leaf_reg': 3.339394192498823, 'learning_rate': 0.12206064813848794, 'random_strength': 4.3218829781185635, 'objective': 'Logloss', 'iterations': 69}. Best is trial 8 with value: 0.8883928571428571.\n",
      "[I 2024-10-24 17:51:14,351] Trial 12 finished with value: 0.8658788515406163 and parameters: {'depth': 7, 'l2_leaf_reg': 2.7567806661116157, 'learning_rate': 0.11493570803175114, 'random_strength': 4.961471465101774, 'objective': 'CrossEntropy', 'iterations': 39}. Best is trial 8 with value: 0.8883928571428571.\n",
      "[I 2024-10-24 17:51:14,790] Trial 13 finished with value: 0.8849964985994397 and parameters: {'depth': 7, 'l2_leaf_reg': 2.1532093587920182, 'learning_rate': 0.05860950729359022, 'random_strength': 4.770853115225395, 'objective': 'Logloss', 'iterations': 99}. Best is trial 8 with value: 0.8883928571428571.\n",
      "[I 2024-10-24 17:51:15,853] Trial 14 finished with value: 0.8709558823529412 and parameters: {'depth': 5, 'l2_leaf_reg': 1.6282786217871374, 'learning_rate': 0.033705085308249914, 'random_strength': 3.8405336969655557, 'objective': 'CrossEntropy', 'iterations': 372}. Best is trial 8 with value: 0.8883928571428571.\n"
     ]
    }
   ],
   "source": [
    "# Optimize using Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4dc7db7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores for validation and test set\n",
    "y_score_val, y_score_test = detailed_objective(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "7934b88e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log-odds transformation\n",
    "f_scores = lambda prob, l: (np.log(prob) - np.log(1 - prob)) * (2 * l - 1)\n",
    "y_score_test = f_scores(y_score_test, y_test)\n",
    "y_score_val = f_scores(y_score_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4e6dd7df",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 17:51:58,659] A new study created in memory with name: no-name-cc6b9a8e-0d8a-43bd-9863-a570186826a1\n",
      "[I 2024-10-24 17:51:59,262] Trial 6 finished with value: 1.9649004264810583 and parameters: {'depth': 1, 'l2_leaf_reg': 508.18547254244606, 'learning_rate': 0.00011328445525377983, 'iterations': 5}. Best is trial 6 with value: 1.9649004264810583.\n",
      "[I 2024-10-24 17:52:00,666] Trial 10 finished with value: 1.7114137418036632 and parameters: {'depth': 6, 'l2_leaf_reg': 273.570791123805, 'learning_rate': 0.3166716725298157, 'iterations': 3}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:01,125] Trial 9 finished with value: 1.9468043988274464 and parameters: {'depth': 2, 'l2_leaf_reg': 0.00028840034535457335, 'learning_rate': 0.00019280627405769953, 'iterations': 15}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:01,131] Trial 5 finished with value: 1.9362733934175058 and parameters: {'depth': 6, 'l2_leaf_reg': 0.0016978988644780078, 'learning_rate': 0.003900331849449077, 'iterations': 44}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:01,267] Trial 2 finished with value: 1.9375420721222494 and parameters: {'depth': 7, 'l2_leaf_reg': 0.0030007695636707038, 'learning_rate': 0.00022668126902680255, 'iterations': 25}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:01,526] Trial 1 finished with value: 1.8921845701564195 and parameters: {'depth': 1, 'l2_leaf_reg': 1255.823467800108, 'learning_rate': 0.0010734948866375671, 'iterations': 293}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:01,528] Trial 0 finished with value: 1.875040944671802 and parameters: {'depth': 3, 'l2_leaf_reg': 0.5795795287862476, 'learning_rate': 0.30137867739367913, 'iterations': 119}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:01,528] Trial 13 finished with value: 1.9546676807257708 and parameters: {'depth': 5, 'l2_leaf_reg': 7041.9787017649205, 'learning_rate': 0.00011318297509231116, 'iterations': 34}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:01,714] Trial 14 finished with value: 1.854157651323525 and parameters: {'depth': 4, 'l2_leaf_reg': 0.00023279205586478944, 'learning_rate': 0.002882778019443588, 'iterations': 50}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:01,878] Trial 11 finished with value: 1744.044010453892 and parameters: {'depth': 3, 'l2_leaf_reg': 0.07071010556986404, 'learning_rate': 0.24096270801756062, 'iterations': 382}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:02,721] Trial 4 finished with value: 1.9212228546222783 and parameters: {'depth': 6, 'l2_leaf_reg': 388.56901177087076, 'learning_rate': 0.0006158186534654753, 'iterations': 150}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:03,113] Trial 12 finished with value: 348.0926089900429 and parameters: {'depth': 3, 'l2_leaf_reg': 0.012920595851085912, 'learning_rate': 0.5255842066674445, 'iterations': 663}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:07,640] Trial 7 finished with value: 3.179646610186677 and parameters: {'depth': 8, 'l2_leaf_reg': 30.262677222497533, 'learning_rate': 0.03748901799923339, 'iterations': 170}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:08,534] Trial 8 finished with value: 1.9070317465854851 and parameters: {'depth': 10, 'l2_leaf_reg': 0.004026989753791348, 'learning_rate': 0.001318437019516757, 'iterations': 43}. Best is trial 10 with value: 1.7114137418036632.\n",
      "[I 2024-10-24 17:52:13,965] Trial 3 finished with value: 1.8849848004647671 and parameters: {'depth': 7, 'l2_leaf_reg': 1699.7486250141199, 'learning_rate': 0.0013728505112478022, 'iterations': 687}. Best is trial 10 with value: 1.7114137418036632.\n"
     ]
    }
   ],
   "source": [
    "# Optimize quantile regression\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective_2, n_trials=15, n_jobs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "e1a3b12f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence values for test set\n",
    "y_conf = detailed_objective2(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a797cbb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_pred = {}\n",
    "\n",
    "gaussian_pred[\"score\"] = y_score_test\n",
    "gaussian_pred[\"mu\"] = y_conf[:, 0]\n",
    "gaussian_pred[\"log_sigma\"] = y_conf[:, 1]\n",
    "gaussian_pred[\"membership\"] = membership\n",
    "\n",
    "gaussian_pred = pd.DataFrame(gaussian_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "cb186221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QMIA Privacy Accuracy: 0.5884363429222443\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Step 1: Set the threshold as `mu`, and classify based on whether the actual score exceeds `mu`\n",
    "predicted_membership = [\"in\" if score > mu else \"out\" for score, mu in zip(gaussian_pred[\"score\"], gaussian_pred[\"mu\"])]\n",
    "\n",
    "# Step 2: Calculate the accuracy by comparing the predicted membership with the true membership\n",
    "true_membership = gaussian_pred[\"membership\"]\n",
    "\n",
    "\n",
    "true_membership_numeric = [1 if m == \"in\" else 0 for m in true_membership]\n",
    "predicted_membership_numeric = [1 if m == \"in\" else 0 for m in predicted_membership]\n",
    "\n",
    "qmia_accuracy = accuracy_score(true_membership_numeric, predicted_membership_numeric)\n",
    "\n",
    "\n",
    "print(f\"QMIA Privacy Accuracy: {qmia_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3c444dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileged group with favorable outcome MIA accuracy: 0.4329501915708812\n",
      "Privileged group with unfavorable outcome MIA accuracy: 0.17142857142857143\n",
      "Unprivileged group with favorable outcome MIA accuracy: 0.6186507312470514\n",
      "Unprivileged group with unfavorable outcome MIA accuracy: 0.1678832116788321\n"
     ]
    }
   ],
   "source": [
    "def calculate_mia_accuracy_for_subpopulation(group_indices, gaussian_pred):\n",
    "    sub_gaussian_pred = gaussian_pred.iloc[group_indices]\n",
    "    predicted_membership = [\"in\" if score > mu else \"out\" for score, mu in zip(sub_gaussian_pred[\"score\"], sub_gaussian_pred[\"mu\"])]\n",
    "    predicted_membership_numeric = [1 if member == \"in\" else 0 for member in predicted_membership]\n",
    "    true_membership_numeric = [1 if member == \"in\" else 0 for member in sub_gaussian_pred[\"membership\"]]\n",
    "    return accuracy_score(true_membership_numeric, predicted_membership_numeric)\n",
    "\n",
    "# Calculate subpopulation indices within test set\n",
    "priv_fav_indices = np.where((test_dataset.protected_attributes.ravel() == 1) & (test_dataset.labels.ravel() == 1))[0]\n",
    "priv_unfav_indices = np.where((test_dataset.protected_attributes.ravel() == 1) & (test_dataset.labels.ravel() == 0))[0]\n",
    "unpriv_fav_indices = np.where((test_dataset.protected_attributes.ravel() == 0) & (test_dataset.labels.ravel() == 1))[0]\n",
    "unpriv_unfav_indices = np.where((test_dataset.protected_attributes.ravel() == 0) & (test_dataset.labels.ravel() == 0))[0]\n",
    "\n",
    "# Calculate MIA accuracy for each subpopulation within the test set\n",
    "priv_fav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(priv_fav_indices, gaussian_pred)\n",
    "priv_unfav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(priv_unfav_indices, gaussian_pred)\n",
    "unpriv_fav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(unpriv_fav_indices, gaussian_pred)\n",
    "unpriv_unfav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(unpriv_unfav_indices, gaussian_pred)\n",
    "\n",
    "# Print MIA accuracy for each subpopulation\n",
    "print(f\"Privileged group with favorable outcome MIA accuracy: {priv_fav_mia_accuracy}\")\n",
    "print(f\"Privileged group with unfavorable outcome MIA accuracy: {priv_unfav_mia_accuracy}\")\n",
    "print(f\"Unprivileged group with favorable outcome MIA accuracy: {unpriv_fav_mia_accuracy}\")\n",
    "print(f\"Unprivileged group with unfavorable outcome MIA accuracy: {unpriv_unfav_mia_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564cbf97",
   "metadata": {},
   "source": [
    "# Applying DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1b0062f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "DIR = DisparateImpactRemover(sensitive_attribute='gender')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "275a366a",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_dir_train = DIR.fit_transform(dataset_orig_train)\n",
    "dataset = dataset_dir_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "1cce0ff1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "X_val, X_test_us, y_val, y_test_us = train_test_split(X_test, y_test, test_size=0.2, random_state=42, stratify=y_test)\n",
    "X_test = np.concatenate((X_train, X_test_us), axis=0)\n",
    "y_test = np.concatenate((y_train, y_test_us), axis=0)\n",
    "membership = [\"in\"] * X_train.shape[0] + [\"out\"] * X_test_us.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8dc06d81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the test dataset to recreate the BinaryLabelDataset\n",
    "df_test = pd.DataFrame(X_test, columns=dataset.feature_names)\n",
    "df_test['pass_bar'] = y_test  # Add the label\n",
    "# df_test['race'] = protected_test  # Add the protected attribute\n",
    "\n",
    "# Recreate the BinaryLabelDataset for the test set\n",
    "test_dataset = BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=df_test,\n",
    "    label_names=['pass_bar'],  # Label column name\n",
    "    protected_attribute_names=['gender']  # Protected attribute column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "02ed6da4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 17:53:28,186] A new study created in memory with name: no-name-07dfdcd0-13ff-46cf-806d-93440d5e2650\n",
      "[I 2024-10-24 17:53:28,306] Trial 0 finished with value: 0.8589898459383754 and parameters: {'depth': 5, 'l2_leaf_reg': 1.2596145424272962, 'learning_rate': 0.7587504038166499, 'random_strength': 1.5986430301818266, 'objective': 'Logloss', 'iterations': 4}. Best is trial 0 with value: 0.8589898459383754.\n",
      "[I 2024-10-24 17:53:30,264] Trial 1 finished with value: 0.860451680672269 and parameters: {'depth': 2, 'l2_leaf_reg': 1.1229511724692205, 'learning_rate': 0.530282382973134, 'random_strength': 6.455817200262652, 'objective': 'Logloss', 'iterations': 762}. Best is trial 1 with value: 0.860451680672269.\n",
      "[I 2024-10-24 17:53:30,547] Trial 2 finished with value: 0.7574404761904762 and parameters: {'depth': 8, 'l2_leaf_reg': 7.316577729033343, 'learning_rate': 0.06825674029869885, 'random_strength': 6.333307587207756, 'objective': 'Logloss', 'iterations': 22}. Best is trial 1 with value: 0.860451680672269.\n",
      "[I 2024-10-24 17:53:33,202] Trial 3 finished with value: 0.7563900560224089 and parameters: {'depth': 5, 'l2_leaf_reg': 2.393218146209887, 'learning_rate': 0.389152837653523, 'random_strength': 6.6565105896829815, 'objective': 'CrossEntropy', 'iterations': 652}. Best is trial 1 with value: 0.860451680672269.\n",
      "[I 2024-10-24 17:53:34,202] Trial 4 finished with value: 0.8520133053221288 and parameters: {'depth': 6, 'l2_leaf_reg': 2.9757203569632846, 'learning_rate': 0.05078574705086334, 'random_strength': 7.351615783412445, 'objective': 'Logloss', 'iterations': 177}. Best is trial 1 with value: 0.860451680672269.\n",
      "[I 2024-10-24 17:53:34,597] Trial 5 finished with value: 0.8505427170868347 and parameters: {'depth': 3, 'l2_leaf_reg': 1.8804368205482154, 'learning_rate': 0.04567116678003936, 'random_strength': 2.5680590473072096, 'objective': 'Logloss', 'iterations': 133}. Best is trial 1 with value: 0.860451680672269.\n",
      "[I 2024-10-24 17:53:35,074] Trial 6 finished with value: 0.7512605042016807 and parameters: {'depth': 4, 'l2_leaf_reg': 2.646801890252647, 'learning_rate': 0.01042228972487031, 'random_strength': 3.6739892881114167, 'objective': 'Logloss', 'iterations': 1}. Best is trial 1 with value: 0.860451680672269.\n",
      "[I 2024-10-24 17:53:35,228] Trial 7 finished with value: 0.7831845238095236 and parameters: {'depth': 3, 'l2_leaf_reg': 7.8911904250296825, 'learning_rate': 0.16342655095668276, 'random_strength': 4.781794457144418, 'objective': 'Logloss', 'iterations': 3}. Best is trial 1 with value: 0.860451680672269.\n",
      "[I 2024-10-24 17:53:35,361] Trial 8 finished with value: 0.8666754201680672 and parameters: {'depth': 2, 'l2_leaf_reg': 2.6760927694621532, 'learning_rate': 0.24196915704247945, 'random_strength': 2.4379498531912764, 'objective': 'CrossEntropy', 'iterations': 17}. Best is trial 8 with value: 0.8666754201680672.\n",
      "[I 2024-10-24 17:53:35,506] Trial 9 finished with value: 0.861484593837535 and parameters: {'depth': 4, 'l2_leaf_reg': 5.765699639493968, 'learning_rate': 0.31885512086932183, 'random_strength': 5.927287510323867, 'objective': 'Logloss', 'iterations': 11}. Best is trial 8 with value: 0.8666754201680672.\n",
      "[I 2024-10-24 17:53:35,753] Trial 10 finished with value: 0.8499562324929972 and parameters: {'depth': 1, 'l2_leaf_reg': 4.671243197765206, 'learning_rate': 0.15573779127633874, 'random_strength': 1.0119686076917849, 'objective': 'CrossEntropy', 'iterations': 75}. Best is trial 8 with value: 0.8666754201680672.\n",
      "[I 2024-10-24 17:53:36,246] Trial 11 finished with value: 0.8149072128851541 and parameters: {'depth': 7, 'l2_leaf_reg': 4.686612150438635, 'learning_rate': 0.32477310447724644, 'random_strength': 2.313114785548567, 'objective': 'CrossEntropy', 'iterations': 16}. Best is trial 8 with value: 0.8666754201680672.\n",
      "[I 2024-10-24 17:53:36,383] Trial 12 finished with value: 0.8349264705882352 and parameters: {'depth': 1, 'l2_leaf_reg': 4.685518930245615, 'learning_rate': 0.23471311712416457, 'random_strength': 9.617255539331495, 'objective': 'CrossEntropy', 'iterations': 7}. Best is trial 8 with value: 0.8666754201680672.\n",
      "[I 2024-10-24 17:53:37,718] Trial 13 finished with value: 0.8247899159663866 and parameters: {'depth': 10, 'l2_leaf_reg': 5.850195268030473, 'learning_rate': 0.11399968418855039, 'random_strength': 3.6585128760911187, 'objective': 'CrossEntropy', 'iterations': 43}. Best is trial 8 with value: 0.8666754201680672.\n",
      "[I 2024-10-24 17:53:37,820] Trial 14 finished with value: 0.7941439075630252 and parameters: {'depth': 3, 'l2_leaf_reg': 9.525315390440113, 'learning_rate': 0.6337744625932938, 'random_strength': 1.6269297305970778, 'objective': 'CrossEntropy', 'iterations': 1}. Best is trial 8 with value: 0.8666754201680672.\n"
     ]
    }
   ],
   "source": [
    "# Optimize using Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "84d0d5e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores for validation and test set\n",
    "y_score_val, y_score_test = detailed_objective(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4ac7589d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log-odds transformation\n",
    "f_scores = lambda prob, l: (np.log(prob) - np.log(1 - prob)) * (2 * l - 1)\n",
    "y_score_test = f_scores(y_score_test, y_test)\n",
    "y_score_val = f_scores(y_score_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "d7d2faae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 17:53:42,931] A new study created in memory with name: no-name-17389ecc-811d-401a-93a1-be622a7faf69\n",
      "[I 2024-10-24 17:53:43,226] Trial 4 finished with value: 1.6260695056013044 and parameters: {'depth': 6, 'l2_leaf_reg': 4.928317947053785, 'learning_rate': 0.26924147988478336, 'iterations': 1}. Best is trial 4 with value: 1.6260695056013044.\n",
      "[I 2024-10-24 17:53:43,404] Trial 1 finished with value: 695508497750.2505 and parameters: {'depth': 6, 'l2_leaf_reg': 0.06849041839425724, 'learning_rate': 0.8714204319646773, 'iterations': 3}. Best is trial 4 with value: 1.6260695056013044.\n",
      "[I 2024-10-24 17:53:43,461] Trial 9 finished with value: 1.7223527093228501 and parameters: {'depth': 3, 'l2_leaf_reg': 0.007047905745244669, 'learning_rate': 0.00048788186963738555, 'iterations': 1}. Best is trial 4 with value: 1.6260695056013044.\n",
      "[I 2024-10-24 17:53:44,120] Trial 6 finished with value: 1.6791248916646007 and parameters: {'depth': 5, 'l2_leaf_reg': 0.000842606810533249, 'learning_rate': 0.000635053827233999, 'iterations': 6}. Best is trial 4 with value: 1.6260695056013044.\n",
      "[I 2024-10-24 17:53:44,159] Trial 0 finished with value: 2593.961558571944 and parameters: {'depth': 4, 'l2_leaf_reg': 0.10663969374261167, 'learning_rate': 0.6521525550088616, 'iterations': 2}. Best is trial 4 with value: 1.6260695056013044.\n",
      "[I 2024-10-24 17:53:44,377] Trial 11 finished with value: 1.6966358758358175 and parameters: {'depth': 7, 'l2_leaf_reg': 0.003177171969590314, 'learning_rate': 0.00023819625256104606, 'iterations': 9}. Best is trial 4 with value: 1.6260695056013044.\n",
      "[I 2024-10-24 17:53:44,533] Trial 13 finished with value: 1.3907868134551584 and parameters: {'depth': 5, 'l2_leaf_reg': 0.050508898647401386, 'learning_rate': 0.01301971981031317, 'iterations': 48}. Best is trial 13 with value: 1.3907868134551584.\n",
      "[I 2024-10-24 17:53:44,607] Trial 12 finished with value: 1.6360202604066703 and parameters: {'depth': 7, 'l2_leaf_reg': 0.00500880467135448, 'learning_rate': 0.001051443119858849, 'iterations': 26}. Best is trial 13 with value: 1.3907868134551584.\n",
      "[I 2024-10-24 17:53:44,609] Trial 14 finished with value: 1.5871776848634316 and parameters: {'depth': 10, 'l2_leaf_reg': 700.5659009498963, 'learning_rate': 0.6718970023663171, 'iterations': 3}. Best is trial 13 with value: 1.3907868134551584.\n",
      "[I 2024-10-24 17:53:44,630] Trial 10 finished with value: 1.6495217969388734 and parameters: {'depth': 5, 'l2_leaf_reg': 0.0005075553444801218, 'learning_rate': 0.00061475983796058, 'iterations': 75}. Best is trial 13 with value: 1.3907868134551584.\n",
      "[I 2024-10-24 17:53:44,914] Trial 5 finished with value: 4.106655935320609 and parameters: {'depth': 2, 'l2_leaf_reg': 0.17333520014646686, 'learning_rate': 0.20448322310248918, 'iterations': 353}. Best is trial 13 with value: 1.3907868134551584.\n",
      "[I 2024-10-24 17:53:45,328] Trial 7 finished with value: 1.2322089842785302 and parameters: {'depth': 2, 'l2_leaf_reg': 0.015611388024921331, 'learning_rate': 0.03746801099712669, 'iterations': 596}. Best is trial 7 with value: 1.2322089842785302.\n",
      "[I 2024-10-24 17:53:48,933] Trial 3 finished with value: 1.597955222017673 and parameters: {'depth': 10, 'l2_leaf_reg': 7.16173486531682, 'learning_rate': 0.01173568174070356, 'iterations': 35}. Best is trial 7 with value: 1.2322089842785302.\n",
      "[I 2024-10-24 17:54:01,285] Trial 8 finished with value: 1.5283991680906013 and parameters: {'depth': 8, 'l2_leaf_reg': 0.15966972771063193, 'learning_rate': 0.0007994269153487063, 'iterations': 519}. Best is trial 7 with value: 1.2322089842785302.\n",
      "[I 2024-10-24 17:54:08,346] Trial 2 finished with value: 2507256.9956639805 and parameters: {'depth': 8, 'l2_leaf_reg': 0.2206791703425896, 'learning_rate': 0.0978467858804811, 'iterations': 743}. Best is trial 7 with value: 1.2322089842785302.\n"
     ]
    }
   ],
   "source": [
    "# Optimize quantile regression\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective_2, n_trials=15, n_jobs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "6733ea0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence values for test set\n",
    "y_conf = detailed_objective2(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a2189947",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_pred = {}\n",
    "\n",
    "gaussian_pred[\"score\"] = y_score_test\n",
    "gaussian_pred[\"mu\"] = y_conf[:, 0]\n",
    "gaussian_pred[\"log_sigma\"] = y_conf[:, 1]\n",
    "gaussian_pred[\"membership\"] = membership\n",
    "\n",
    "gaussian_pred = pd.DataFrame(gaussian_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "6ba71640",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "QMIA Privacy Accuracy: 0.6200512674451724\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Set the threshold as `mu`, and classify based on whether the actual score exceeds `mu`\n",
    "predicted_membership = [\"in\" if score > mu else \"out\" for score, mu in zip(gaussian_pred[\"score\"], gaussian_pred[\"mu\"])]\n",
    "\n",
    "# Step 2: Calculate the accuracy by comparing the predicted membership with the true membership\n",
    "true_membership = gaussian_pred[\"membership\"]\n",
    "\n",
    "\n",
    "true_membership_numeric = [1 if m == \"in\" else 0 for m in true_membership]\n",
    "predicted_membership_numeric = [1 if m == \"in\" else 0 for m in predicted_membership]\n",
    "\n",
    "qmia_accuracy = accuracy_score(true_membership_numeric, predicted_membership_numeric)\n",
    "\n",
    "\n",
    "print(f\"QMIA Privacy Accuracy: {qmia_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "6a4a658b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Privileged group with favorable outcome MIA accuracy: 0.6246290801186943\n",
      "Privileged group with unfavorable outcome MIA accuracy: 0.18543046357615894\n",
      "Unprivileged group with favorable outcome MIA accuracy: 0.6548010768770566\n",
      "Unprivileged group with unfavorable outcome MIA accuracy: 0.20253164556962025\n"
     ]
    }
   ],
   "source": [
    "def calculate_mia_accuracy_for_subpopulation(group_indices, gaussian_pred):\n",
    "    sub_gaussian_pred = gaussian_pred.iloc[group_indices]\n",
    "    predicted_membership = [\"in\" if score > mu else \"out\" for score, mu in zip(sub_gaussian_pred[\"score\"], sub_gaussian_pred[\"mu\"])]\n",
    "    predicted_membership_numeric = [1 if member == \"in\" else 0 for member in predicted_membership]\n",
    "    true_membership_numeric = [1 if member == \"in\" else 0 for member in sub_gaussian_pred[\"membership\"]]\n",
    "    return accuracy_score(true_membership_numeric, predicted_membership_numeric)\n",
    "\n",
    "# Calculate subpopulation indices within test set\n",
    "priv_fav_indices = np.where((test_dataset.protected_attributes.ravel() == 1) & (test_dataset.labels.ravel() == 1))[0]\n",
    "priv_unfav_indices = np.where((test_dataset.protected_attributes.ravel() == 1) & (test_dataset.labels.ravel() == 0))[0]\n",
    "unpriv_fav_indices = np.where((test_dataset.protected_attributes.ravel() == 0) & (test_dataset.labels.ravel() == 1))[0]\n",
    "unpriv_unfav_indices = np.where((test_dataset.protected_attributes.ravel() == 0) & (test_dataset.labels.ravel() == 0))[0]\n",
    "\n",
    "# Calculate MIA accuracy for each subpopulation within the test set\n",
    "priv_fav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(priv_fav_indices, gaussian_pred)\n",
    "priv_unfav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(priv_unfav_indices, gaussian_pred)\n",
    "unpriv_fav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(unpriv_fav_indices, gaussian_pred)\n",
    "unpriv_unfav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(unpriv_unfav_indices, gaussian_pred)\n",
    "\n",
    "# Print MIA accuracy for each subpopulation\n",
    "print(f\"Privileged group with favorable outcome MIA accuracy: {priv_fav_mia_accuracy}\")\n",
    "print(f\"Privileged group with unfavorable outcome MIA accuracy: {priv_unfav_mia_accuracy}\")\n",
    "print(f\"Unprivileged group with favorable outcome MIA accuracy: {unpriv_fav_mia_accuracy}\")\n",
    "print(f\"Unprivileged group with unfavorable outcome MIA accuracy: {unpriv_unfav_mia_accuracy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "254a84d6",
   "metadata": {},
   "source": [
    "# Applying Reweighing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4e9a19d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                        privileged_groups=privileged_groups)\n",
    "\n",
    "dataset_reweigh_train = RW.fit_transform(dataset_orig_train)\n",
    "dataset = dataset_reweigh_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bb113cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.5, random_state=42, stratify=y)\n",
    "X_val, X_test_us, y_val, y_test_us = train_test_split(X_test, y_test, test_size=0.2, random_state=42, stratify=y_test)\n",
    "X_test = np.concatenate((X_train, X_test_us), axis=0)\n",
    "y_test = np.concatenate((y_train, y_test_us), axis=0)\n",
    "membership = [\"in\"] * X_train.shape[0] + [\"out\"] * X_test_us.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "98f5870c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for the test dataset to recreate the BinaryLabelDataset\n",
    "df_test = pd.DataFrame(X_test, columns=dataset.feature_names)\n",
    "df_test['pass_bar'] = y_test  # Add the label\n",
    "# df_test['race'] = protected_test  # Add the protected attribute\n",
    "\n",
    "# Recreate the BinaryLabelDataset for the test set\n",
    "test_dataset = BinaryLabelDataset(\n",
    "    favorable_label=1,\n",
    "    unfavorable_label=0,\n",
    "    df=df_test,\n",
    "    label_names=['pass_bar'],  # Label column name\n",
    "    protected_attribute_names=['gender']  # Protected attribute column name\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c37201f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 17:55:09,999] A new study created in memory with name: no-name-1a54a793-25f8-4c93-9cd3-b6cc991a82ef\n",
      "[I 2024-10-24 17:55:10,119] Trial 0 finished with value: 0.8554796918767507 and parameters: {'depth': 4, 'l2_leaf_reg': 6.490379319662036, 'learning_rate': 0.4480671897016578, 'random_strength': 1.586130635772114, 'objective': 'Logloss', 'iterations': 6}. Best is trial 0 with value: 0.8554796918767507.\n",
      "[I 2024-10-24 17:55:10,505] Trial 1 finished with value: 0.7733630952380953 and parameters: {'depth': 10, 'l2_leaf_reg': 2.086861066683219, 'learning_rate': 0.26886050121442395, 'random_strength': 6.70966404877289, 'objective': 'CrossEntropy', 'iterations': 8}. Best is trial 0 with value: 0.8554796918767507.\n",
      "[I 2024-10-24 17:55:10,617] Trial 2 finished with value: 0.790765056022409 and parameters: {'depth': 1, 'l2_leaf_reg': 2.7934175635840566, 'learning_rate': 0.10680198456588656, 'random_strength': 6.683896428266283, 'objective': 'Logloss', 'iterations': 6}. Best is trial 0 with value: 0.8554796918767507.\n",
      "[I 2024-10-24 17:55:11,157] Trial 3 finished with value: 0.8308473389355742 and parameters: {'depth': 7, 'l2_leaf_reg': 4.746668006960559, 'learning_rate': 0.5403012450028118, 'random_strength': 1.0901915229942531, 'objective': 'Logloss', 'iterations': 60}. Best is trial 0 with value: 0.8554796918767507.\n",
      "[I 2024-10-24 17:55:11,273] Trial 4 finished with value: 0.802170868347339 and parameters: {'depth': 3, 'l2_leaf_reg': 1.2670612091134008, 'learning_rate': 0.5101196551061029, 'random_strength': 8.032031242569115, 'objective': 'CrossEntropy', 'iterations': 3}. Best is trial 0 with value: 0.8554796918767507.\n",
      "[I 2024-10-24 17:55:11,399] Trial 5 finished with value: 0.7774684873949579 and parameters: {'depth': 7, 'l2_leaf_reg': 1.1458765221074982, 'learning_rate': 0.010970590030978219, 'random_strength': 2.7402479923584946, 'objective': 'CrossEntropy', 'iterations': 2}. Best is trial 0 with value: 0.8554796918767507.\n",
      "[I 2024-10-24 17:55:13,281] Trial 6 finished with value: 0.7625000000000001 and parameters: {'depth': 3, 'l2_leaf_reg': 3.4733786570268825, 'learning_rate': 0.3381206916287489, 'random_strength': 3.8227680911413717, 'objective': 'Logloss', 'iterations': 654}. Best is trial 0 with value: 0.8554796918767507.\n",
      "[I 2024-10-24 17:55:15,690] Trial 7 finished with value: 0.843172268907563 and parameters: {'depth': 8, 'l2_leaf_reg': 1.456195147829887, 'learning_rate': 0.01857643476415631, 'random_strength': 2.8896212764096467, 'objective': 'Logloss', 'iterations': 220}. Best is trial 0 with value: 0.8554796918767507.\n",
      "[I 2024-10-24 17:55:15,904] Trial 8 finished with value: 0.8043767507002801 and parameters: {'depth': 6, 'l2_leaf_reg': 7.985704986446738, 'learning_rate': 0.08105568798839537, 'random_strength': 7.060320655589887, 'objective': 'Logloss', 'iterations': 22}. Best is trial 0 with value: 0.8554796918767507.\n",
      "[I 2024-10-24 17:55:16,322] Trial 9 finished with value: 0.8814775910364147 and parameters: {'depth': 6, 'l2_leaf_reg': 2.5307303151530993, 'learning_rate': 0.25580646898110904, 'random_strength': 3.1510069091227675, 'objective': 'CrossEntropy', 'iterations': 80}. Best is trial 9 with value: 0.8814775910364147.\n",
      "[I 2024-10-24 17:55:19,569] Trial 10 finished with value: 0.833718487394958 and parameters: {'depth': 10, 'l2_leaf_reg': 2.006036936559694, 'learning_rate': 0.10571229522763254, 'random_strength': 4.282732796984569, 'objective': 'CrossEntropy', 'iterations': 95}. Best is trial 9 with value: 0.8814775910364147.\n",
      "[I 2024-10-24 17:55:19,698] Trial 11 finished with value: 0.7120448179271709 and parameters: {'depth': 4, 'l2_leaf_reg': 9.538042171782369, 'learning_rate': 0.9892977784000933, 'random_strength': 1.5620809892805443, 'objective': 'CrossEntropy', 'iterations': 1}. Best is trial 9 with value: 0.8814775910364147.\n",
      "[I 2024-10-24 17:55:19,893] Trial 12 finished with value: 0.8376838235294118 and parameters: {'depth': 5, 'l2_leaf_reg': 6.1942956639296485, 'learning_rate': 0.2138783903544403, 'random_strength': 1.8824928016353846, 'objective': 'CrossEntropy', 'iterations': 18}. Best is trial 9 with value: 0.8814775910364147.\n",
      "[I 2024-10-24 17:55:20,066] Trial 13 finished with value: 0.8589285714285714 and parameters: {'depth': 1, 'l2_leaf_reg': 4.213874655984189, 'learning_rate': 0.04171673384745679, 'random_strength': 1.8715487894531417, 'objective': 'Logloss', 'iterations': 60}. Best is trial 9 with value: 0.8814775910364147.\n",
      "[I 2024-10-24 17:55:20,258] Trial 14 finished with value: 0.8523721988795517 and parameters: {'depth': 1, 'l2_leaf_reg': 2.988752863176669, 'learning_rate': 0.04379683970190518, 'random_strength': 2.1928207845097836, 'objective': 'Logloss', 'iterations': 76}. Best is trial 9 with value: 0.8814775910364147.\n"
     ]
    }
   ],
   "source": [
    "# Optimize using Optuna\n",
    "study = optuna.create_study(direction=\"maximize\")\n",
    "study.optimize(objective, n_trials=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "4d89426c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get scores for validation and test set\n",
    "y_score_val, y_score_test = detailed_objective(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "a76523a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply log-odds transformation\n",
    "f_scores = lambda prob, l: (np.log(prob) - np.log(1 - prob)) * (2 * l - 1)\n",
    "y_score_test = f_scores(y_score_test, y_test)\n",
    "y_score_val = f_scores(y_score_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2c447e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2024-10-24 17:55:27,661] A new study created in memory with name: no-name-a08ea94c-a931-4205-868e-344ff397f2e9\n",
      "[I 2024-10-24 17:55:28,205] Trial 1 finished with value: 2.0328573534836782 and parameters: {'depth': 4, 'l2_leaf_reg': 1346.5976601682673, 'learning_rate': 0.00024337163417264274, 'iterations': 3}. Best is trial 1 with value: 2.0328573534836782.\n",
      "[I 2024-10-24 17:55:29,249] Trial 7 finished with value: 2.0445144871980627 and parameters: {'depth': 4, 'l2_leaf_reg': 0.002672616240208124, 'learning_rate': 0.002148936323338968, 'iterations': 2}. Best is trial 1 with value: 2.0328573534836782.\n",
      "[I 2024-10-24 17:55:29,256] Trial 14 finished with value: 2.0384238034747657 and parameters: {'depth': 7, 'l2_leaf_reg': 7.099771821360429, 'learning_rate': 0.00021729931719790475, 'iterations': 8}. Best is trial 1 with value: 2.0328573534836782.\n",
      "[I 2024-10-24 17:55:29,272] Trial 13 finished with value: 2.0355251686160125 and parameters: {'depth': 2, 'l2_leaf_reg': 0.0002533573494158136, 'learning_rate': 0.0003994904857517038, 'iterations': 1}. Best is trial 1 with value: 2.0328573534836782.\n",
      "[I 2024-10-24 17:55:29,344] Trial 6 finished with value: 1.6026116786980287 and parameters: {'depth': 5, 'l2_leaf_reg': 15.338616968949308, 'learning_rate': 0.3648638882197001, 'iterations': 74}. Best is trial 6 with value: 1.6026116786980287.\n",
      "[I 2024-10-24 17:55:29,387] Trial 0 finished with value: 2.060848866696843 and parameters: {'depth': 5, 'l2_leaf_reg': 0.4612054693077801, 'learning_rate': 0.0007428354483433856, 'iterations': 8}. Best is trial 6 with value: 1.6026116786980287.\n",
      "[I 2024-10-24 17:55:29,387] Trial 10 finished with value: 2.0610658441510954 and parameters: {'depth': 5, 'l2_leaf_reg': 4120.397761320798, 'learning_rate': 0.00019563746872631042, 'iterations': 4}. Best is trial 6 with value: 1.6026116786980287.\n",
      "[I 2024-10-24 17:55:29,432] Trial 12 finished with value: 2.0482052422075983 and parameters: {'depth': 8, 'l2_leaf_reg': 0.14446347523382483, 'learning_rate': 0.0009533424270935317, 'iterations': 5}. Best is trial 6 with value: 1.6026116786980287.\n",
      "[I 2024-10-24 17:55:29,432] Trial 5 finished with value: 2.0433173763674715 and parameters: {'depth': 7, 'l2_leaf_reg': 0.7149419224789749, 'learning_rate': 0.00032307144706184324, 'iterations': 6}. Best is trial 6 with value: 1.6026116786980287.\n",
      "[I 2024-10-24 17:55:29,495] Trial 11 finished with value: 2.0261728961531453 and parameters: {'depth': 7, 'l2_leaf_reg': 387.19587706403416, 'learning_rate': 0.11963567643540338, 'iterations': 2}. Best is trial 6 with value: 1.6026116786980287.\n",
      "[I 2024-10-24 17:55:29,860] Trial 3 finished with value: 1.9977509605172916 and parameters: {'depth': 6, 'l2_leaf_reg': 79.7208537622004, 'learning_rate': 0.0007580504748986568, 'iterations': 79}. Best is trial 6 with value: 1.6026116786980287.\n",
      "[I 2024-10-24 17:55:30,085] Trial 8 finished with value: 2.0063470061418207 and parameters: {'depth': 1, 'l2_leaf_reg': 0.12206636884280779, 'learning_rate': 0.0002571692167417554, 'iterations': 551}. Best is trial 6 with value: 1.6026116786980287.\n",
      "[I 2024-10-24 17:55:32,319] Trial 2 finished with value: 1.948602262536314 and parameters: {'depth': 4, 'l2_leaf_reg': 62.875068617061906, 'learning_rate': 0.00043003726210423304, 'iterations': 589}. Best is trial 6 with value: 1.6026116786980287.\n"
     ]
    }
   ],
   "source": [
    "# Optimize quantile regression\n",
    "study = optuna.create_study(direction=\"minimize\", pruner=optuna.pruners.HyperbandPruner())\n",
    "study.optimize(objective_2, n_trials=15, n_jobs=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb8c0e15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get confidence values for test set\n",
    "y_conf = detailed_objective2(study.best_trial)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc73fdda",
   "metadata": {},
   "outputs": [],
   "source": [
    "gaussian_pred = {}\n",
    "\n",
    "gaussian_pred[\"score\"] = y_score_test\n",
    "gaussian_pred[\"mu\"] = y_conf[:, 0]\n",
    "gaussian_pred[\"log_sigma\"] = y_conf[:, 1]\n",
    "gaussian_pred[\"membership\"] = membership\n",
    "\n",
    "gaussian_pred = pd.DataFrame(gaussian_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f49fec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Set the threshold as `mu`, and classify based on whether the actual score exceeds `mu`\n",
    "predicted_membership = [\"in\" if score > mu else \"out\" for score, mu in zip(gaussian_pred[\"score\"], gaussian_pred[\"mu\"])]\n",
    "\n",
    "# Step 2: Calculate the accuracy by comparing the predicted membership with the true membership\n",
    "true_membership = gaussian_pred[\"membership\"]\n",
    "\n",
    "\n",
    "true_membership_numeric = [1 if m == \"in\" else 0 for m in true_membership]\n",
    "predicted_membership_numeric = [1 if m == \"in\" else 0 for m in predicted_membership]\n",
    "\n",
    "qmia_accuracy = accuracy_score(true_membership_numeric, predicted_membership_numeric)\n",
    "\n",
    "\n",
    "print(f\"QMIA Privacy Accuracy: {qmia_accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "650f7db8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_mia_accuracy_for_subpopulation(group_indices, gaussian_pred):\n",
    "    sub_gaussian_pred = gaussian_pred.iloc[group_indices]\n",
    "    predicted_membership = [\"in\" if score > mu else \"out\" for score, mu in zip(sub_gaussian_pred[\"score\"], sub_gaussian_pred[\"mu\"])]\n",
    "    predicted_membership_numeric = [1 if member == \"in\" else 0 for member in predicted_membership]\n",
    "    true_membership_numeric = [1 if member == \"in\" else 0 for member in sub_gaussian_pred[\"membership\"]]\n",
    "    return accuracy_score(true_membership_numeric, predicted_membership_numeric)\n",
    "\n",
    "# Calculate subpopulation indices within test set\n",
    "priv_fav_indices = np.where((test_dataset.protected_attributes.ravel() == 1) & (test_dataset.labels.ravel() == 1))[0]\n",
    "priv_unfav_indices = np.where((test_dataset.protected_attributes.ravel() == 1) & (test_dataset.labels.ravel() == 0))[0]\n",
    "unpriv_fav_indices = np.where((test_dataset.protected_attributes.ravel() == 0) & (test_dataset.labels.ravel() == 1))[0]\n",
    "unpriv_unfav_indices = np.where((test_dataset.protected_attributes.ravel() == 0) & (test_dataset.labels.ravel() == 0))[0]\n",
    "\n",
    "# Calculate MIA accuracy for each subpopulation within the test set\n",
    "priv_fav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(priv_fav_indices, gaussian_pred)\n",
    "priv_unfav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(priv_unfav_indices, gaussian_pred)\n",
    "unpriv_fav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(unpriv_fav_indices, gaussian_pred)\n",
    "unpriv_unfav_mia_accuracy = calculate_mia_accuracy_for_subpopulation(unpriv_unfav_indices, gaussian_pred)\n",
    "\n",
    "# Print MIA accuracy for each subpopulation\n",
    "print(f\"Privileged group with favorable outcome MIA accuracy: {priv_fav_mia_accuracy}\")\n",
    "print(f\"Privileged group with unfavorable outcome MIA accuracy: {priv_unfav_mia_accuracy}\")\n",
    "print(f\"Unprivileged group with favorable outcome MIA accuracy: {unpriv_fav_mia_accuracy}\")\n",
    "print(f\"Unprivileged group with unfavorable outcome MIA accuracy: {unpriv_unfav_mia_accuracy}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
