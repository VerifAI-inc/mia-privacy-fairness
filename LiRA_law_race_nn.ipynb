{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7dc9ed90",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import gc\n",
    "import random\n",
    "import warnings; warnings.filterwarnings(\"ignore\")\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import special\n",
    "from typing import Optional\n",
    "\n",
    "# Scikit-learn and AIF360\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.over_sampling import ADASYN\n",
    "from aif360.datasets import BinaryLabelDataset\n",
    "from aif360.metrics import utils, BinaryLabelDatasetMetric, ClassificationMetric\n",
    "\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "\n",
    "\n",
    "# TensorFlow and Privacy Tools\n",
    "import tensorflow as tf\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import (\n",
    "    advanced_mia as amia,\n",
    "    membership_inference_attack as mia,\n",
    ")\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack.data_structures import AttackInputData\n",
    "from tensorflow_privacy.privacy.privacy_tests.membership_inference_attack import plotting as mia_plotting\n",
    "\n",
    "# Fairness-related Pre-/In-processing\n",
    "from aif360.algorithms.preprocessing import DisparateImpactRemover, LFR, OptimPreproc, Reweighing\n",
    "from fairlearn.reductions import EqualizedOdds, ExponentiatedGradient\n",
    "from aif360.sklearn.inprocessing import ExponentiatedGradientReduction\n",
    "from data_utils import DatasetBuilder\n",
    "\n",
    "# Plotting\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# DP\n",
    "from diffprivlib.models import RandomForestClassifier as RandomForestClassifierDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "278da60c",
   "metadata": {},
   "outputs": [],
   "source": [
    "###############################################\n",
    "#   1. Utility Functions (loss & statistic)   #\n",
    "###############################################\n",
    "def log_loss(labels: np.ndarray,\n",
    "             pred: np.ndarray,\n",
    "             sample_weight: Optional[np.ndarray] = None,\n",
    "             from_logits: bool = False,\n",
    "             small_value: float = 1e-8) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the per-example cross-entropy loss.\n",
    "    \"\"\"\n",
    "    if labels.shape[0] != pred.shape[0]:\n",
    "        raise ValueError('Mismatch between labels and predictions.')\n",
    "    if sample_weight is None:\n",
    "        sample_weight = 1.0\n",
    "    else:\n",
    "        if np.shape(sample_weight)[0] != np.shape(labels)[0]:\n",
    "            raise ValueError('Sample weights and labels must have the same length.')\n",
    "\n",
    "    if pred.size == pred.shape[0]:\n",
    "        pred = pred.flatten()\n",
    "        if from_logits:\n",
    "            pred = special.expit(pred)\n",
    "        indices_class0 = (labels == 0)\n",
    "        prob_correct = np.copy(pred)\n",
    "        prob_correct[indices_class0] = 1 - prob_correct[indices_class0]\n",
    "        return -np.log(np.maximum(prob_correct, small_value)) * sample_weight\n",
    "\n",
    "    if from_logits:\n",
    "        pred = special.softmax(pred, axis=-1)\n",
    "    return -np.log(np.maximum(pred[np.arange(labels.size), labels], small_value)) * sample_weight\n",
    "\n",
    "\n",
    "def calculate_statistic(probabilities: np.ndarray,\n",
    "                        labels: np.ndarray,\n",
    "                        sample_weight: Optional[np.ndarray] = None,\n",
    "                        convert_to_prob: bool = False) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Calculates, for each example, the probability assigned to the true class.\n",
    "    \"\"\"\n",
    "    if convert_to_prob:\n",
    "        probabilities = special.softmax(probabilities, axis=-1)\n",
    "    stat = probabilities[np.arange(labels.size), labels]\n",
    "    if sample_weight is not None:\n",
    "        stat *= sample_weight\n",
    "    return stat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "11cd6929",
   "metadata": {},
   "outputs": [],
   "source": [
    "# #####################################\n",
    "# #   2. Data Loading & Preprocessing #\n",
    "# #####################################\n",
    "# def load_bank_dataset():\n",
    "#     \"\"\"\n",
    "#     Loads the preprocessed bank dataset and returns:\n",
    "#       - features X,\n",
    "#       - labels y,\n",
    "#       - index of protected attribute,\n",
    "#       - the BinaryLabelDataset (AIF360),\n",
    "#       - the original dataframe,\n",
    "#       - protected attribute name,\n",
    "#       - label name.\n",
    "#     \"\"\"\n",
    "#     df = pd.read_csv('./data/bank_preprocessed.csv')\n",
    "#     if 'Unnamed: 0' in df.columns:\n",
    "#         df = df.drop(columns=['Unnamed: 0'])\n",
    "#     protected_attribute_name = 'age'\n",
    "#     label_name = 'y'\n",
    "    \n",
    "#     dataset_binary = BinaryLabelDataset(\n",
    "#         favorable_label=0,\n",
    "#         unfavorable_label=1,\n",
    "#         df=df,\n",
    "#         label_names=[label_name],\n",
    "#         protected_attribute_names=[protected_attribute_name]\n",
    "#     )\n",
    "    \n",
    "#     X = dataset_binary.features\n",
    "#     y = dataset_binary.labels.ravel().astype(int)\n",
    "#     protected_attribute_index = df.columns.get_loc(protected_attribute_name)\n",
    "#     return X, y, protected_attribute_index, dataset_binary, df, protected_attribute_name, label_name\n",
    "\n",
    "def load_dataset(dataset):\n",
    "    dataset_builder =  DatasetBuilder(dataset)\n",
    "    dataset_binary = dataset_builder.load_data()\n",
    "    \n",
    "    scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "    dataset_binary.features = scaler.fit_transform(dataset_binary.features)  \n",
    "    \n",
    "    X = dataset_binary.features\n",
    "    y = dataset_binary.labels.ravel().astype(int)\n",
    "    \n",
    "    df = dataset_binary.convert_to_dataframe()[0]\n",
    "    protected_attribute_name, label_name = dataset_binary.protected_attribute_names[0], dataset_binary.label_names[0]\n",
    "    protected_attribute_index = df.columns.get_loc(protected_attribute_name)\n",
    "    \n",
    "    return X, y, protected_attribute_index, dataset_binary, df, protected_attribute_name, label_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f7b76414",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "#         3. Model Builders         #\n",
    "#####################################\n",
    "def weighted_resample(X, y, weights):\n",
    "    \"\"\"\n",
    "    Resample X and y with replacement according to the provided weights.\n",
    "    \"\"\"\n",
    "    weights = weights / np.sum(weights)\n",
    "    n_samples = len(y)\n",
    "    indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=True, p=weights)\n",
    "    return X[indices], y[indices]\n",
    "\n",
    "class MLPClassifierWithWeightWrapper(BaseEstimator, ClassifierMixin):\n",
    "    def __init__(self, hidden_layer_sizes=(64, 32), activation='relu',\n",
    "                 solver='adam', alpha=1e-4, learning_rate='adaptive',\n",
    "                 max_iter=500, random_state=42, early_stopping=True):\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "        self.activation = activation\n",
    "        self.solver = solver\n",
    "        self.alpha = alpha\n",
    "        self.learning_rate = learning_rate\n",
    "        self.max_iter = max_iter\n",
    "        self.random_state = random_state\n",
    "        self.early_stopping = early_stopping\n",
    "        self.model_ = MLPClassifier(\n",
    "            hidden_layer_sizes=self.hidden_layer_sizes,\n",
    "            activation=self.activation,\n",
    "            solver=self.solver,\n",
    "            alpha=self.alpha,\n",
    "            learning_rate=self.learning_rate,\n",
    "            max_iter=self.max_iter,\n",
    "            random_state=self.random_state,\n",
    "            early_stopping=self.early_stopping\n",
    "        )\n",
    "\n",
    "    def fit(self, X, y, sample_weight=None):\n",
    "        # If sample_weight is provided, use weighted resampling.\n",
    "        if sample_weight is not None:\n",
    "            X, y = weighted_resample(np.array(X), np.array(y), sample_weight)\n",
    "        self.model_.fit(X, y)\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return self.model_.predict(X)\n",
    "\n",
    "    def predict_proba(self, X):\n",
    "        return self.model_.predict_proba(X)\n",
    "\n",
    "\n",
    "\n",
    "# def scikit_learn_model():\n",
    "#     # Create a standard random forest classifier with 100 trees and max depth of 10.\n",
    "#     model = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "#     return model\n",
    "\n",
    "def scikit_learn_model():\n",
    "    model = MLPClassifier(\n",
    "    hidden_layer_sizes=(64,32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=1e-4,\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True\n",
    ")\n",
    "    return model\n",
    "\n",
    "def scikit_learn_model_dp():\n",
    "    model = MLPClassifier(\n",
    "    hidden_layer_sizes=(64,32),\n",
    "    activation='relu',\n",
    "    solver='adam',\n",
    "    alpha=1e-4,\n",
    "    learning_rate='adaptive',\n",
    "    max_iter=500,\n",
    "    random_state=42,\n",
    "    early_stopping=True\n",
    ")\n",
    "    return model\n",
    "\n",
    "# def scikit_learn_model_dp():\n",
    "#     model = RandomForestClassifierDP(\n",
    "#         n_estimators=100,\n",
    "#         max_depth=5,\n",
    "#         epsilon=100.0,        \n",
    "#         bounds=(0, 1),  \n",
    "#         random_state=42,\n",
    "#         classes=np.unique(y)\n",
    "#     )\n",
    "#     return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "188c324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 4. Statistics and Loss Extraction Functions\n",
    "##############################################\n",
    "def get_stat_and_loss_tabular(model, x, y, batch_size=256, use_proba: bool = True):\n",
    "    \"\"\"\n",
    "    Compute statistics and losses.\n",
    "      - If use_proba is True then we assume a scikit-learn model (using predict_proba).\n",
    "      - Otherwise (e.g. for neural nets) we use model.predict with an optional softmax conversion.\n",
    "    \"\"\"\n",
    "    if use_proba:\n",
    "        prob = model.predict_proba(x)\n",
    "    else:\n",
    "        prob = model.predict(x, batch_size=batch_size)\n",
    "        if prob.shape[1] > 1:\n",
    "            prob = special.softmax(prob, axis=-1)\n",
    "    losses = log_loss(y, prob)\n",
    "    stats = calculate_statistic(prob, y)\n",
    "    return np.expand_dims(stats, axis=1), np.expand_dims(losses, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "15412198",
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################\n",
    "# 5. Accuracy & Metric Calculation Methods #\n",
    "############################################\n",
    "def calculate_subpopulation_accuracies(X_combined, y_combined, protected_attribute_index, model):\n",
    "    results = {}\n",
    "    if isinstance(X_combined, pd.DataFrame):\n",
    "        prot_col = X_combined.columns[protected_attribute_index]\n",
    "        subgroups = {\n",
    "            'Privileged Favorable': ((X_combined[prot_col] == 1) & (y_combined == 1)),\n",
    "            'Unprivileged Favorable': ((X_combined[prot_col] == 0) & (y_combined == 1)),\n",
    "            'Unprivileged Unfavorable': ((X_combined[prot_col] == 0) & (y_combined == 0)),\n",
    "            'Privileged Unfavorable': ((X_combined[prot_col] == 1) & (y_combined == 0)),\n",
    "        }\n",
    "        for group_name, condition in subgroups.items():\n",
    "            subgroup_indices = np.where(condition)[0]\n",
    "            X_subgroup = X_combined.iloc[subgroup_indices]\n",
    "            y_subgroup = np.array(y_combined)[subgroup_indices]\n",
    "            predictions = model.predict(X_subgroup)\n",
    "            accuracy = accuracy_score(y_subgroup, predictions)\n",
    "            results[group_name] = accuracy\n",
    "    else:\n",
    "        # If it's a numpy array, assume the protected attribute is at position protected_attribute_index\n",
    "        subgroups = {\n",
    "            'Privileged Favorable': ((X_combined[:, protected_attribute_index] == 1) & (y_combined == 1)),\n",
    "            'Unprivileged Favorable': ((X_combined[:, protected_attribute_index] == 0) & (y_combined == 1)),\n",
    "            'Unprivileged Unfavorable': ((X_combined[:, protected_attribute_index] == 0) & (y_combined == 0)),\n",
    "            'Privileged Unfavorable': ((X_combined[:, protected_attribute_index] == 1) & (y_combined == 0)),\n",
    "        }\n",
    "        for group_name, condition in subgroups.items():\n",
    "            subgroup_indices = np.where(condition)[0]\n",
    "            X_subgroup = X_combined[subgroup_indices]\n",
    "            y_subgroup = y_combined[subgroup_indices]\n",
    "            predictions = model.predict(X_subgroup)\n",
    "            accuracy = accuracy_score(y_subgroup, predictions)\n",
    "            results[group_name] = accuracy\n",
    "    return results\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_metrics(X_test, y_test, y_pred, protected_attribute_index):\n",
    "    \"\"\"\n",
    "    Calculate fairness and performance metrics using AIF360's ClassificationMetric.\n",
    "    \"\"\"\n",
    "    # Create dataframes with features named as \"feature_i\"\n",
    "    num_features = X_test.shape[1]\n",
    "    feature_names = [f'feature_{i}' for i in range(num_features)]\n",
    "    df_true = pd.DataFrame(X_test, columns=feature_names)\n",
    "    df_true['label'] = y_test\n",
    "    df_pred = pd.DataFrame(X_test, columns=feature_names)\n",
    "    df_pred['label'] = y_pred\n",
    "\n",
    "    dataset_true = BinaryLabelDataset(\n",
    "        favorable_label=1,\n",
    "        unfavorable_label=0,\n",
    "        df=df_true,\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=[f'feature_{protected_attribute_index}']\n",
    "    )\n",
    "    dataset_pred = BinaryLabelDataset(\n",
    "        favorable_label=1,\n",
    "        unfavorable_label=0,\n",
    "        df=df_pred,\n",
    "        label_names=['label'],\n",
    "        protected_attribute_names=[f'feature_{protected_attribute_index}']\n",
    "    )\n",
    "\n",
    "    classification_metric = ClassificationMetric(\n",
    "        dataset_true,\n",
    "        dataset_pred,\n",
    "        unprivileged_groups=[{f'feature_{protected_attribute_index}': 0}],\n",
    "        privileged_groups=[{f'feature_{protected_attribute_index}': 1}]\n",
    "    )\n",
    "    \n",
    "    balanced_accuracy = (classification_metric.sensitivity() + classification_metric.specificity()) / 2\n",
    "    metrics = {\n",
    "        'balanced_accuracy': balanced_accuracy,\n",
    "        'average_odds_difference': classification_metric.average_odds_difference(),\n",
    "        'disparate_impact': (1 - min((classification_metric.disparate_impact()),\n",
    "                                      1 / classification_metric.disparate_impact())),\n",
    "        'statistical_parity_difference': classification_metric.statistical_parity_difference(),\n",
    "        'equal_opportunity_difference': classification_metric.equal_opportunity_difference(),\n",
    "        'theil_index': classification_metric.theil_index()\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "def print_mean_accuracies(accuracies_train, accuracies_test, train_subpop, test_subpop):\n",
    "    \"\"\"\n",
    "    Print overall and subpopulation mean accuracies.\n",
    "    \"\"\"\n",
    "    mean_train_overall = np.mean(accuracies_train)\n",
    "    mean_test_overall = np.mean(accuracies_test)\n",
    "    print(\"Mean Train Accuracy (Overall):\", mean_train_overall)\n",
    "    print(\"Mean Test Accuracy (Overall):\", mean_test_overall)\n",
    "    print('-------------------')\n",
    "    mean_train_subpop = {key: np.mean([sub[key] for sub in train_subpop])\n",
    "                         for key in train_subpop[0].keys()}\n",
    "    mean_test_subpop = {key: np.mean([sub[key] for sub in test_subpop])\n",
    "                        for key in test_subpop[0].keys()}\n",
    "    print(\"Mean Train Accuracy (Subpopulations):\")\n",
    "    for key, value in mean_train_subpop.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print(\"\\nMean Test Accuracy (Subpopulations):\")\n",
    "    for key, value in mean_test_subpop.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b31a4ca5",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 6. Membership Inference Attack Functions   #\n",
    "##############################################\n",
    "def perform_mia(in_indices, stats, losses, num_shadows=5):\n",
    "    \"\"\"\n",
    "    For each model (treated as the target), use the other models as shadows to perform the LiRA attack.\n",
    "    Returns the AUC scores (one per model) and the overall mean.\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    for idx in range(num_shadows + 1):\n",
    "        stat_target = stats[idx]\n",
    "        in_indices_target = in_indices[idx]\n",
    "        # Exclude target model from shadow models\n",
    "        stat_shadow = np.array(stats[:idx] + stats[idx + 1:])\n",
    "        in_indices_shadow = np.array(in_indices[:idx] + in_indices[idx + 1:])\n",
    "        # For each example in the target model, gather in/out stats from shadows\n",
    "        stat_in = [stat_shadow[:, j][in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        stat_out = [stat_shadow[:, j][~in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        scores = amia.compute_score_lira(stat_target, stat_in, stat_out, fix_variance=True)\n",
    "        attack_input = AttackInputData(\n",
    "            loss_train=scores[in_indices_target],\n",
    "            loss_test=scores[~in_indices_target]\n",
    "        )\n",
    "        result_lira = mia.run_attacks(attack_input).single_attack_results[0]\n",
    "        results.append(result_lira.get_auc())\n",
    "    return np.round(results, 6), np.round(np.mean(results), 6)\n",
    "\n",
    "\n",
    "def perform_mia_on_subgroups(X_combined, y_combined, protected_attr,\n",
    "                             in_indices, stats, losses, num_shadows=5):\n",
    "    \"\"\"\n",
    "    Perform MIA for subgroups. For numpy arrays, protected_attr should be an integer index.\n",
    "    For DataFrames, it can be a column name.\n",
    "    \"\"\"\n",
    "    results_dict = {}\n",
    "    # Define subgroup conditions based on type of X_combined\n",
    "    if isinstance(X_combined, np.ndarray):\n",
    "        subgroups = {\n",
    "            'Privileged Favorable': ((X_combined[:, protected_attr] == 1) & (y_combined == 1)),\n",
    "            'Unprivileged Favorable': ((X_combined[:, protected_attr] == 0) & (y_combined == 1)),\n",
    "            'Unprivileged Unfavorable': ((X_combined[:, protected_attr] == 0) & (y_combined == 0)),\n",
    "            'Privileged Unfavorable': ((X_combined[:, protected_attr] == 1) & (y_combined == 0)),\n",
    "        }\n",
    "    else:\n",
    "        # Assuming X_combined is a DataFrame\n",
    "        subgroups = {\n",
    "            'Privileged Favorable': ((X_combined[protected_attr] == 1) & (y_combined == 1)),\n",
    "            'Unprivileged Favorable': ((X_combined[protected_attr] == 0) & (y_combined == 1)),\n",
    "            'Unprivileged Unfavorable': ((X_combined[protected_attr] == 0) & (y_combined == 0)),\n",
    "            'Privileged Unfavorable': ((X_combined[protected_attr] == 1) & (y_combined == 0)),\n",
    "        }\n",
    "    print(\"Results for subgroup:\\n\")\n",
    "    for group_name, condition in subgroups.items():\n",
    "        subgroup_indices = np.where(condition)[0]\n",
    "        subgroup_in_indices = [arr[subgroup_indices] for arr in in_indices]\n",
    "        subgroup_stat = [arr[subgroup_indices] for arr in stats]\n",
    "        subgroup_losses = [arr[subgroup_indices] for arr in losses]\n",
    "        mia_results, mia_mean = perform_mia(subgroup_in_indices, subgroup_stat, subgroup_losses, num_shadows=num_shadows)\n",
    "        results_dict[group_name] = mia_mean\n",
    "        print(f\"{group_name}: {mia_results}\\nMean: {mia_mean}\")\n",
    "        print('---------------------')\n",
    "    return results_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "96421e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#####################################\n",
    "# 7. Oversampling/Synthetic Methods #\n",
    "#####################################\n",
    "def group_indices(dataset, unprivileged_groups):\n",
    "    \"\"\"\n",
    "    Returns indices of examples in the unprivileged and privileged groups.\n",
    "    \"\"\"\n",
    "    feature_names = dataset.feature_names\n",
    "    cond_vec = utils.compute_boolean_conditioning_vector(dataset.features, feature_names, unprivileged_groups)\n",
    "    indices = [i for i, x in enumerate(cond_vec) if x]\n",
    "    priv_indices = [i for i, x in enumerate(cond_vec) if not x]\n",
    "    return indices, priv_indices\n",
    "\n",
    "\n",
    "def balance(dataset, n_extra, inflate_rate, f_label, uf_label):\n",
    "    \"\"\"\n",
    "    Oversample one of the groups using ADASYN and then select extra samples.\n",
    "    \"\"\"\n",
    "    dataset_transf_train = dataset.copy(deepcopy=True)\n",
    "    f_indices = np.where(dataset.labels == f_label)[0].tolist()\n",
    "    uf_indices = np.where(dataset.labels == uf_label)[0].tolist()\n",
    "    f_dataset = dataset.subset(f_indices)\n",
    "    uf_dataset = dataset.subset(uf_indices)\n",
    "    \n",
    "    inflated_uf_features = np.repeat(uf_dataset.features, inflate_rate, axis=0)\n",
    "    sample_features = np.concatenate((f_dataset.features, inflated_uf_features))\n",
    "    inflated_uf_labels = np.repeat(uf_dataset.labels, inflate_rate, axis=0)\n",
    "    sample_labels = np.concatenate((f_dataset.labels, inflated_uf_labels))\n",
    "    \n",
    "    oversample = ADASYN(sampling_strategy='minority')\n",
    "    X, y = oversample.fit_resample(sample_features, sample_labels)\n",
    "    y = y.reshape(-1, 1)\n",
    "    # Only keep samples of f_label (favorable)\n",
    "    X = X[np.where(y == f_label)[0].tolist()]\n",
    "    y = y[y == f_label]\n",
    "    selected = int(f_dataset.features.shape[0] + n_extra)\n",
    "    X = X[:selected, :]\n",
    "    y = y[:selected]\n",
    "    y = y.reshape(-1, 1)\n",
    "    \n",
    "    # Set new instance weights and protected attributes for extra samples\n",
    "    instance_weights_list = (f_dataset.instance_weights.flatten().tolist()\n",
    "                             if isinstance(f_dataset.instance_weights, np.ndarray)\n",
    "                             else f_dataset.instance_weights)\n",
    "    protected_attributes_list = (f_dataset.protected_attributes.flatten().tolist()\n",
    "                                 if isinstance(f_dataset.protected_attributes, np.ndarray)\n",
    "                                 else f_dataset.protected_attributes)\n",
    "    inc = X.shape[0] - f_dataset.features.shape[0]\n",
    "    new_weights = [random.choice(instance_weights_list) for _ in range(inc)]\n",
    "    new_attributes = np.array([random.choice(protected_attributes_list) for _ in range(inc)]).reshape(-1, 1)\n",
    "    \n",
    "    dataset_transf_train.features = np.concatenate((uf_dataset.features, X))\n",
    "    dataset_transf_train.labels = np.concatenate((uf_dataset.labels, y))\n",
    "    dataset_transf_train.instance_weights = np.concatenate((uf_dataset.instance_weights, f_dataset.instance_weights, new_weights))\n",
    "    dataset_transf_train.protected_attributes = np.concatenate((uf_dataset.protected_attributes, f_dataset.protected_attributes, new_attributes))\n",
    "    \n",
    "    # Also create an extra dataset with just the new samples\n",
    "    dataset_extra_train = dataset.copy()\n",
    "    X_ex = X[-int(n_extra):]\n",
    "    y_ex = y[-int(n_extra):].reshape(-1, 1)\n",
    "    new_weights = [random.choice(instance_weights_list) for _ in range(int(n_extra))]\n",
    "    new_attributes = np.array([random.choice(protected_attributes_list) for _ in range(int(n_extra))]).reshape(-1, 1)\n",
    "    dataset_extra_train.features = X_ex\n",
    "    dataset_extra_train.labels = y_ex\n",
    "    dataset_extra_train.instance_weights = new_weights\n",
    "    dataset_extra_train.protected_attributes = new_attributes\n",
    "    return dataset_transf_train, dataset_extra_train\n",
    "\n",
    "\n",
    "def synthetic_balance(dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.0):\n",
    "    \"\"\"\n",
    "    Oversample the unprivileged group so that the number of favorable samples matches that of the privileged group.\n",
    "    \"\"\"\n",
    "    dataset_transf_train = dataset.copy(deepcopy=True)\n",
    "    indices, priv_indices = group_indices(dataset, unprivileged_groups)\n",
    "    unprivileged_dataset = dataset.subset(indices)\n",
    "    privileged_dataset = dataset.subset(priv_indices)\n",
    "    n_unpriv_favor = np.count_nonzero(unprivileged_dataset.labels == f_label)\n",
    "    n_unpriv_unfavor = np.count_nonzero(unprivileged_dataset.labels != f_label)\n",
    "    n_priv_favor = np.count_nonzero(privileged_dataset.labels == f_label)\n",
    "    \n",
    "    if n_unpriv_favor < n_priv_favor:\n",
    "        n_extra_sample = (n_priv_favor - n_unpriv_favor) * sampling_strategy\n",
    "        if n_extra_sample + n_unpriv_favor >= n_unpriv_unfavor:\n",
    "            inflate_rate = int(((n_extra_sample + n_unpriv_favor) / n_unpriv_unfavor) + 1)\n",
    "        else:\n",
    "            inflate_rate = round(((n_extra_sample + n_unpriv_favor) / n_unpriv_unfavor) + 1)\n",
    "        _, extra_favored = balance(unprivileged_dataset, n_extra_sample, inflate_rate, f_label, uf_label)\n",
    "        \n",
    "        n_extra_sample = (n_extra_sample + n_unpriv_favor - bp * (n_extra_sample + n_unpriv_favor + n_unpriv_unfavor)) / bp\n",
    "        if n_extra_sample + n_unpriv_unfavor >= n_unpriv_favor:\n",
    "            inflate_rate = int(((n_extra_sample + n_unpriv_unfavor) / n_unpriv_favor) + 1)\n",
    "        else:\n",
    "            inflate_rate = round(((n_extra_sample + n_unpriv_unfavor) / n_unpriv_favor) + 1)\n",
    "        _, extra_unfavored = balance(unprivileged_dataset, n_extra_sample, inflate_rate, uf_label, f_label)\n",
    "        \n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, extra_favored.features, extra_unfavored.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, extra_favored.labels, extra_unfavored.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, extra_favored.instance_weights, extra_unfavored.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, extra_favored.protected_attributes, extra_unfavored.protected_attributes))\n",
    "    return dataset_transf_train\n",
    "\n",
    "\n",
    "def synthetic_favor_unpriv(dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.0):\n",
    "    \"\"\"\n",
    "    Oversample favorable examples in the unprivileged group.\n",
    "    \"\"\"\n",
    "    indices, priv_indices = group_indices(dataset, unprivileged_groups)\n",
    "    unprivileged_dataset = dataset.subset(indices)\n",
    "    privileged_dataset = dataset.subset(priv_indices)\n",
    "    n_unpriv_favor = np.count_nonzero(unprivileged_dataset.labels == f_label)\n",
    "    n_unpriv_unfavor = np.count_nonzero(unprivileged_dataset.labels != f_label)\n",
    "    n_extra_sample = (bp * len(indices) - n_unpriv_favor) / (1 - bp) * sampling_strategy\n",
    "    if n_extra_sample + n_unpriv_favor >= n_unpriv_unfavor:\n",
    "        inflate_rate = int(((n_extra_sample + n_unpriv_favor) / n_unpriv_unfavor) + 1)\n",
    "    else:\n",
    "        inflate_rate = round(((n_extra_sample + n_unpriv_favor) / n_unpriv_unfavor) + 1)\n",
    "    _, extra_favored_unpriv = balance(unprivileged_dataset, n_extra_sample, inflate_rate, f_label, uf_label)\n",
    "    return unprivileged_dataset, extra_favored_unpriv\n",
    "\n",
    "\n",
    "def synthetic_unfavor_priv(dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.0):\n",
    "    \"\"\"\n",
    "    Oversample the unfavored examples in the privileged group.\n",
    "    \"\"\"\n",
    "    indices, priv_indices = group_indices(dataset, unprivileged_groups)\n",
    "    unprivileged_dataset = dataset.subset(indices)\n",
    "    privileged_dataset = dataset.subset(priv_indices)\n",
    "    n_priv_favor = np.count_nonzero(privileged_dataset.labels == f_label)\n",
    "    n_priv_unfavor = np.count_nonzero(privileged_dataset.labels != f_label)\n",
    "    n_extra_sample = (n_priv_favor - bnp * len(priv_indices)) / bnp * sampling_strategy\n",
    "    if n_extra_sample + n_priv_unfavor >= n_priv_favor:\n",
    "        inflate_rate = int(((n_extra_sample + n_priv_unfavor) / n_priv_favor) + 1)\n",
    "    else:\n",
    "        inflate_rate = round(((n_extra_sample + n_priv_unfavor) / n_priv_favor) + 1)\n",
    "    _, extra_unfavored_priv = balance(privileged_dataset, n_extra_sample, inflate_rate, uf_label, f_label)\n",
    "    return privileged_dataset, extra_unfavored_priv\n",
    "\n",
    "\n",
    "def synthetic(dataset, unprivileged_groups, bp, bnp, f_label, uf_label, os_mode=2, sampling_strategy=0.5):\n",
    "    \"\"\"\n",
    "    Depending on os_mode, perform one of the following oversampling methods:\n",
    "      1: Oversample unfavorable privileged.\n",
    "      2: Oversample favorable unprivileged.\n",
    "      3: Both.\n",
    "    If bp < bnp then use synthetic_balance.\n",
    "    \"\"\"\n",
    "    dataset_transf_train = dataset.copy(deepcopy=True)\n",
    "    if bp < bnp:\n",
    "        dataset_transf_train = synthetic_balance(dataset, unprivileged_groups, bp, bnp, f_label, uf_label)\n",
    "        return dataset_transf_train\n",
    "\n",
    "    if os_mode == 1:\n",
    "        _, sample_unfavor_priv = synthetic_unfavor_priv(dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.0)\n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, sample_unfavor_priv.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, sample_unfavor_priv.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, sample_unfavor_priv.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, sample_unfavor_priv.protected_attributes))\n",
    "    elif os_mode == 2:\n",
    "        _, sample_favor_unpriv = synthetic_favor_unpriv(dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.0)\n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, sample_favor_unpriv.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, sample_favor_unpriv.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, sample_favor_unpriv.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, sample_favor_unpriv.protected_attributes))\n",
    "    elif os_mode == 3:\n",
    "        _, sample_unfavor_priv = synthetic_unfavor_priv(dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.0)\n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, sample_unfavor_priv.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, sample_unfavor_priv.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, sample_unfavor_priv.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, sample_unfavor_priv.protected_attributes))\n",
    "        _, sample_favor_unpriv = synthetic_favor_unpriv(dataset, unprivileged_groups, bp, bnp, f_label, uf_label, sampling_strategy=1.0)\n",
    "        dataset_transf_train.features = np.concatenate((dataset_transf_train.features, sample_favor_unpriv.features))\n",
    "        dataset_transf_train.labels = np.concatenate((dataset_transf_train.labels, sample_favor_unpriv.labels))\n",
    "        dataset_transf_train.instance_weights = np.concatenate((dataset_transf_train.instance_weights, sample_favor_unpriv.instance_weights))\n",
    "        dataset_transf_train.protected_attributes = np.concatenate((dataset_transf_train.protected_attributes, sample_favor_unpriv.protected_attributes))\n",
    "    else:\n",
    "        sys.exit(\"Oversampling mode is missing: 1, 2, or 3 must be specified.\")\n",
    "    return dataset_transf_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83db43f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "##############################################\n",
    "# 8. Training Functions for Different Scenarios\n",
    "##############################################\n",
    "\n",
    "def train_orig(X, y, dataset_binary, protected_attribute_index, num_shadows=5,\n",
    "                             shadow_model_builder=scikit_learn_model, target_model_builder=scikit_learn_model_dp):\n",
    "\n",
    "    if target_model_builder is None:\n",
    "        raise ValueError(\"You must provide a target_model_builder function for the DP model.\")\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    overall_results = []\n",
    "    subgroup_results = {}\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    subpop_train_list = []\n",
    "    subpop_test_list = []\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Define subgroup conditions (for MIA on subpopulations)\n",
    "    subgroups = {\n",
    "        'Privileged Favorable': ((X[:, protected_attribute_index] == 1) & (y == 1)),\n",
    "        'Unprivileged Favorable': ((X[:, protected_attribute_index] == 0) & (y == 1)),\n",
    "        'Unprivileged Unfavorable': ((X[:, protected_attribute_index] == 0) & (y == 0)),\n",
    "        'Privileged Unfavorable': ((X[:, protected_attribute_index] == 1) & (y == 0)),\n",
    "    }\n",
    "    \n",
    "    # Loop: Each iteration, one model will act as the target and the others are shadow models.\n",
    "    for target_idx in range(num_shadows + 1):\n",
    "        print(f\"Training Model #{target_idx} as the Target\")\n",
    "        in_indices_list = []\n",
    "        stats, losses = [], []\n",
    "        \n",
    "        # For collecting target-only metrics\n",
    "        for i in range(num_shadows + 1):\n",
    "            indices = np.random.binomial(1, 0.5, n_samples).astype(bool)\n",
    "            in_indices_list.append(indices)\n",
    "            train_indices = indices\n",
    "            val_indices = ~indices\n",
    "            \n",
    "            \n",
    "            dataset_train = dataset_binary.subset(train_indices)\n",
    "            dataset_val = dataset_binary.subset(val_indices)\n",
    "         \n",
    "            X_train, y_train = dataset_train.features, dataset_train.labels.ravel()\n",
    "            X_val, y_val = dataset_val.features, dataset_val.labels.ravel()\n",
    "            \n",
    "            if i == target_idx:\n",
    "                # Build target model using DP builder\n",
    "                model = target_model_builder()\n",
    "            else:\n",
    "                # Shadow models use the original model builder\n",
    "                model = shadow_model_builder()\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # For the target model, record its performance\n",
    "            if i == target_idx:\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_test = model.predict(X_val)\n",
    "                train_accuracies.append(accuracy_score(y_train, pred_train))\n",
    "                test_accuracies.append(accuracy_score(y_val, pred_test))\n",
    "                met = get_metrics(X_val, y_val, pred_test, protected_attribute_index)\n",
    "                all_metrics.append(met)\n",
    "                subpop_train = calculate_subpopulation_accuracies(X_train, y_train, protected_attribute_index, model)\n",
    "                subpop_test = calculate_subpopulation_accuracies(X_val, y_val, protected_attribute_index, model)\n",
    "                subpop_train_list.append(subpop_train)\n",
    "                subpop_test_list.append(subpop_test)\n",
    "            \n",
    "            stat, loss = get_stat_and_loss_tabular(model, X, y, use_proba=True)\n",
    "            stats.append(stat)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        # Now perform MIA for the target model (using shadows)\n",
    "        print(f\"Performing MIA for Target Model #{target_idx}\")\n",
    "        stat_target = stats[target_idx]\n",
    "        in_indices_target = in_indices_list[target_idx]\n",
    "        stat_shadow = np.array([stats[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        in_indices_shadow = np.array([in_indices_list[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        stat_in = [stat_shadow[:, j][in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        stat_out = [stat_shadow[:, j][~in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        scores = amia.compute_score_lira(stat_target, stat_in, stat_out, fix_variance=True)\n",
    "        attack_input = AttackInputData(\n",
    "            loss_train=scores[in_indices_target],\n",
    "            loss_test=scores[~in_indices_target]\n",
    "        )\n",
    "        result_lira = mia.run_attacks(attack_input).single_attack_results[0]\n",
    "        overall_results.append(result_lira.get_auc())\n",
    "        \n",
    "        # MIA for subpopulations\n",
    "        for group_name, condition in subgroups.items():\n",
    "            subgroup_indices = np.where(condition)[0]\n",
    "            subgroup_in_indices = [arr[subgroup_indices] for arr in in_indices_list]\n",
    "            subgroup_stat = [arr[subgroup_indices] for arr in stats]\n",
    "            subgroup_stat_target = subgroup_stat[target_idx]\n",
    "            subgroup_in_indices_target = subgroup_in_indices[target_idx]\n",
    "            subgroup_stat_shadow = np.array([subgroup_stat[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_in_indices_shadow = np.array([subgroup_in_indices[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_stat_in = [subgroup_stat_shadow[:, j][subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_stat_out = [subgroup_stat_shadow[:, j][~subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_scores = amia.compute_score_lira(subgroup_stat_target, subgroup_stat_in, subgroup_stat_out, fix_variance=True)\n",
    "            subgroup_attack_input = AttackInputData(\n",
    "                loss_train=subgroup_scores[subgroup_in_indices_target],\n",
    "                loss_test=subgroup_scores[~subgroup_in_indices_target]\n",
    "            )\n",
    "            subgroup_result = mia.run_attacks(subgroup_attack_input).single_attack_results[0]\n",
    "            if group_name not in subgroup_results:\n",
    "                subgroup_results[group_name] = []\n",
    "            subgroup_results[group_name].append(subgroup_result.get_auc())\n",
    "    \n",
    "    overall_mean = np.round(np.mean(overall_results), 6)\n",
    "    subgroup_means = {group: np.round(np.mean(vals), 6) for group, vals in subgroup_results.items()}\n",
    "    \n",
    "    print(\"\\nOverall MIA Results:\")\n",
    "    print(f\"Results: {np.round(overall_results, 6)}\\nMean: {overall_mean}\")\n",
    "    print(\"\\nSubgroup MIA Results:\")\n",
    "    for group, results in subgroup_results.items():\n",
    "        print(f\"{group}: Results={np.round(results, 6)}, Mean={subgroup_means[group]}\")\n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    print(f\"Mean Train Accuracy (Overall): {np.mean(train_accuracies)}\")\n",
    "    print(f\"Mean Test Accuracy (Overall): {np.mean(test_accuracies)}\")\n",
    "    for group in subgroups:\n",
    "        mean_train_sub = np.mean([acc[group] for acc in subpop_train_list])\n",
    "        mean_test_sub = np.mean([acc[group] for acc in subpop_test_list])\n",
    "        print(f\"{group}: Mean Train Accuracy = {mean_train_sub}, Mean Test Accuracy = {mean_test_sub}\")\n",
    "    \n",
    "    return (overall_results, overall_mean, subgroup_results, subgroup_means,\n",
    "            train_accuracies, test_accuracies, subpop_train_list, subpop_test_list, all_metrics)\n",
    "\n",
    "def train_syn(X, y, dataset_binary, protected_attribute_index, num_shadows=5,\n",
    "                             shadow_model_builder=scikit_learn_model, target_model_builder=scikit_learn_model_dp):\n",
    "  \n",
    "    if target_model_builder is None:\n",
    "        raise ValueError(\"You must provide a target_model_builder function for the DP model.\")\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    overall_results = []\n",
    "    subgroup_results = {}\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    subpop_train_list = []\n",
    "    subpop_test_list = []\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Define subgroup conditions (for MIA on subpopulations)\n",
    "    subgroups = {\n",
    "        'Privileged Favorable': ((X[:, protected_attribute_index] == 1) & (y == 1)),\n",
    "        'Unprivileged Favorable': ((X[:, protected_attribute_index] == 0) & (y == 1)),\n",
    "        'Unprivileged Unfavorable': ((X[:, protected_attribute_index] == 0) & (y == 0)),\n",
    "        'Privileged Unfavorable': ((X[:, protected_attribute_index] == 1) & (y == 0)),\n",
    "    }\n",
    "    \n",
    "    # Loop: Each iteration, one model will act as the target and the others are shadow models.\n",
    "    for target_idx in range(num_shadows + 1):\n",
    "        print(f\"Training Model #{target_idx} as the Target\")\n",
    "        in_indices_list = []\n",
    "        stats, losses = [], []\n",
    "        \n",
    "        # For collecting target-only metrics\n",
    "        for i in range(num_shadows + 1):\n",
    "            indices = np.random.binomial(1, 0.5, n_samples).astype(bool)\n",
    "            in_indices_list.append(indices)\n",
    "            train_indices = indices\n",
    "            val_indices = ~indices\n",
    "            \n",
    "            # For the target model, we apply a synthetic transformation and use the DP model builder.\n",
    "            \n",
    "            dataset_train = dataset_binary.subset(train_indices)\n",
    "            dataset_val = dataset_binary.subset(val_indices)\n",
    "            # Apply your synthetic transformation if needed\n",
    "            transformed_dataset = synthetic(dataset_train,\n",
    "                                            unprivileged_groups,\n",
    "                                            base_rate_privileged_private,\n",
    "                                            base_rate_unprivileged_private,\n",
    "                                            f_label, uf_label, os_mode=2)\n",
    "            X_train, y_train = transformed_dataset.features, transformed_dataset.labels.ravel()\n",
    "            X_val, y_val = dataset_val.features, dataset_val.labels.ravel()\n",
    "            \n",
    "            if i == target_idx:\n",
    "                # Build target model using DP builder (which expects y_train)\n",
    "                model = target_model_builder()\n",
    "            else:\n",
    "                # Shadow models use the original model builder\n",
    "                model = shadow_model_builder()\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # For the target model, record its performance\n",
    "            if i == target_idx:\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_test = model.predict(X_val)\n",
    "                train_accuracies.append(accuracy_score(y_train, pred_train))\n",
    "                test_accuracies.append(accuracy_score(y_val, pred_test))\n",
    "                met = get_metrics(X_val, y_val, pred_test, protected_attribute_index)\n",
    "                all_metrics.append(met)\n",
    "                subpop_train = calculate_subpopulation_accuracies(X_train, y_train, protected_attribute_index, model)\n",
    "                subpop_test = calculate_subpopulation_accuracies(X_val, y_val, protected_attribute_index, model)\n",
    "                subpop_train_list.append(subpop_train)\n",
    "                subpop_test_list.append(subpop_test)\n",
    "            \n",
    "            stat, loss = get_stat_and_loss_tabular(model, X, y, use_proba=True)\n",
    "            stats.append(stat)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        # Now perform MIA for the target model (using shadows)\n",
    "        print(f\"Performing MIA for Target Model #{target_idx}\")\n",
    "        stat_target = stats[target_idx]\n",
    "        in_indices_target = in_indices_list[target_idx]\n",
    "        stat_shadow = np.array([stats[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        in_indices_shadow = np.array([in_indices_list[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        stat_in = [stat_shadow[:, j][in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        stat_out = [stat_shadow[:, j][~in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        scores = amia.compute_score_lira(stat_target, stat_in, stat_out, fix_variance=True)\n",
    "        attack_input = AttackInputData(\n",
    "            loss_train=scores[in_indices_target],\n",
    "            loss_test=scores[~in_indices_target]\n",
    "        )\n",
    "        result_lira = mia.run_attacks(attack_input).single_attack_results[0]\n",
    "        overall_results.append(result_lira.get_auc())\n",
    "        \n",
    "        # MIA for subpopulations\n",
    "        for group_name, condition in subgroups.items():\n",
    "            subgroup_indices = np.where(condition)[0]\n",
    "            subgroup_in_indices = [arr[subgroup_indices] for arr in in_indices_list]\n",
    "            subgroup_stat = [arr[subgroup_indices] for arr in stats]\n",
    "            subgroup_stat_target = subgroup_stat[target_idx]\n",
    "            subgroup_in_indices_target = subgroup_in_indices[target_idx]\n",
    "            subgroup_stat_shadow = np.array([subgroup_stat[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_in_indices_shadow = np.array([subgroup_in_indices[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_stat_in = [subgroup_stat_shadow[:, j][subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_stat_out = [subgroup_stat_shadow[:, j][~subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_scores = amia.compute_score_lira(subgroup_stat_target, subgroup_stat_in, subgroup_stat_out, fix_variance=True)\n",
    "            subgroup_attack_input = AttackInputData(\n",
    "                loss_train=subgroup_scores[subgroup_in_indices_target],\n",
    "                loss_test=subgroup_scores[~subgroup_in_indices_target]\n",
    "            )\n",
    "            subgroup_result = mia.run_attacks(subgroup_attack_input).single_attack_results[0]\n",
    "            if group_name not in subgroup_results:\n",
    "                subgroup_results[group_name] = []\n",
    "            subgroup_results[group_name].append(subgroup_result.get_auc())\n",
    "    \n",
    "    overall_mean = np.round(np.mean(overall_results), 6)\n",
    "    subgroup_means = {group: np.round(np.mean(vals), 6) for group, vals in subgroup_results.items()}\n",
    "    \n",
    "    print(\"\\nOverall MIA Results:\")\n",
    "    print(f\"Results: {np.round(overall_results, 6)}\\nMean: {overall_mean}\")\n",
    "    print(\"\\nSubgroup MIA Results:\")\n",
    "    for group, results in subgroup_results.items():\n",
    "        print(f\"{group}: Results={np.round(results, 6)}, Mean={subgroup_means[group]}\")\n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    print(f\"Mean Train Accuracy (Overall): {np.mean(train_accuracies)}\")\n",
    "    print(f\"Mean Test Accuracy (Overall): {np.mean(test_accuracies)}\")\n",
    "    for group in subgroups:\n",
    "        mean_train_sub = np.mean([acc[group] for acc in subpop_train_list])\n",
    "        mean_test_sub = np.mean([acc[group] for acc in subpop_test_list])\n",
    "        print(f\"{group}: Mean Train Accuracy = {mean_train_sub}, Mean Test Accuracy = {mean_test_sub}\")\n",
    "    \n",
    "    return (overall_results, overall_mean, subgroup_results, subgroup_means,\n",
    "            train_accuracies, test_accuracies, subpop_train_list, subpop_test_list, all_metrics)\n",
    "\n",
    "\n",
    "def train_syn_target(X, y, dataset_binary, protected_attribute_index, num_shadows=5,\n",
    "                             shadow_model_builder=scikit_learn_model, target_model_builder=scikit_learn_model_dp):\n",
    "    \"\"\"\n",
    "    Train a set of models where shadow models are trained with the original model builder and the target model\n",
    "    is trained using a different (DP) model builder.\n",
    "    \n",
    "    Parameters:\n",
    "      X, y: the data arrays\n",
    "      dataset_binary: the AIF360 dataset object\n",
    "      protected_attribute_index: index of the protected attribute in X\n",
    "      num_shadows: number of shadow models (plus one target)\n",
    "      shadow_model_builder: function that returns an instance of the original model (no arguments)\n",
    "      target_model_builder: function that returns an instance of the DP model; must accept training labels as input\n",
    "      \n",
    "    Returns:\n",
    "      overall_results, overall_mean, subgroup_results, subgroup_means, train_accuracies, test_accuracies,\n",
    "      subpop_train_list, subpop_test_list, all_metrics\n",
    "    \"\"\"\n",
    "    if target_model_builder is None:\n",
    "        raise ValueError(\"You must provide a target_model_builder function for the DP model.\")\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    overall_results = []\n",
    "    subgroup_results = {}\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    subpop_train_list = []\n",
    "    subpop_test_list = []\n",
    "    all_metrics = []\n",
    "    \n",
    "    # Define subgroup conditions (for MIA on subpopulations)\n",
    "    subgroups = {\n",
    "        'Privileged Favorable': ((X[:, protected_attribute_index] == 1) & (y == 1)),\n",
    "        'Unprivileged Favorable': ((X[:, protected_attribute_index] == 0) & (y == 1)),\n",
    "        'Unprivileged Unfavorable': ((X[:, protected_attribute_index] == 0) & (y == 0)),\n",
    "        'Privileged Unfavorable': ((X[:, protected_attribute_index] == 1) & (y == 0)),\n",
    "    }\n",
    "    \n",
    "    # Loop: Each iteration, one model will act as the target and the others are shadow models.\n",
    "    for target_idx in range(num_shadows + 1):\n",
    "        print(f\"Training Model #{target_idx} as the Target\")\n",
    "        in_indices_list = []\n",
    "        stats, losses = [], []\n",
    "        \n",
    "        # For collecting target-only metrics\n",
    "        for i in range(num_shadows + 1):\n",
    "            indices = np.random.binomial(1, 0.5, n_samples).astype(bool)\n",
    "            in_indices_list.append(indices)\n",
    "            train_indices = indices\n",
    "            val_indices = ~indices\n",
    "            \n",
    "            # For the target model, we apply a synthetic transformation and use the DP model builder.\n",
    "            if i == target_idx:\n",
    "                dataset_train = dataset_binary.subset(train_indices)\n",
    "                dataset_val = dataset_binary.subset(val_indices)\n",
    "                # Apply your synthetic transformation if needed\n",
    "                transformed_dataset = synthetic(dataset_train,\n",
    "                                                unprivileged_groups,\n",
    "                                                base_rate_privileged_private,\n",
    "                                                base_rate_unprivileged_private,\n",
    "                                                f_label, uf_label, os_mode=2)\n",
    "                X_train, y_train = transformed_dataset.features, transformed_dataset.labels.ravel()\n",
    "                X_val, y_val = dataset_val.features, dataset_val.labels.ravel()\n",
    "                # Build target model using DP builder (which expects y_train)\n",
    "                model = target_model_builder()\n",
    "            else:\n",
    "                # Shadow models use the original model builder and untransformed data.\n",
    "                X_train, y_train = X[train_indices], y[train_indices]\n",
    "                X_val, y_val = X[val_indices], y[val_indices]\n",
    "                model = shadow_model_builder()\n",
    "            \n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # For the target model, record its performance\n",
    "            if i == target_idx:\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_test = model.predict(X_val)\n",
    "                train_accuracies.append(accuracy_score(y_train, pred_train))\n",
    "                test_accuracies.append(accuracy_score(y_val, pred_test))\n",
    "                met = get_metrics(X_val, y_val, pred_test, protected_attribute_index)\n",
    "                all_metrics.append(met)\n",
    "                subpop_train = calculate_subpopulation_accuracies(X_train, y_train, protected_attribute_index, model)\n",
    "                subpop_test = calculate_subpopulation_accuracies(X_val, y_val, protected_attribute_index, model)\n",
    "                subpop_train_list.append(subpop_train)\n",
    "                subpop_test_list.append(subpop_test)\n",
    "            \n",
    "            stat, loss = get_stat_and_loss_tabular(model, X, y, use_proba=True)\n",
    "            stats.append(stat)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        # Now perform MIA for the target model (using shadows)\n",
    "        print(f\"Performing MIA for Target Model #{target_idx}\")\n",
    "        stat_target = stats[target_idx]\n",
    "        in_indices_target = in_indices_list[target_idx]\n",
    "        stat_shadow = np.array([stats[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        in_indices_shadow = np.array([in_indices_list[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        stat_in = [stat_shadow[:, j][in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        stat_out = [stat_shadow[:, j][~in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        scores = amia.compute_score_lira(stat_target, stat_in, stat_out, fix_variance=True)\n",
    "        attack_input = AttackInputData(\n",
    "            loss_train=scores[in_indices_target],\n",
    "            loss_test=scores[~in_indices_target]\n",
    "        )\n",
    "        result_lira = mia.run_attacks(attack_input).single_attack_results[0]\n",
    "        overall_results.append(result_lira.get_auc())\n",
    "        \n",
    "        # MIA for subpopulations\n",
    "        for group_name, condition in subgroups.items():\n",
    "            subgroup_indices = np.where(condition)[0]\n",
    "            subgroup_in_indices = [arr[subgroup_indices] for arr in in_indices_list]\n",
    "            subgroup_stat = [arr[subgroup_indices] for arr in stats]\n",
    "            subgroup_stat_target = subgroup_stat[target_idx]\n",
    "            subgroup_in_indices_target = subgroup_in_indices[target_idx]\n",
    "            subgroup_stat_shadow = np.array([subgroup_stat[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_in_indices_shadow = np.array([subgroup_in_indices[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_stat_in = [subgroup_stat_shadow[:, j][subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_stat_out = [subgroup_stat_shadow[:, j][~subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_scores = amia.compute_score_lira(subgroup_stat_target, subgroup_stat_in, subgroup_stat_out, fix_variance=True)\n",
    "            subgroup_attack_input = AttackInputData(\n",
    "                loss_train=subgroup_scores[subgroup_in_indices_target],\n",
    "                loss_test=subgroup_scores[~subgroup_in_indices_target]\n",
    "            )\n",
    "            subgroup_result = mia.run_attacks(subgroup_attack_input).single_attack_results[0]\n",
    "            if group_name not in subgroup_results:\n",
    "                subgroup_results[group_name] = []\n",
    "            subgroup_results[group_name].append(subgroup_result.get_auc())\n",
    "    \n",
    "    overall_mean = np.round(np.mean(overall_results), 6)\n",
    "    subgroup_means = {group: np.round(np.mean(vals), 6) for group, vals in subgroup_results.items()}\n",
    "    \n",
    "    print(\"\\nOverall MIA Results:\")\n",
    "    print(f\"Results: {np.round(overall_results, 6)}\\nMean: {overall_mean}\")\n",
    "    print(\"\\nSubgroup MIA Results:\")\n",
    "    for group, results in subgroup_results.items():\n",
    "        print(f\"{group}: Results={np.round(results, 6)}, Mean={subgroup_means[group]}\")\n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    print(f\"Mean Train Accuracy (Overall): {np.mean(train_accuracies)}\")\n",
    "    print(f\"Mean Test Accuracy (Overall): {np.mean(test_accuracies)}\")\n",
    "    for group in subgroups:\n",
    "        mean_train_sub = np.mean([acc[group] for acc in subpop_train_list])\n",
    "        mean_test_sub = np.mean([acc[group] for acc in subpop_test_list])\n",
    "        print(f\"{group}: Mean Train Accuracy = {mean_train_sub}, Mean Test Accuracy = {mean_test_sub}\")\n",
    "    \n",
    "    return (overall_results, overall_mean, subgroup_results, subgroup_means,\n",
    "            train_accuracies, test_accuracies, subpop_train_list, subpop_test_list, all_metrics)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def train_rew(X, y, dataset_binary, protected_attribute_index, num_shadows=5,\n",
    "              shadow_model_builder=scikit_learn_model, target_model_builder=scikit_learn_model_dp):\n",
    "\n",
    "    if target_model_builder is None:\n",
    "        raise ValueError(\"You must provide a target_model_builder function for the DP model.\")\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    overall_results = []\n",
    "    subgroup_results = {}\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    subpop_train_list = []\n",
    "    subpop_test_list = []\n",
    "    all_metrics = []\n",
    "    \n",
    "    def weighted_resample(X, y, weights):\n",
    "        \"\"\"\n",
    "        Resample the dataset (X, y) with replacement based on the provided weights.\n",
    "        Each sample is drawn with probability proportional to its weight.\n",
    "        \"\"\"\n",
    "        # Normalize weights so they sum to 1\n",
    "        weights = weights / np.sum(weights)\n",
    "        n_samples = len(y)\n",
    "        indices = np.random.choice(np.arange(n_samples), size=n_samples, replace=True, p=weights)\n",
    "        return X[indices], y[indices]\n",
    "\n",
    "\n",
    "    RW = Reweighing(unprivileged_groups=unprivileged_groups,\n",
    "                    privileged_groups=privileged_groups)\n",
    "    \n",
    "    # Define subgroup conditions (for MIA on subpopulations)\n",
    "    subgroups = {\n",
    "        'Privileged Favorable': ((X[:, protected_attribute_index] == 1) & (y == 1)),\n",
    "        'Unprivileged Favorable': ((X[:, protected_attribute_index] == 0) & (y == 1)),\n",
    "        'Unprivileged Unfavorable': ((X[:, protected_attribute_index] == 0) & (y == 0)),\n",
    "        'Privileged Unfavorable': ((X[:, protected_attribute_index] == 1) & (y == 0)),\n",
    "    }\n",
    "    \n",
    "    # Loop: Each iteration, one model will act as the target and the others are shadow models.\n",
    "    for target_idx in range(num_shadows + 1):\n",
    "        print(f\"Training Model #{target_idx} as the Target\")\n",
    "        in_indices_list = []\n",
    "        stats = []\n",
    "        losses = []\n",
    "        for i in range(num_shadows + 1):\n",
    "            indices = np.random.binomial(1, 0.5, n_samples).astype(bool)\n",
    "            in_indices_list.append(indices)\n",
    "            train_indices = indices\n",
    "            val_indices = ~indices\n",
    "            if i == target_idx:\n",
    "                # For the target model, apply reweighing and then perform weighted resampling\n",
    "                dataset_train = dataset_binary.subset(train_indices)\n",
    "                dataset_val = dataset_binary.subset(val_indices)\n",
    "                reweighted_dataset = RW.fit_transform(dataset_train)\n",
    "                X_train = reweighted_dataset.features\n",
    "                y_train = reweighted_dataset.labels.ravel().astype(int)\n",
    "                X_val = dataset_val.features\n",
    "                y_val = dataset_val.labels.ravel().astype(int)\n",
    "                # Use weighted resampling instead of sample_weight parameter\n",
    "                X_train_resampled, y_train_resampled = weighted_resample(X_train, y_train, reweighted_dataset.instance_weights)\n",
    "                model = target_model_builder()\n",
    "                model.fit(X_train_resampled, y_train_resampled)\n",
    "            else:\n",
    "                # For shadow models, simply use the original data split\n",
    "                X_train, y_train = X[train_indices], y[train_indices]\n",
    "                X_val, y_val = X[val_indices], y[val_indices]\n",
    "                model = shadow_model_builder()\n",
    "                model.fit(X_train, y_train)\n",
    "                \n",
    "            # For the target model, record its performance\n",
    "            if i == target_idx:\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_test = model.predict(X_val)\n",
    "                train_accuracies.append(accuracy_score(y_train, pred_train))\n",
    "                test_accuracies.append(accuracy_score(y_val, pred_test))\n",
    "                met = get_metrics(X_val, y_val, pred_test, protected_attribute_index)\n",
    "                all_metrics.append(met)\n",
    "                subpop_train = calculate_subpopulation_accuracies(X_train, y_train, protected_attribute_index, model)\n",
    "                subpop_test = calculate_subpopulation_accuracies(X_val, y_val, protected_attribute_index, model)\n",
    "                subpop_train_list.append(subpop_train)\n",
    "                subpop_test_list.append(subpop_test)\n",
    "            \n",
    "            stat, loss = get_stat_and_loss_tabular(model, X, y, use_proba=True)\n",
    "            stats.append(stat)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        # Now perform MIA for the target model (using shadows)\n",
    "        print(f\"Performing MIA for Target Model #{target_idx}\")\n",
    "        stat_target = stats[target_idx]\n",
    "        in_indices_target = in_indices_list[target_idx]\n",
    "        stat_shadow = np.array([stats[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        in_indices_shadow = np.array([in_indices_list[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        stat_in = [stat_shadow[:, j][in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        stat_out = [stat_shadow[:, j][~in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        scores = amia.compute_score_lira(stat_target, stat_in, stat_out, fix_variance=True)\n",
    "        attack_input = AttackInputData(\n",
    "            loss_train=scores[in_indices_target],\n",
    "            loss_test=scores[~in_indices_target]\n",
    "        )\n",
    "        result_lira = mia.run_attacks(attack_input).single_attack_results[0]\n",
    "        overall_results.append(result_lira.get_auc())\n",
    "        \n",
    "        # MIA for subpopulations\n",
    "        for group_name, condition in subgroups.items():\n",
    "            subgroup_indices = np.where(condition)[0]\n",
    "            subgroup_in_indices = [arr[subgroup_indices] for arr in in_indices_list]\n",
    "            subgroup_stat = [arr[subgroup_indices] for arr in stats]\n",
    "            subgroup_stat_target = subgroup_stat[target_idx]\n",
    "            subgroup_in_indices_target = subgroup_in_indices[target_idx]\n",
    "            subgroup_stat_shadow = np.array([subgroup_stat[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_in_indices_shadow = np.array([subgroup_in_indices[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_stat_in = [subgroup_stat_shadow[:, j][subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_stat_out = [subgroup_stat_shadow[:, j][~subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_scores = amia.compute_score_lira(subgroup_stat_target, subgroup_stat_in, subgroup_stat_out, fix_variance=True)\n",
    "            subgroup_attack_input = AttackInputData(\n",
    "                loss_train=subgroup_scores[subgroup_in_indices_target],\n",
    "                loss_test=subgroup_scores[~subgroup_in_indices_target]\n",
    "            )\n",
    "            subgroup_result = mia.run_attacks(subgroup_attack_input).single_attack_results[0]\n",
    "            if group_name not in subgroup_results:\n",
    "                subgroup_results[group_name] = []\n",
    "            subgroup_results[group_name].append(subgroup_result.get_auc())\n",
    "    \n",
    "    overall_mean = np.round(np.mean(overall_results), 6)\n",
    "    subgroup_means = {group: np.round(np.mean(vals), 6) for group, vals in subgroup_results.items()}\n",
    "    \n",
    "    print(\"\\nOverall MIA Results:\")\n",
    "    print(f\"Results: {np.round(overall_results, 6)}\\nMean: {overall_mean}\")\n",
    "    print(\"\\nSubgroup MIA Results:\")\n",
    "    for group, results in subgroup_results.items():\n",
    "        print(f\"{group}: Results={np.round(results, 6)}, Mean={subgroup_means[group]}\")\n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    print(f\"Mean Train Accuracy (Overall): {np.mean(train_accuracies)}\")\n",
    "    print(f\"Mean Test Accuracy (Overall): {np.mean(test_accuracies)}\")\n",
    "    for group in subgroups:\n",
    "        mean_train_sub = np.mean([acc[group] for acc in subpop_train_list])\n",
    "        mean_test_sub = np.mean([acc[group] for acc in subpop_test_list])\n",
    "        print(f\"{group}: Mean Train Accuracy = {mean_train_sub}, Mean Test Accuracy = {mean_test_sub}\")\n",
    "    \n",
    "    return (overall_results, overall_mean, subgroup_results, subgroup_means,\n",
    "            train_accuracies, test_accuracies, subpop_train_list, subpop_test_list, all_metrics)\n",
    "\n",
    "\n",
    "def train_eg(X, y, dataset_binary, protected_attribute_index, num_shadows=5,\n",
    "                             shadow_model_builder=scikit_learn_model, target_model_builder=scikit_learn_model_dp):\n",
    "\n",
    "    if target_model_builder is None:\n",
    "        raise ValueError(\"You must provide a target_model_builder function for the DP model.\")\n",
    "    \n",
    "    n_samples = X.shape[0]\n",
    "    overall_results = []\n",
    "    subgroup_results = {}\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "    subpop_train_list = []\n",
    "    subpop_test_list = []\n",
    "    all_metrics = []\n",
    "    \n",
    "    def target_model_builder():\n",
    "        return MLPClassifierWithWeightWrapper(\n",
    "            hidden_layer_sizes=(64, 32),\n",
    "            activation='relu',\n",
    "            solver='adam',\n",
    "            alpha=1e-4,\n",
    "            learning_rate='adaptive',\n",
    "            max_iter=500,\n",
    "            random_state=42,\n",
    "            early_stopping=True\n",
    "        )\n",
    "\n",
    "    \n",
    "    # Define subgroup conditions (for MIA on subpopulations)\n",
    "    subgroups = {\n",
    "            'Privileged Favorable': ((X[protected_attribute_name] == 1) & (y == 1)),\n",
    "            'Unprivileged Favorable': ((X[protected_attribute_name] == 0) & (y == 1)),\n",
    "            'Unprivileged Unfavorable': ((X[protected_attribute_name] == 0) & (y == 0)),\n",
    "            'Privileged Unfavorable': ((X[protected_attribute_name] == 1) & (y == 0)),\n",
    "        }\n",
    "    \n",
    "    # Loop: Each iteration, one model will act as the target and the others are shadow models.\n",
    "    for target_idx in range(num_shadows + 1):\n",
    "        print(f\"Training Model #{target_idx} as the Target\")\n",
    "        in_indices_list = []\n",
    "        stats = []\n",
    "        losses = []\n",
    "        for i in range(num_shadows + 1):\n",
    "            indices = np.random.binomial(1, 0.5, n_samples).astype(bool)\n",
    "            in_indices_list.append(indices)\n",
    "            X_train, y_train = X.iloc[indices], y[indices]\n",
    "            X_val, y_val = X.iloc[~indices], y[~indices]\n",
    "            if i == target_idx:\n",
    "                _model = target_model_builder()\n",
    "                constraint = EqualizedOdds(difference_bound=0.001)\n",
    "                model = ExponentiatedGradientReduction(prot_attr=protected_attribute_name,\n",
    "                                                   estimator=_model,\n",
    "                                                   constraints=constraint)\n",
    "                model.classes_ = np.unique(y)\n",
    "                model.model_ = model.estimator\n",
    "                \n",
    "            else:\n",
    "                model = shadow_model_builder()\n",
    "                \n",
    "            model.fit(X.iloc[indices], y[indices])\n",
    "            # For the target model, record its performance\n",
    "            if i == target_idx:\n",
    "                pred_train = model.predict(X_train)\n",
    "                pred_test = model.predict(X_val)\n",
    "                train_accuracies.append(accuracy_score(y_train, pred_train))\n",
    "                test_accuracies.append(accuracy_score(y_val, pred_test))\n",
    "                met = get_metrics(X_val.to_numpy(), y_val, pred_test, protected_attribute_index)\n",
    "                all_metrics.append(met)\n",
    "                subpop_train = calculate_subpopulation_accuracies(X_train, y_train, protected_attribute_index, model)\n",
    "                subpop_test = calculate_subpopulation_accuracies(X_val, y_val, protected_attribute_index, model)\n",
    "                subpop_train_list.append(subpop_train)\n",
    "                subpop_test_list.append(subpop_test)\n",
    "            \n",
    "            stat, loss = get_stat_and_loss_tabular(model, X, y, use_proba=True)\n",
    "            stats.append(stat)\n",
    "            losses.append(loss)\n",
    "        \n",
    "        # Now perform MIA for the target model (using shadows)\n",
    "        print(f\"Performing MIA for Target Model #{target_idx}\")\n",
    "        stat_target = stats[target_idx]\n",
    "        in_indices_target = in_indices_list[target_idx]\n",
    "        stat_shadow = np.array([stats[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        in_indices_shadow = np.array([in_indices_list[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "        stat_in = [stat_shadow[:, j][in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        stat_out = [stat_shadow[:, j][~in_indices_shadow[:, j]] for j in range(len(stat_target))]\n",
    "        scores = amia.compute_score_lira(stat_target, stat_in, stat_out, fix_variance=True)\n",
    "        attack_input = AttackInputData(\n",
    "            loss_train=scores[in_indices_target],\n",
    "            loss_test=scores[~in_indices_target]\n",
    "        )\n",
    "        result_lira = mia.run_attacks(attack_input).single_attack_results[0]\n",
    "        overall_results.append(result_lira.get_auc())\n",
    "        \n",
    "        # MIA for subpopulations\n",
    "        for group_name, condition in subgroups.items():\n",
    "            subgroup_indices = np.where(condition)[0]\n",
    "            subgroup_in_indices = [arr[subgroup_indices] for arr in in_indices_list]\n",
    "            subgroup_stat = [arr[subgroup_indices] for arr in stats]\n",
    "            subgroup_stat_target = subgroup_stat[target_idx]\n",
    "            subgroup_in_indices_target = subgroup_in_indices[target_idx]\n",
    "            subgroup_stat_shadow = np.array([subgroup_stat[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_in_indices_shadow = np.array([subgroup_in_indices[i] for i in range(num_shadows + 1) if i != target_idx])\n",
    "            subgroup_stat_in = [subgroup_stat_shadow[:, j][subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_stat_out = [subgroup_stat_shadow[:, j][~subgroup_in_indices_shadow[:, j]] for j in range(len(subgroup_stat_shadow[0]))]\n",
    "            subgroup_scores = amia.compute_score_lira(subgroup_stat_target, subgroup_stat_in, subgroup_stat_out, fix_variance=True)\n",
    "            subgroup_attack_input = AttackInputData(\n",
    "                loss_train=subgroup_scores[subgroup_in_indices_target],\n",
    "                loss_test=subgroup_scores[~subgroup_in_indices_target]\n",
    "            )\n",
    "            subgroup_result = mia.run_attacks(subgroup_attack_input).single_attack_results[0]\n",
    "            if group_name not in subgroup_results:\n",
    "                subgroup_results[group_name] = []\n",
    "            subgroup_results[group_name].append(subgroup_result.get_auc())\n",
    "    \n",
    "    overall_mean = np.round(np.mean(overall_results), 6)\n",
    "    subgroup_means = {group: np.round(np.mean(vals), 6) for group, vals in subgroup_results.items()}\n",
    "    \n",
    "    print(\"\\nOverall MIA Results:\")\n",
    "    print(f\"Results: {np.round(overall_results, 6)}\\nMean: {overall_mean}\")\n",
    "    print(\"\\nSubgroup MIA Results:\")\n",
    "    for group, results in subgroup_results.items():\n",
    "        print(f\"{group}: Results={np.round(results, 6)}, Mean={subgroup_means[group]}\")\n",
    "    print(\"\\nAccuracy Results:\")\n",
    "    print(f\"Mean Train Accuracy (Overall): {np.mean(train_accuracies)}\")\n",
    "    print(f\"Mean Test Accuracy (Overall): {np.mean(test_accuracies)}\")\n",
    "    for group in subgroups:\n",
    "        mean_train_sub = np.mean([acc[group] for acc in subpop_train_list])\n",
    "        mean_test_sub = np.mean([acc[group] for acc in subpop_test_list])\n",
    "        print(f\"{group}: Mean Train Accuracy = {mean_train_sub}, Mean Test Accuracy = {mean_test_sub}\")\n",
    "    \n",
    "    return (overall_results, overall_mean, subgroup_results, subgroup_means,\n",
    "            train_accuracies, test_accuracies, subpop_train_list, subpop_test_list, all_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d853c472",
   "metadata": {},
   "outputs": [],
   "source": [
    "def average_dicts(dict_list):\n",
    "    \"\"\"Given a list of dictionaries, return a Series with the average for each key.\"\"\"\n",
    "    return pd.DataFrame(dict_list).mean()\n",
    "\n",
    "def save_summary_tables(dataset, model):\n",
    "    \"\"\"\n",
    "    Merges and saves three summary tables as CSV files:\n",
    "      1. Merged Accuracies: Combines overall (mean train and test) and subpopulation accuracies.\n",
    "      2. Merged MIA: Combines the overall MIA score with subpopulation MIA scores.\n",
    "      3. Merged Fairness: A table of average fairness metrics per mitigator.\n",
    "\n",
    "    The function assumes that the following global variables exist:\n",
    "    \n",
    "      -- ACCURACIES --\n",
    "      accuracies_train_orig, accuracies_test_orig,\n",
    "      accuracies_train_syn, accuracies_test_syn,\n",
    "      accuracies_train_syn_target, accuracies_test_syn_target,\n",
    "      accuracies_train_dir, accuracies_test_dir,\n",
    "      accuracies_train_rew, accuracies_test_rew,\n",
    "      accuracies_train_egr, accuracies_test_egr,\n",
    "      \n",
    "      train_subpop_orig, test_subpop_orig,\n",
    "      train_subpop_syn, test_subpop_syn,\n",
    "      train_subpop_syn_target, test_subpop_syn_target,\n",
    "      train_subpop_dir, test_subpop_dir,\n",
    "      train_subpop_rew, test_subpop_rew,\n",
    "      train_subpop_egr, test_subpop_egr,\n",
    "      \n",
    "      -- MIA --\n",
    "      mia_orig, mia_syn, mia_syn_target, mia_dir, mia_rew, mia_egr,\n",
    "      results_mia_subpop_orig, results_mia_subpop_syn, results_mia_subpop_syn_target,\n",
    "      results_mia_subpop_dir, results_mia_subpop_rew, results_mia_subpop_egr,\n",
    "      \n",
    "      -- FAIRNESS --\n",
    "      all_metrics_orig, all_metrics_syn, all_metrics_syn_target,\n",
    "      all_metrics_dir, all_metrics_rew, all_metrics_egr.\n",
    "    \"\"\"\n",
    "    ### 1. Merged Accuracies Table ###\n",
    "    # Overall accuracies (mean of each experiment)\n",
    "    overall_acc = {\n",
    "        \"orig\": [np.mean(train_accuracies_orig), np.mean(test_accuracies_orig)],\n",
    "        \"syn\": [np.mean(train_accuracies_syn), np.mean(test_accuracies_syn)],\n",
    "        \"syn_target\": [np.mean(train_accuracies_syn_target), np.mean(test_accuracies_syn_target)],\n",
    "        \"dir\": [np.mean(train_accuracies_dir), np.mean(test_accuracies_dir)],\n",
    "        \"rew\": [np.mean(train_accuracies_rew), np.mean(test_accuracies_rew)],\n",
    "        \"egr\": [np.mean(train_accuracies_egr), np.mean(test_accuracies_egr)]\n",
    "    }\n",
    "\n",
    "    overall_acc_df = pd.DataFrame(overall_acc, index=[\"Overall Train Accuracy\", \"Overall Test Accuracy\"])\n",
    "    \n",
    "    # Average train subpopulation accuracies per experiment\n",
    "    train_subpop_agg = {\n",
    "        \"orig\": average_dicts(train_subpop_orig),\n",
    "        \"syn\": average_dicts(train_subpop_syn),\n",
    "        \"syn_target\": average_dicts(train_subpop_syn_target),\n",
    "        \"dir\": average_dicts(train_subpop_dir),\n",
    "        \"rew\": average_dicts(train_subpop_rew),\n",
    "        \"egr\": average_dicts(train_subpop_egr)\n",
    "    }\n",
    "    train_subpop_df = pd.DataFrame(train_subpop_agg)\n",
    "    # Prefix row labels with \"Train: \"\n",
    "    train_subpop_df.index = [\"Train: \" + str(idx) for idx in train_subpop_df.index]\n",
    "    \n",
    "    # Average test subpopulation accuracies per experiment\n",
    "    test_subpop_agg = {\n",
    "        \"orig\": average_dicts(test_subpop_orig),\n",
    "        \"syn\": average_dicts(test_subpop_syn),\n",
    "        \"syn_target\": average_dicts(test_subpop_syn_target),\n",
    "        \"dir\": average_dicts(test_subpop_dir),\n",
    "        \"rew\": average_dicts(test_subpop_rew),\n",
    "        \"egr\": average_dicts(test_subpop_egr)\n",
    "    }\n",
    "    test_subpop_df = pd.DataFrame(test_subpop_agg)\n",
    "    test_subpop_df.index = [\"Test: \" + str(idx) for idx in test_subpop_df.index]\n",
    "    \n",
    "    # Merge overall accuracies, train subpopulation, and test subpopulation vertically\n",
    "    accuracies_df = pd.concat([overall_acc_df, train_subpop_df, test_subpop_df], axis=0)\n",
    "    accuracies_df.index.name = \"Accuracy Metric\"\n",
    "    accuracies_df.to_csv(f\"new_results/lira_train_test_accuracies/lira_{dataset}_{model}_train_test_accuracies.csv\")\n",
    "    print(f\"Saved {dataset}_train_test_accuracies.csv\")\n",
    "    \n",
    "    ### 2. Merged MIA Table ###\n",
    "    # Overall MIA scores\n",
    "    mia_overall = {\n",
    "        \"orig\": mia_orig,\n",
    "        \"syn\": mia_syn,\n",
    "        \"syn_target\": mia_syn_target,\n",
    "        \"dir\": mia_dir,\n",
    "        \"rew\": mia_rew,\n",
    "        \"egr\": mia_egr\n",
    "    }\n",
    "    mia_overall_df = pd.DataFrame(mia_overall, index=[\"Overall MIA\"])\n",
    "    \n",
    "    # Subpopulation MIA scores (assumed to be dictionaries)\n",
    "    mia_subpop_dict = {\n",
    "        \"orig\": subgroup_means_orig,\n",
    "        \"syn\": subgroup_means_syn,\n",
    "        \"syn_target\": subgroup_means_syn_target,\n",
    "        \"dir\": subgroup_means_dir,\n",
    "        \"rew\": subgroup_means_rew,\n",
    "        \"egr\": subgroup_means_egr\n",
    "    }\n",
    "    mia_subpop_df = pd.DataFrame(mia_subpop_dict)\n",
    "    mia_subpop_df.index.name = \"Subpopulation\"\n",
    "    \n",
    "    # Merge overall and subpopulation MIA vertically\n",
    "    mia_df = pd.concat([mia_overall_df, mia_subpop_df], axis=0)\n",
    "    mia_df.to_csv(f\"new_results/lira_mia_results/lira_{dataset}_{model}_mia.csv\")\n",
    "    print(f\"Saved {dataset}_mia.csv\")\n",
    "    \n",
    "    ### 3. Merged Fairness Table ###\n",
    "    # For each experiment, average the fairness metrics (list of dictionaries) into one Series.\n",
    "    fairness_agg = {\n",
    "        \"orig\": average_dicts(all_metrics_orig),\n",
    "        \"syn\": average_dicts(all_metrics_syn),\n",
    "        \"syn_target\": average_dicts(all_metrics_syn_target),\n",
    "        \"dir\": average_dicts(all_metrics_dir),\n",
    "        \"rew\": average_dicts(all_metrics_rew),\n",
    "        \"egr\": average_dicts(all_metrics_egr)\n",
    "    }\n",
    "    fairness_df = pd.DataFrame(fairness_agg)\n",
    "    fairness_df.index.name = \"Fairness Metric\"\n",
    "    fairness_df.to_csv(f\"new_results/lira_fairness/lira_{dataset}_{model}_fairness.csv\")\n",
    "    print(f\"Saved {dataset}_train_test_accuracies.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "bc555bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y, protected_attribute_index, dataset_binary, df, protected_attribute_name, label_name = load_dataset('law_race')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "239032af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20798, 14)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ef64d30a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.01"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(df[(df[protected_attribute_name]==1) & (df[label_name]==0)])/len(df), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e1f59e3a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(df[(df[protected_attribute_name]==1) & (df[label_name]==1)])/len(df),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5da72262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.04"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(df[(df[protected_attribute_name]==0) & (df[label_name]==0)])/len(df),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a16757c8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "round(len(df[(df[protected_attribute_name]==0) & (df[label_name]==1)])/len(df),2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c69b19d8",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training on Original Data ===\n",
      "Training Model #0 as the Target\n",
      "Performing MIA for Target Model #0\n",
      "Training Model #1 as the Target\n",
      "Performing MIA for Target Model #1\n",
      "Training Model #2 as the Target\n",
      "Performing MIA for Target Model #2\n",
      "Training Model #3 as the Target\n",
      "Performing MIA for Target Model #3\n",
      "Training Model #4 as the Target\n",
      "Performing MIA for Target Model #4\n",
      "Training Model #5 as the Target\n",
      "Performing MIA for Target Model #5\n",
      "\n",
      "Overall MIA Results:\n",
      "Results: [0.505412 0.50115  0.507012 0.501294 0.504725 0.50341 ]\n",
      "Mean: 0.503834\n",
      "\n",
      "Subgroup MIA Results:\n",
      "Privileged Favorable: Results=[0.512867 0.497996 0.492662 0.491199 0.483745 0.506736], Mean=0.497534\n",
      "Unprivileged Favorable: Results=[0.504308 0.500792 0.507469 0.502234 0.502713 0.501078], Mean=0.503099\n",
      "Unprivileged Unfavorable: Results=[0.532701 0.499871 0.515467 0.522634 0.543646 0.52281 ], Mean=0.522855\n",
      "Privileged Unfavorable: Results=[0.545156 0.573226 0.617488 0.454242 0.528167 0.523187], Mean=0.540244\n",
      "\n",
      "Accuracy Results:\n",
      "Mean Train Accuracy (Overall): 0.9515455802924938\n",
      "Mean Test Accuracy (Overall): 0.9498382090120061\n",
      "Privileged Favorable: Mean Train Accuracy = 0.9951575451631443, Mean Test Accuracy = 0.9941711328183468\n",
      "Unprivileged Favorable: Mean Train Accuracy = 0.9989152970671902, Mean Test Accuracy = 0.9985674892490936\n",
      "Unprivileged Unfavorable: Mean Train Accuracy = 0.04038618847680653, Mean Test Accuracy = 0.03792081312193873\n",
      "Privileged Unfavorable: Mean Train Accuracy = 0.053306471723479916, Mean Test Accuracy = 0.059532919910278405\n",
      "\n",
      "=== Training with Synthetic Oversampling ===\n",
      "Training Model #0 as the Target\n",
      "Performing MIA for Target Model #0\n",
      "Training Model #1 as the Target\n",
      "Performing MIA for Target Model #1\n",
      "Training Model #2 as the Target\n",
      "Performing MIA for Target Model #2\n",
      "Training Model #3 as the Target\n",
      "Performing MIA for Target Model #3\n",
      "Training Model #4 as the Target\n",
      "Performing MIA for Target Model #4\n",
      "Training Model #5 as the Target\n",
      "Performing MIA for Target Model #5\n",
      "\n",
      "Overall MIA Results:\n",
      "Results: [0.4985   0.504128 0.50512  0.500241 0.498431 0.502377]\n",
      "Mean: 0.501466\n",
      "\n",
      "Subgroup MIA Results:\n",
      "Privileged Favorable: Results=[0.473164 0.474055 0.485923 0.522946 0.5253   0.526014], Mean=0.501234\n",
      "Unprivileged Favorable: Results=[0.498436 0.505648 0.503169 0.498044 0.498372 0.497454], Mean=0.500187\n",
      "Unprivileged Unfavorable: Results=[0.521033 0.50775  0.523417 0.502436 0.489019 0.537724], Mean=0.513563\n",
      "Privileged Unfavorable: Results=[0.502081 0.471382 0.520664 0.50327  0.439054 0.612079], Mean=0.508088\n",
      "\n",
      "Accuracy Results:\n",
      "Mean Train Accuracy (Overall): 0.9506122560772524\n",
      "Mean Test Accuracy (Overall): 0.9522984162473157\n",
      "Privileged Favorable: Mean Train Accuracy = 0.9943726691618345, Mean Test Accuracy = 0.992516353322189\n",
      "Unprivileged Favorable: Mean Train Accuracy = 0.9975576061664141, Mean Test Accuracy = 0.9973545373047433\n",
      "Unprivileged Unfavorable: Mean Train Accuracy = 0.089909578705959, Mean Test Accuracy = 0.07318270545687791\n",
      "Privileged Unfavorable: Mean Train Accuracy = 0.08059101768239856, Mean Test Accuracy = 0.06287279132106718\n",
      "\n",
      "=== Training with Synthetic Target ===\n",
      "Training Model #0 as the Target\n",
      "Performing MIA for Target Model #0\n",
      "Training Model #1 as the Target\n",
      "Performing MIA for Target Model #1\n",
      "Training Model #2 as the Target\n",
      "Performing MIA for Target Model #2\n",
      "Training Model #3 as the Target\n",
      "Performing MIA for Target Model #3\n",
      "Training Model #4 as the Target\n",
      "Performing MIA for Target Model #4\n",
      "Training Model #5 as the Target\n",
      "Performing MIA for Target Model #5\n",
      "\n",
      "Overall MIA Results:\n",
      "Results: [0.495184 0.505105 0.501604 0.501441 0.499413 0.497389]\n",
      "Mean: 0.500023\n",
      "\n",
      "Subgroup MIA Results:\n",
      "Privileged Favorable: Results=[0.48989  0.493641 0.508451 0.515702 0.498905 0.4994  ], Mean=0.500998\n",
      "Unprivileged Favorable: Results=[0.494574 0.506231 0.500541 0.499018 0.495774 0.495873], Mean=0.498668\n",
      "Unprivileged Unfavorable: Results=[0.512878 0.496128 0.513021 0.518821 0.529381 0.50428 ], Mean=0.512418\n",
      "Privileged Unfavorable: Results=[0.476786 0.461839 0.596131 0.589057 0.442121 0.598268], Mean=0.527367\n",
      "\n",
      "Accuracy Results:\n",
      "Mean Train Accuracy (Overall): 0.9530059299805255\n",
      "Mean Test Accuracy (Overall): 0.9494239183236943\n",
      "Privileged Favorable: Mean Train Accuracy = 0.9962572846008032, Mean Test Accuracy = 0.993959896091304\n",
      "Unprivileged Favorable: Mean Train Accuracy = 0.9980337193453802, Mean Test Accuracy = 0.9979371666534823\n",
      "Unprivileged Unfavorable: Mean Train Accuracy = 0.07015083427208245, Mean Test Accuracy = 0.06038848431596655\n",
      "Privileged Unfavorable: Mean Train Accuracy = 0.07599054083340806, Mean Test Accuracy = 0.05564113777186782\n",
      "\n",
      "=== Training with Disparate Impact Remover (DIR) ===\n",
      "Training Model #0 as the Target\n",
      "Performing MIA for Target Model #0\n",
      "Training Model #1 as the Target\n",
      "Performing MIA for Target Model #1\n",
      "Training Model #2 as the Target\n",
      "Performing MIA for Target Model #2\n",
      "Training Model #3 as the Target\n",
      "Performing MIA for Target Model #3\n",
      "Training Model #4 as the Target\n",
      "Performing MIA for Target Model #4\n",
      "Training Model #5 as the Target\n",
      "Performing MIA for Target Model #5\n",
      "\n",
      "Overall MIA Results:\n",
      "Results: [0.49716  0.491373 0.501855 0.495869 0.501655 0.500619]\n",
      "Mean: 0.498089\n",
      "\n",
      "Subgroup MIA Results:\n",
      "Privileged Favorable: Results=[0.544987 0.464335 0.4946   0.486876 0.486708 0.463595], Mean=0.490183\n",
      "Unprivileged Favorable: Results=[0.495382 0.493279 0.504347 0.503812 0.504795 0.500044], Mean=0.500277\n",
      "Unprivileged Unfavorable: Results=[0.504488 0.472233 0.49219  0.454256 0.482259 0.54526 ], Mean=0.491781\n",
      "Privileged Unfavorable: Results=[0.554167 0.434545 0.505952 0.459336 0.532996 0.511905], Mean=0.499817\n",
      "\n",
      "Accuracy Results:\n",
      "Mean Train Accuracy (Overall): 0.9510138133410339\n",
      "Mean Test Accuracy (Overall): 0.9512841968403031\n",
      "Privileged Favorable: Mean Train Accuracy = 0.9931740386149391, Mean Test Accuracy = 0.9893341525993851\n",
      "Unprivileged Favorable: Mean Train Accuracy = 0.9980592496842032, Mean Test Accuracy = 0.997557825221207\n",
      "Unprivileged Unfavorable: Mean Train Accuracy = 0.07359601844921686, Mean Test Accuracy = 0.06218808131091564\n",
      "Privileged Unfavorable: Mean Train Accuracy = 0.10460578693337313, Mean Test Accuracy = 0.06656677613574165\n",
      "\n",
      "=== Training with Reweighing ===\n",
      "Training Model #0 as the Target\n",
      "Performing MIA for Target Model #0\n",
      "Training Model #1 as the Target\n",
      "Performing MIA for Target Model #1\n",
      "Training Model #2 as the Target\n",
      "Performing MIA for Target Model #2\n",
      "Training Model #3 as the Target\n",
      "Performing MIA for Target Model #3\n",
      "Training Model #4 as the Target\n",
      "Performing MIA for Target Model #4\n",
      "Training Model #5 as the Target\n",
      "Performing MIA for Target Model #5\n",
      "\n",
      "Overall MIA Results:\n",
      "Results: [0.506142 0.497183 0.500521 0.504902 0.502091 0.498936]\n",
      "Mean: 0.501629\n",
      "\n",
      "Subgroup MIA Results:\n",
      "Privileged Favorable: Results=[0.508651 0.493245 0.494018 0.527866 0.517943 0.506604], Mean=0.508055\n",
      "Unprivileged Favorable: Results=[0.508507 0.496703 0.501541 0.506002 0.501692 0.500253], Mean=0.50245\n",
      "Unprivileged Unfavorable: Results=[0.495678 0.520451 0.490607 0.467094 0.482381 0.487437], Mean=0.490608\n",
      "Privileged Unfavorable: Results=[0.518155 0.473196 0.492662 0.431297 0.479167 0.470273], Mean=0.477458\n",
      "\n",
      "Accuracy Results:\n",
      "Mean Train Accuracy (Overall): 0.9512358532544045\n",
      "Mean Test Accuracy (Overall): 0.9505799644386297\n",
      "Privileged Favorable: Mean Train Accuracy = 1.0, Mean Test Accuracy = 0.999194847020934\n",
      "Unprivileged Favorable: Mean Train Accuracy = 0.9976032285689737, Mean Test Accuracy = 0.9971734423859616\n",
      "Unprivileged Unfavorable: Mean Train Accuracy = 0.07543356740667938, Mean Test Accuracy = 0.07295985669870876\n",
      "Privileged Unfavorable: Mean Train Accuracy = 0.006018217306441119, Mean Test Accuracy = 0.0\n",
      "\n",
      "=== Training with Inprocessing (Exponentiated Gradient) ===\n",
      "Training Model #0 as the Target\n",
      "Performing MIA for Target Model #0\n",
      "Training Model #1 as the Target\n",
      "Performing MIA for Target Model #1\n",
      "Training Model #2 as the Target\n",
      "Performing MIA for Target Model #2\n",
      "Training Model #3 as the Target\n",
      "Performing MIA for Target Model #3\n",
      "Training Model #4 as the Target\n",
      "Performing MIA for Target Model #4\n",
      "Training Model #5 as the Target\n",
      "Performing MIA for Target Model #5\n",
      "\n",
      "Overall MIA Results:\n",
      "Results: [0.503574 0.494478 0.504082 0.493099 0.494527 0.497028]\n",
      "Mean: 0.497798\n",
      "\n",
      "Subgroup MIA Results:\n",
      "Privileged Favorable: Results=[0.500096 0.48112  0.511849 0.460442 0.500941 0.506664], Mean=0.493519\n",
      "Unprivileged Favorable: Results=[0.503504 0.497667 0.502568 0.495561 0.496024 0.499028], Mean=0.499059\n",
      "Unprivileged Unfavorable: Results=[0.522096 0.466627 0.518992 0.479732 0.500738 0.485528], Mean=0.495619\n",
      "Privileged Unfavorable: Results=[0.464764 0.495493 0.583607 0.49494  0.503567 0.485434], Mean=0.504634\n",
      "\n",
      "Accuracy Results:\n",
      "Mean Train Accuracy (Overall): 0.9517126452433526\n",
      "Mean Test Accuracy (Overall): 0.9503171056422427\n",
      "Privileged Favorable: Mean Train Accuracy = 0.9971287904062364, Mean Test Accuracy = 0.9931864024606085\n",
      "Unprivileged Favorable: Mean Train Accuracy = 0.9977976419029645, Mean Test Accuracy = 0.9976436974321619\n",
      "Unprivileged Unfavorable: Mean Train Accuracy = 0.06805369800007773, Mean Test Accuracy = 0.056326180433138535\n",
      "Privileged Unfavorable: Mean Train Accuracy = 0.07863524380272627, Mean Test Accuracy = 0.06483397829051037\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    # Load data and define global variables\n",
    "    # choices=['bank', 'compas_sex', 'compas_race', 'german_age', 'german_sex', 'meps19', 'grade', 'law_sex', 'law_race', 'law_gender_aif', 'law_race_aif']\n",
    "    dataset = 'law_race' \n",
    "    \n",
    "    X, y, protected_attribute_index, dataset_binary, df, protected_attribute_name, label_name = load_dataset(dataset)\n",
    "    \n",
    "    # Define groups for fairness (for reweighting and synthetic functions)\n",
    "    privileged_groups = [{protected_attribute_name: 1}]\n",
    "    unprivileged_groups = [{protected_attribute_name: 0}]\n",
    "    \n",
    "    metric_orig = BinaryLabelDatasetMetric(dataset_binary,\n",
    "                                             unprivileged_groups=unprivileged_groups,\n",
    "                                             privileged_groups=privileged_groups)\n",
    "    f_label = dataset_binary.favorable_label\n",
    "    uf_label = dataset_binary.unfavorable_label\n",
    "    base_rate_privileged_private = metric_orig.base_rate(privileged=True)\n",
    "    base_rate_unprivileged_private = metric_orig.base_rate(privileged=False)\n",
    "    \n",
    "    print(\"\\n=== Training on Original Data ===\")\n",
    "    (mia_results_orig, mia_orig, subgroup_results_orig, subgroup_means_orig,\n",
    "     train_accuracies_orig, test_accuracies_orig, train_subpop_orig, test_subpop_orig,\n",
    "     all_metrics_orig) = train_orig(X, y, dataset_binary, protected_attribute_index, num_shadows=5,\n",
    "                             shadow_model_builder=scikit_learn_model, target_model_builder=scikit_learn_model_dp)\n",
    "    \n",
    "    \n",
    "    print(\"\\n=== Training with Synthetic Oversampling ===\")\n",
    "    (mia_results_syn, mia_syn, subgroup_results_syn, subgroup_means_syn,\n",
    "     train_accuracies_syn, test_accuracies_syn, train_subpop_syn, test_subpop_syn,\n",
    "     all_metrics_syn) = train_syn(X, y, dataset_binary, protected_attribute_index, num_shadows=5,\n",
    "                             shadow_model_builder=scikit_learn_model, target_model_builder=scikit_learn_model_dp)\n",
    "\n",
    "    \n",
    "    print(\"\\n=== Training with Synthetic Target ===\")\n",
    "    (mia_results_syn_target, mia_syn_target, subgroup_results_syn_target, subgroup_means_syn_target,\n",
    "     train_accuracies_syn_target, test_accuracies_syn_target, train_subpop_syn_target, test_subpop_syn_target,\n",
    "     all_metrics_syn_target) = train_syn_target(X, y, dataset_binary, protected_attribute_index, num_shadows=5,\n",
    "                             shadow_model_builder=scikit_learn_model, target_model_builder=scikit_learn_model_dp)\n",
    "\n",
    "    \n",
    "    print(\"\\n=== Training with Disparate Impact Remover (DIR) ===\")\n",
    "    # Apply DIR transformation to the original dataset\n",
    "    DIR = DisparateImpactRemover(repair_level=0.5, sensitive_attribute=protected_attribute_name)\n",
    "    dataset_dir = DIR.fit_transform(dataset_binary)\n",
    "    X_dir = dataset_dir.features\n",
    "    y_dir = dataset_dir.labels.ravel().astype(int)\n",
    "    \n",
    "    (mia_results_dir, mia_dir, subgroup_results_dir, subgroup_means_dir,\n",
    "     train_accuracies_dir, test_accuracies_dir, train_subpop_dir, test_subpop_dir,\n",
    "     all_metrics_dir) = train_orig(X, y, dataset_binary, protected_attribute_index, num_shadows=5,\n",
    "                             shadow_model_builder=scikit_learn_model, target_model_builder=scikit_learn_model_dp)\n",
    "    \n",
    "    \n",
    "    print(\"\\n=== Training with Reweighing ===\")\n",
    "    (mia_results_rew, mia_rew, subgroup_results_rew, subgroup_means_rew,\n",
    "     train_accuracies_rew, test_accuracies_rew, train_subpop_rew, test_subpop_rew,\n",
    "     all_metrics_rew) = train_rew(X, y, dataset_binary, protected_attribute_index, num_shadows=5)\n",
    "\n",
    "        \n",
    "    print(\"\\n=== Training with Inprocessing (Exponentiated Gradient) ===\")\n",
    "    # For the inprocessing example, use a DataFrame for X and a NumPy array for y.\n",
    "    X_df = df.drop(columns=[label_name])\n",
    "    y_arr = np.array(df[label_name]).astype(int)\n",
    "    \n",
    "    (mia_results_egr, mia_egr, subgroup_results_egr, subgroup_means_egr,\n",
    "     train_accuracies_egr, test_accuracies_egr, train_subpop_egr, test_subpop_egr,\n",
    "     all_metrics_egr) = train_eg(X_df, y_arr, dataset_binary, protected_attribute_index, num_shadows=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "08a34d9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved law_race_train_test_accuracies.csv\n",
      "Saved law_race_mia.csv\n",
      "Saved law_race_train_test_accuracies.csv\n"
     ]
    }
   ],
   "source": [
    "model = 'neural_network'\n",
    "save_summary_tables(dataset,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e6ddfa7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bank_train_test_accuracies.csv\n",
      "Saved bank_mia.csv\n",
      "Saved bank_train_test_accuracies.csv\n"
     ]
    }
   ],
   "source": [
    "model = 'rf'\n",
    "save_summary_tables(dataset,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "ad5c2735",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bank_train_test_accuracies.csv\n",
      "Saved bank_mia.csv\n",
      "Saved bank_train_test_accuracies.csv\n"
     ]
    }
   ],
   "source": [
    "model = 'dp_e=1_rf'\n",
    "save_summary_tables(dataset,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "247f8289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bank_train_test_accuracies.csv\n",
      "Saved bank_mia.csv\n",
      "Saved bank_train_test_accuracies.csv\n"
     ]
    }
   ],
   "source": [
    "model = 'dp_e=5_rf'\n",
    "save_summary_tables(dataset,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "cbe8e34c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved bank_train_test_accuracies.csv\n",
      "Saved bank_mia.csv\n",
      "Saved bank_train_test_accuracies.csv\n"
     ]
    }
   ],
   "source": [
    "model = 'dp_e=10_rf'\n",
    "save_summary_tables(dataset,model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0370578",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
