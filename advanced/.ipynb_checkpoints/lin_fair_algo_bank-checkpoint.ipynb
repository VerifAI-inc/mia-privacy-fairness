{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6360b740",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('../')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d4945c52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fairlearn.datasets import fetch_acs_income\n",
    "from sklearn import preprocessing\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import equalized_odds_difference\n",
    "import numpy as np\n",
    "# from privacy_meter.audit import Audit, MetricEnum\n",
    "# from privacy_meter.dataset import Dataset\n",
    "# from privacy_meter.information_source import InformationSource\n",
    "\n",
    "\n",
    "import keras as keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# load dataset\n",
    "import pandas as pd\n",
    "from fairlearn.reductions import EqualizedOdds\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "# from privacy_meter.audit import Audit, MetricEnum\n",
    "# from privacy_meter.dataset import Dataset\n",
    "# from privacy_meter.information_source import InformationSource\n",
    "# from privacy_meter.model import Fairlearn_Model, Sklearn_Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b43e1340",
   "metadata": {
    "id": "Cm_csV0yFJ-O"
   },
   "source": [
    "# Load dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8193fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = fetch_acs_income(as_frame=False,states=[\"CA\"])\n",
    "sex = data.data[:,data['feature_names'].index('SEX')]\n",
    "X = data.data\n",
    "y_true = (data.target > 50000) * 1\n",
    "X = preprocessing.normalize(X, norm='l2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a33f82d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((195665, 10), (195665,))"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape, y_true.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b49664a9",
   "metadata": {
    "id": "IemPkiZBFJ-U"
   },
   "source": [
    "# Standard model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "760a4b4c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7455702348401605\n",
      "sensitive_feature_0\n",
      "1.0    0.732420\n",
      "2.0    0.760281\n",
      "Name: accuracy_score, dtype: float64\n",
      "0.029908267724111592\n"
     ]
    }
   ],
   "source": [
    "classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=4)\n",
    "classifier.fit(X, y_true)\n",
    "\n",
    "y_pred = classifier.predict(X)\n",
    "gm = MetricFrame(metrics=accuracy_score, y_true=y_true, y_pred=y_pred, sensitive_features=sex)\n",
    "print(gm.overall)\n",
    "print(gm.by_group)\n",
    "print(equalized_odds_difference(y_true,\n",
    "                                y_pred,\n",
    "                                sensitive_features=sex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a203999",
   "metadata": {
    "id": "QjCn07GOFJ-V"
   },
   "source": [
    "# Fair models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ebdaf06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7544016558914471\n",
      "sensitive_feature_0\n",
      "1.0    0.744606\n",
      "2.0    0.765359\n",
      "Name: accuracy_score, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "from fairlearn.reductions import ExponentiatedGradient, DemographicParity\n",
    "from fairlearn.metrics import equalized_odds_difference\n",
    "\n",
    "np.random.seed(0)  # set seed for consistent results with ExponentiatedGradient\n",
    "constraint = EqualizedOdds(difference_bound=0.0001)\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=4)\n",
    "mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "mitigator.fit(X, y_true, sensitive_features=sex)\n",
    "y_pred_mitigated = mitigator.predict(X)\n",
    "sr_mitigated = MetricFrame(metrics=accuracy_score, y_true=y_true, y_pred=y_pred_mitigated, sensitive_features=sex)\n",
    "print(sr_mitigated.overall)\n",
    "print(sr_mitigated.by_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fed254d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006599766260567685\n"
     ]
    }
   ],
   "source": [
    "print(equalized_odds_difference(y_true,\n",
    "                                y_pred_mitigated,\n",
    "                                sensitive_features=sex))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3ec23d8",
   "metadata": {
    "id": "XEiWIinUFJ-Y"
   },
   "source": [
    "# Attack models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bae94ea9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for the population metric\n",
    "seed = 1234\n",
    "np.random.seed(seed)\n",
    "rng = np.random.default_rng(seed=seed) # useless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7e17872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training the target and reference models\n",
    "num_train_points = 2000\n",
    "num_test_points = 2000\n",
    "num_fair_train_points = 10000\n",
    "num_population_points = 30000\n",
    "\n",
    "train_index = np.random.choice(X.shape[0], num_train_points, replace=False)\n",
    "test_index = np.random.choice(X.shape[0], num_test_points, replace=False)\n",
    "fair_train_index = np.random.choice(X.shape[0], num_fair_train_points, replace=False)\n",
    "population_index = np.random.choice(X.shape[0], num_population_points, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7b49eaec",
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Overlapping indices detected!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m combined_indices \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mconcatenate([train_index, test_index, fair_train_index, population_index])\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(combined_indices) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39munique(combined_indices)), \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverlapping indices detected!\u001b[39m\u001b[38;5;124m\"\u001b[39m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Overlapping indices detected!"
     ]
    }
   ],
   "source": [
    "combined_indices = np.concatenate([train_index, test_index, fair_train_index, population_index])\n",
    "assert len(combined_indices) == len(np.unique(combined_indices)), \"Overlapping indices detected!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "671e36d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape= X.shape[-1]\n",
    "num_classes= 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ca2e0e89",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2., 5., 2., ..., 2., 3., 3.])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(y_true[train_index])+ (sex[train_index])*2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "0df9beb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the target model's dataset\n",
    "g_train  = (y_true[train_index])+ (sex[train_index])*2\n",
    "g_test  = (y_true[test_index])+ (sex[test_index])*2\n",
    "g_pop_train = (y_true[population_index])+ (sex[population_index])*2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "695ba749",
   "metadata": {},
   "source": [
    "## from privacy_meter.dataset import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "128a722b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import product\n",
    "from typing import Dict, Union\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "########################################################################################################################\n",
    "# DATASET CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class Dataset:\n",
    "    \"\"\"\n",
    "    Wrapper around a dictionary-like formatted dataset, with functions to run preprocessing, to define default\n",
    "    input/output features, and to split a dataset easily.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dict: dict,\n",
    "        default_input: str,\n",
    "        default_output: str,\n",
    "        default_group: str = None,\n",
    "        preproc_fn_dict: dict = None,\n",
    "        preprocessed: bool = False,\n",
    "    ):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            data_dict: Contains the dataset as a dict.\n",
    "            default_input: The key of the data_dict that should be used by default to get the input of a model.\n",
    "            default_output: The key of the data_dict that should be used by default to get the expected output\n",
    "                of a model.\n",
    "            default_group: The key of the data_dict that shouuld be used by default to get the group of the data points.\n",
    "                This is to contruct class dependent threshold.\n",
    "            preproc_fn_dict: Contains optional preprocessing functions for each feature.\n",
    "            preprocessed: Indicates if the preprocessing of preproc_fn_dict has already been applied.\n",
    "        \"\"\"\n",
    "\n",
    "        # Store parameters\n",
    "        self.data_dict = data_dict\n",
    "        self.default_input = default_input\n",
    "        self.default_output = default_output\n",
    "        self.default_group = default_group\n",
    "        self.preproc_fn_dict = preproc_fn_dict\n",
    "\n",
    "        # Store splits names and features names\n",
    "        self.splits = list(self.data_dict)\n",
    "        self.features = list(self.data_dict[self.splits[0]])\n",
    "\n",
    "        # If preprocessing functions were passed as parameters, execute them\n",
    "        if not preprocessed and preproc_fn_dict is not None:\n",
    "            self.preprocess()\n",
    "\n",
    "    def preprocess(self):\n",
    "        \"\"\"\n",
    "        Preprocessing function, executed by the constructor, based on the preproc_fn_dict attribute.\n",
    "        \"\"\"\n",
    "        for (split, feature) in product(self.splits, self.features):\n",
    "            if feature in list(self.preproc_fn_dict):\n",
    "                fn = self.preproc_fn_dict[feature]\n",
    "                self.data_dict[split][feature] = fn(self.data_dict[split][feature])\n",
    "\n",
    "    def get_feature(self, split_name: str, feature_name: str, indices: list = None):\n",
    "        \"\"\"Returns a specific feature from samples of a specific split.\n",
    "\n",
    "        Args:\n",
    "            split_name: Name of the split.\n",
    "            feature_name: Name of the feature.\n",
    "            indices: Optional list of indices. If not specified, the entire subset is returned.\n",
    "\n",
    "        Returns:\n",
    "            The requested feature, from samples of the requested split.\n",
    "        \"\"\"\n",
    "\n",
    "        # Two placeholders can be used to trigger either the default input or the default output, as specified during\n",
    "        # object creation\n",
    "        if feature_name == \"<default_input>\":\n",
    "            feature_name = self.default_input\n",
    "        elif feature_name == \"<default_output>\":\n",
    "            feature_name = self.default_output\n",
    "        elif feature_name == \"<default_group>\":\n",
    "            feature_name = self.default_group\n",
    "\n",
    "        # If 'indices' is not specified, returns the entire array. Else just return those indices\n",
    "        if indices is None:\n",
    "            return self.data_dict[split_name][feature_name]\n",
    "        else:\n",
    "            return self.data_dict[split_name][feature_name][indices]\n",
    "\n",
    "    def subdivide(\n",
    "        self,\n",
    "        num_splits: int,\n",
    "        split_names: list = None,\n",
    "        method: str = \"independent\",\n",
    "        split_size: Union[int, Dict[str, int]] = None,\n",
    "        delete_original: bool = False,\n",
    "        in_place: bool = True,\n",
    "        return_results: bool = False,\n",
    "    ):\n",
    "        \"\"\"Subdivides the splits contained in split_names into sub-splits, e.g. for shadow model training.\n",
    "\n",
    "        Args:\n",
    "            num_splits: Number of sub-splits per original split.\n",
    "            split_names: The splits to subdivide (e.g. train and test). By default, includes all splits.\n",
    "            method: Either independent or random. If method is independent, then the sub-splits are a partition of the\n",
    "                original split (i.e. they contain the entire split without repetition). If method is random, then each\n",
    "                sub-split is a random subset of the original split (i.e. some samples might be missing or repeated). If\n",
    "                method is hybrid, then each sub-split is a random subset of the original split, with the guarantee that\n",
    "                the 1st one is not overlapping with the others.\n",
    "            split_size: If method is random, this is the size of one split (ignored if method is independent). Can\n",
    "                either be an integer, or a dictionary of integer (one per split).\n",
    "            delete_original: Indicates if the original split should be deleted.\n",
    "            in_place: Indicates if the new splits should be included in the parent object or not\n",
    "            return_results: Indicates if the new splits should be returned or not\n",
    "\n",
    "        Returns:\n",
    "            If in_place, a list of new Dataset objects, with the sub-splits. Otherwise, nothing, as the results are\n",
    "            stored in self.data_dict.\n",
    "        \"\"\"\n",
    "\n",
    "        # By default, includes all splits.\n",
    "        if split_names is None:\n",
    "            split_names = self.splits\n",
    "\n",
    "        # List of results if in_place is False\n",
    "        new_datasets_dict = [{} for _ in range(num_splits)]\n",
    "\n",
    "        for split in split_names:\n",
    "\n",
    "            if split_size is not None:\n",
    "                parsed_split_size = (\n",
    "                    split_size if isinstance(split_size, int) else split_size[split]\n",
    "                )\n",
    "\n",
    "            # If method is random, then each sub-split is a random subset of the original split.\n",
    "            if method == \"random\":\n",
    "                assert (\n",
    "                    split_size is not None\n",
    "                ), 'Argument split_size is required when method is \"random\" or \"hybrid\"'\n",
    "                indices = np.random.randint(\n",
    "                    self.data_dict[split][self.features[0]].shape[0],\n",
    "                    size=(num_splits, parsed_split_size),\n",
    "                )\n",
    "\n",
    "            # If method is independent, then the sub-splits are a partition of the original split.\n",
    "            elif method == \"independent\":\n",
    "                indices = np.arange(self.data_dict[split][self.features[0]].shape[0])\n",
    "                np.random.shuffle(indices)\n",
    "                indices = np.array_split(indices, num_splits)\n",
    "\n",
    "            # If method is hybrid, then each sub-split is a random subset of the original split, with the guarantee that\n",
    "            # the 1st one is not overlapping with the others\n",
    "            elif method == \"hybrid\":\n",
    "                assert (\n",
    "                    split_size is not None\n",
    "                ), 'Argument split_size is required when method is \"random\" or \"hybrid\"'\n",
    "                available_indices = np.arange(\n",
    "                    self.data_dict[split][self.features[0]].shape[0]\n",
    "                )\n",
    "                indices_a = np.random.choice(\n",
    "                    available_indices, size=(1, parsed_split_size), replace=False\n",
    "                )\n",
    "                available_indices = np.setdiff1d(available_indices, indices_a.flatten())\n",
    "                indices_b = np.random.choice(\n",
    "                    available_indices,\n",
    "                    size=(num_splits - 1, parsed_split_size),\n",
    "                    replace=True,\n",
    "                )\n",
    "                indices = np.concatenate((indices_a, indices_b))\n",
    "\n",
    "            else:\n",
    "                raise ValueError(f'Split method \"{method}\" does not exist.')\n",
    "\n",
    "            for split_n in range(num_splits):\n",
    "                # Fill the dictionary if in_place is True\n",
    "                if in_place:\n",
    "                    self.data_dict[f\"{split}{split_n:03d}\"] = {}\n",
    "                    for feature in self.features:\n",
    "                        self.data_dict[f\"{split}{split_n:03d}\"][\n",
    "                            feature\n",
    "                        ] = self.data_dict[split][feature][indices[split_n]]\n",
    "                # Create new dictionaries if return_results is True\n",
    "                if return_results:\n",
    "                    new_datasets_dict[split_n][f\"{split}\"] = {}\n",
    "                    for feature in self.features:\n",
    "                        new_datasets_dict[split_n][f\"{split}\"][\n",
    "                            feature\n",
    "                        ] = self.data_dict[split][feature][indices[split_n]]\n",
    "\n",
    "            # delete_original indicates if the original split should be deleted.\n",
    "            if delete_original:\n",
    "                del self.data_dict[split]\n",
    "\n",
    "        # Update the list of splits\n",
    "        self.splits = list(self.data_dict)\n",
    "\n",
    "        # Return new datasets if return_results is True\n",
    "        if return_results:\n",
    "            return [\n",
    "                Dataset(\n",
    "                    data_dict=new_datasets_dict[i],\n",
    "                    default_input=self.default_input,\n",
    "                    default_output=self.default_output,\n",
    "                    default_group=self.default_group,\n",
    "                    preproc_fn_dict=self.preproc_fn_dict,\n",
    "                    preprocessed=True,\n",
    "                )\n",
    "                for i in range(num_splits)\n",
    "            ]\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string describing the dataset.\n",
    "        \"\"\"\n",
    "        txt = [\n",
    "            f'{\" DATASET OBJECT \":=^48}',\n",
    "            f\"Splits            = {self.splits}\",\n",
    "            f\"Features          = {self.features}\",\n",
    "            f\"Default features  = {self.default_input} --> {self.default_output}\",\n",
    "            \"=\" * 48,\n",
    "        ]\n",
    "        return \"\\n\".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5a5de940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the target model's dataset\n",
    "train_ds = {'x': X[train_index], 'y': y_true[train_index],'g':g_train}\n",
    "test_ds = {'x': X[test_index], 'y': y_true[test_index], 'g':g_test}\n",
    "target_dataset = Dataset(\n",
    "    data_dict={'train': train_ds, 'test': test_ds},\n",
    "    default_input='x', default_output='y', default_group='g'\n",
    ")\n",
    "\n",
    "# create the reference dataset\n",
    "population_ds = {'x': X[population_index], 'y': y_true[population_index], 'g': g_pop_train}\n",
    "reference_dataset = Dataset(\n",
    "    data_dict={'train': population_ds},\n",
    "    default_input='x', default_output='y', default_group='g'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b88fd2c",
   "metadata": {},
   "source": [
    "## from privacy_meter.model import Fairlearn_Model, Sklearn_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dc431c71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from copy import deepcopy\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import torch\n",
    "from opacus import GradSampleModule  # For speeding up the gradient computation\n",
    "from scipy.special import softmax\n",
    "\n",
    "########################################################################################################################\n",
    "# MODEL CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class Model(ABC):\n",
    "    \"\"\"\n",
    "    Interface to query a model without any assumption on how it is implemented.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_obj, loss_fn):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            model_obj: Model object.\n",
    "            loss_fn: Loss function.\n",
    "        \"\"\"\n",
    "        self.model_obj = model_obj\n",
    "        self.loss_fn = loss_fn\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_logits(self, batch_samples):\n",
    "        \"\"\"Function to get the model output from a given input.\n",
    "\n",
    "        Args:\n",
    "            batch_samples: Model input.\n",
    "\n",
    "        Returns:\n",
    "            Model output\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_loss(self, batch_samples, batch_labels, per_point=True):\n",
    "        \"\"\"Function to get the model loss on a given input and an expected output.\n",
    "\n",
    "        Args:\n",
    "            batch_samples: Model input.\n",
    "            batch_labels: Model expected output.\n",
    "            per_point: Boolean indicating if loss should be returned per point or reduced.\n",
    "\n",
    "        Returns:\n",
    "            The loss value, as defined by the loss_fn attribute.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_grad(self, batch_samples, batch_labels):\n",
    "        \"\"\"Function to get the gradient of the model loss with respect to the model parameters, on a given input and an\n",
    "        expected output.\n",
    "\n",
    "        Args:\n",
    "            batch_samples: Model input.\n",
    "            batch_labels: Model expected output.\n",
    "\n",
    "        Returns:\n",
    "            A list of gradients of the model loss (one item per layer) with respect to the model parameters.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def get_intermediate_outputs(self, layers, batch_samples, forward_pass=True):\n",
    "        \"\"\"Function to get the intermediate output of layers (a.k.a. features), on a given input.\n",
    "\n",
    "        Args:\n",
    "            layers: List of integers and/or strings, indicating which layers values should be returned.\n",
    "            batch_samples: Model input.\n",
    "            forward_pass: Boolean indicating if a new forward pass should be executed. If True, then a forward pass is\n",
    "                executed on batch_samples. Else, the result is the one of the last forward pass.\n",
    "\n",
    "        Returns:\n",
    "            A list of intermediate outputs of layers.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a86312fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sklearn_Model(Model):\n",
    "    \"\"\"Inherits from the Model class, an interface to query a model without any assumption on how it is implemented.\n",
    "    This particular class is to be used with tensorflow models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_obj, loss_fn):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            model_obj: Model object.\n",
    "            loss_fn: Loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        # Imports tensorflow with global scope\n",
    "        globals()[\"tf\"] = __import__(\"tensorflow\")\n",
    "\n",
    "        # Initializes the parent model\n",
    "        super().__init__(model_obj, loss_fn)\n",
    "\n",
    "    def get_logits(self, batch_samples):\n",
    "        \"\"\"Function to get the model output from a given input.\n",
    "\n",
    "        Args:\n",
    "            batch_samples: Model input.\n",
    "\n",
    "        Returns:\n",
    "            Model output.\n",
    "        \"\"\"\n",
    "        return self.model_obj.predict_proba(batch_samples)[:, -1]\n",
    "\n",
    "    def get_loss(self, batch_samples, batch_labels, per_point=True):\n",
    "        \"\"\"Function to get the model loss on a given input and an expected output.\n",
    "\n",
    "        Args:\n",
    "            batch_samples: Model input.\n",
    "            batch_labels: Model expected output.\n",
    "            per_point: Boolean indicating if loss should be returned per point or reduced.\n",
    "\n",
    "        Returns:\n",
    "            The loss value, as defined by the loss_fn attribute.\n",
    "        \"\"\"\n",
    "        return self.loss_fn(batch_labels, self.get_logits(batch_samples))\n",
    "\n",
    "    def get_grad(self, batch_samples, batch_labels):\n",
    "        return None\n",
    "\n",
    "    def get_intermediate_outputs(self, layers, batch_samples, forward_pass=True):\n",
    "        return None\n",
    "\n",
    "    def __tf_list_to_np_list(self, x):\n",
    "        if isinstance(x, list):\n",
    "            return [self.__tf_list_to_np_list(y) for y in x]\n",
    "        else:\n",
    "            return x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "286a5d28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Fairlearn_Model(Model):\n",
    "    \"\"\"Inherits from the Model class, an interface to query a model without any assumption on how it is implemented.\n",
    "    This particular class is to be used with tensorflow models.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model_obj, loss_fn):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            model_obj: Model object.\n",
    "            loss_fn: Loss function.\n",
    "        \"\"\"\n",
    "\n",
    "        # Imports tensorflow with global scope\n",
    "        globals()[\"tf\"] = __import__(\"tensorflow\")\n",
    "\n",
    "        # Initializes the parent model\n",
    "        super().__init__(model_obj, loss_fn)\n",
    "\n",
    "    def get_logits(self, batch_samples):\n",
    "        \"\"\"Function to get the model output from a given input.\n",
    "\n",
    "        Args:\n",
    "            batch_samples: Model input.\n",
    "\n",
    "        Returns:\n",
    "            Model output.\n",
    "        \"\"\"\n",
    "        logits = []\n",
    "        for i in range(len(self.model_obj.predictors_)):\n",
    "            logits.append(\n",
    "                self.model_obj.weights_[i]\n",
    "                * self.model_obj.predictors_[i].predict_proba(batch_samples)[:, -1]\n",
    "            )\n",
    "\n",
    "        return np.mean(logits, axis=0)\n",
    "\n",
    "    def get_loss(self, batch_samples, batch_labels, per_point=True):\n",
    "        \"\"\"Function to get the model loss on a given input and an expected output.\n",
    "\n",
    "        Args:\n",
    "            batch_samples: Model input.\n",
    "            batch_labels: Model expected output.\n",
    "            per_point: Boolean indicating if loss should be returned per point or reduced.\n",
    "\n",
    "        Returns:\n",
    "            The loss value, as defined by the loss_fn attribute.\n",
    "        \"\"\"\n",
    "        loss = []\n",
    "        for i in range(len(self.model_obj.predictors_)):\n",
    "            loss.append(\n",
    "                self.model_obj.weights_[i]\n",
    "                * self.loss_fn(\n",
    "                    batch_labels,\n",
    "                    self.model_obj.predictors_[i].predict_proba(batch_samples)[:, -1],\n",
    "                )\n",
    "            )\n",
    "\n",
    "        return np.mean(loss, axis=0)\n",
    "\n",
    "        # return self.loss_fn(batch_labels, self.get_logits(batch_samples))\n",
    "\n",
    "    def get_grad(self, batch_samples, batch_labels):\n",
    "        return None\n",
    "\n",
    "    def get_intermediate_outputs(self, layers, batch_samples, forward_pass=True):\n",
    "        return None\n",
    "\n",
    "    def __tf_list_to_np_list(self, x):\n",
    "        if isinstance(x, list):\n",
    "            return [self.__tf_list_to_np_list(y) for y in x]\n",
    "        else:\n",
    "            return x.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "0e3ed3d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(max_depth=8, min_samples_leaf=10)\n",
      "0.7405 sensitive_feature_0\n",
      "1.0    0.746860\n",
      "2.0    0.733679\n",
      "Name: accuracy_score, dtype: float64\n",
      "0.806 sensitive_feature_0\n",
      "1.0    0.810732\n",
      "2.0    0.801026\n",
      "Name: accuracy_score, dtype: float64\n",
      "0.22076978939724035\n"
     ]
    }
   ],
   "source": [
    "def log(y, pre):\n",
    "    e = 0.0000001\n",
    "    pre = np.clip(pre, e, 1 - e)\n",
    "    return - y * np.log(pre) - (1 - y) * np.log(1 - pre)\n",
    "\n",
    "model = DecisionTreeClassifier(min_samples_leaf=10, max_depth=8)\n",
    "model.fit(X[train_index], y_true[train_index])\n",
    "target_model = Sklearn_Model(model_obj=model, loss_fn=log)\n",
    "print(model)\n",
    "y_pred_test= model.predict(X[test_index])\n",
    "sr_test = MetricFrame(metrics=accuracy_score, y_true=y_true[test_index], y_pred=y_pred_test, sensitive_features=sex[test_index])\n",
    "y_pred_train= model.predict(X[train_index])\n",
    "sr_train = MetricFrame(metrics=accuracy_score, y_true=y_true[train_index], y_pred=y_pred_train, sensitive_features=sex[train_index])\n",
    "print(sr_test.overall, sr_test.by_group)\n",
    "print(sr_train.overall, sr_train.by_group)\n",
    "print(equalized_odds_difference(y_true[test_index],y_pred_test, sensitive_features=sex[test_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fa1e1768",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ExponentiatedGradient(constraints=<fairlearn.reductions._moments.utility_parity.EqualizedOdds object at 0x0000021231933550>,\n",
      "                      estimator=DecisionTreeClassifier(max_depth=8,\n",
      "                                                       min_samples_leaf=10),\n",
      "                      nu=0.004422134963885591)\n",
      "0.742 sensitive_feature_0\n",
      "1.0    0.743961\n",
      "2.0    0.739896\n",
      "Name: accuracy_score, dtype: float64\n",
      "0.8135 sensitive_feature_0\n",
      "1.0    0.815610\n",
      "2.0    0.811282\n",
      "Name: accuracy_score, dtype: float64\n",
      "0.05319535221496008\n"
     ]
    }
   ],
   "source": [
    "constraint = EqualizedOdds(difference_bound=0.0001)\n",
    "classifier = DecisionTreeClassifier(min_samples_leaf=10, max_depth=8)\n",
    "mitigator = ExponentiatedGradient(classifier, constraint)\n",
    "mitigator.fit(X[train_index], y_true[train_index], sensitive_features=sex[train_index])\n",
    "fair_target_model = Fairlearn_Model(model_obj=mitigator, loss_fn=log)\n",
    "print(mitigator)\n",
    "y_pred_test= mitigator.predict(X[test_index])\n",
    "sr_test = MetricFrame(metrics=accuracy_score, y_true=y_true[test_index], y_pred=y_pred_test, sensitive_features=sex[test_index])\n",
    "y_pred_train= mitigator.predict(X[train_index])\n",
    "sr_train = MetricFrame(metrics=accuracy_score, y_true=y_true[train_index], y_pred=y_pred_train, sensitive_features=sex[train_index])\n",
    "print(sr_test.overall, sr_test.by_group)\n",
    "print(sr_train.overall, sr_train.by_group)\n",
    "print(equalized_odds_difference(y_true[test_index],y_pred_test, sensitive_features=sex[test_index]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7cb0f",
   "metadata": {},
   "source": [
    "## from privacy_meter.information_source_signal import Signal, GroupInfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "072d35f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from typing import List, Tuple\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "########################################################################################################################\n",
    "# SIGNAL CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class Signal(ABC):\n",
    "    \"\"\"\n",
    "    Abstract class, representing any type of signal that can be obtained from a Model and/or a Dataset.\n",
    "    \"\"\"\n",
    "\n",
    "    @abstractmethod\n",
    "    def __call__(\n",
    "        self,\n",
    "        models: List[Model],\n",
    "        datasets: List[Dataset],\n",
    "        model_to_split_mapping: List[Tuple[int, str, str, str]],\n",
    "        extra: dict,\n",
    "    ):\n",
    "        \"\"\"Built-in call method.\n",
    "\n",
    "        Args:\n",
    "            models: List of models that can be queried.\n",
    "            datasets: List of datasets that can be queried.\n",
    "            model_to_split_mapping: List of tuples, indicating how each model should query the dataset.\n",
    "                More specifically, for model #i:\n",
    "                model_to_split_mapping[i][0] contains the index of the dataset in the list,\n",
    "                model_to_split_mapping[i][1] contains the name of the split,\n",
    "                model_to_split_mapping[i][2] contains the name of the input feature,\n",
    "                model_to_split_mapping[i][3] contains the name of the output feature.\n",
    "                This can also be provided once and for all at the instantiation of InformationSource, through the\n",
    "                default_model_to_split_mapping argument.\n",
    "            extra: Dictionary containing any additional parameter that should be passed to the signal object.\n",
    "\n",
    "        Returns:\n",
    "            The signal value.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "84eb1053",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GroupInfo(Signal):\n",
    "    \"\"\"\n",
    "    Inherits from the Signal class, used to represent any type of signal that can be obtained from a Model and/or a\n",
    "    Dataset.\n",
    "    This particular class is used to get the group membership of data records.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        models: List[Model],\n",
    "        datasets: List[Dataset],\n",
    "        model_to_split_mapping: List[Tuple[int, str, str, str]],\n",
    "        extra: dict,\n",
    "    ):\n",
    "        \"\"\"Built-in call method.\n",
    "\n",
    "        Args:\n",
    "            models: List of models that can be queried.\n",
    "            datasets: List of datasets that can be queried.\n",
    "            model_to_split_mapping: List of tuples, indicating how each model should query the dataset.\n",
    "                More specifically, for model #i:\n",
    "                model_to_split_mapping[i][0] contains the index of the dataset in the list,\n",
    "                model_to_split_mapping[i][1] contains the name of the split,\n",
    "                model_to_split_mapping[i][2] contains the name of the group feature\n",
    "                This can also be provided once and for all at the instantiation of InformationSource, through the\n",
    "                default_model_to_split_mapping argument.\n",
    "            extra: Dictionary containing any additional parameter that should be passed to the signal object.\n",
    "\n",
    "        Returns:\n",
    "            The signal value.\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        # Given the group membership for each dataset used by each model\n",
    "        for k in range(len(models)):\n",
    "            dataset_index, split_name, group_feature = model_to_split_mapping[k]\n",
    "            g = datasets[dataset_index].get_feature(split_name, group_feature)\n",
    "            results.append(g)\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b3f7edf",
   "metadata": {},
   "source": [
    "## from privacy_meter.information_source import InformationSource"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "24714210",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Tuple\n",
    "\n",
    "########################################################################################################################\n",
    "# INFORMATION_SOURCE CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class InformationSource:\n",
    "    \"\"\"\n",
    "    Interface to dispatch Model objects, Dataset objects, and any additional objects required, to Signal objects.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        models: List[Model],\n",
    "        datasets: List[Dataset],\n",
    "        default_model_to_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "    ):\n",
    "        \"\"\"Constructor\n",
    "\n",
    "        Args:\n",
    "            models: List of models to be queried.\n",
    "            datasets: List of datasets to be queried.\n",
    "            default_model_to_split_mapping: List of tuples, indicating how each model should query the dataset.\n",
    "                More specifically, for model #i:\n",
    "                default_model_to_split_mapping[i][0] contains the index of the dataset in the list,\n",
    "                default_model_to_split_mapping[i][1] contains the name of the split,\n",
    "                default_model_to_split_mapping[i][2] contains the name of the input feature,\n",
    "                default_model_to_split_mapping[i][3] contains the name of the output feature.\n",
    "                This can also be provided independently for each call, through the model_to_split_mapping argument of\n",
    "                the get_signal function.\n",
    "        \"\"\"\n",
    "        self.models = models\n",
    "        self.datasets = datasets\n",
    "        self.default_model_to_split_mapping = default_model_to_split_mapping\n",
    "\n",
    "    def get_signal(\n",
    "        self,\n",
    "        signal: Signal,\n",
    "        model_to_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        extra: dict = None,\n",
    "    ):\n",
    "        \"\"\"Calls the signal object with the appropriate arguments: Model objects and Dataset objects specified at\n",
    "        object instantiation, plus and any additional object required.\n",
    "\n",
    "        Args:\n",
    "            signal: The signal object to call.\n",
    "            model_to_split_mapping: List of tuples, indicating how each model should query the dataset.\n",
    "                More specifically, for model #i:\n",
    "                model_to_split_mapping[i][0] contains the index of the dataset in the list,\n",
    "                model_to_split_mapping[i][1] contains the name of the split,\n",
    "                model_to_split_mapping[i][2] contains the name of the input feature,\n",
    "                model_to_split_mapping[i][3] contains the name of the output feature.\n",
    "                This can also be provided once and for all at the instantiation of InformationSource, through the\n",
    "                default_model_to_split_mapping argument.\n",
    "            extra: Dictionary containing any additional parameter that should be passed to the signal object.\n",
    "\n",
    "        Returns:\n",
    "            The signal value.\n",
    "        \"\"\"\n",
    "\n",
    "        # If no value of model_to_split_mapping is provided, use the default value\n",
    "        if model_to_split_mapping is None:\n",
    "            model_to_split_mapping = self.default_model_to_split_mapping\n",
    "        # If no value of model_to_split_mapping is provided and no default value is set, raise an exception\n",
    "        if model_to_split_mapping is None:\n",
    "            raise TypeError(\n",
    "                \"At least one of self.default_model_to_split_mapping and model_to_split_mapping should be specified\"\n",
    "            )\n",
    "\n",
    "        # Calls the signal object, and returns the value of the call\n",
    "        return signal(\n",
    "            models=self.models,\n",
    "            datasets=self.datasets,\n",
    "            model_to_split_mapping=model_to_split_mapping,\n",
    "            extra=extra,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8ceebfd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "un_target_info_source = InformationSource(\n",
    "    models=[target_model],\n",
    "    datasets=[target_dataset]\n",
    ")\n",
    "\n",
    "un_reference_info_source = InformationSource(\n",
    "    models=[target_model],\n",
    "    datasets=[reference_dataset]\n",
    ")\n",
    "\n",
    "fair_target_info_source = InformationSource(\n",
    "    models=[fair_target_model],\n",
    "    datasets=[target_dataset]\n",
    ")\n",
    "\n",
    "fair_reference_info_source = InformationSource(\n",
    "    models=[fair_target_model],\n",
    "    datasets=[reference_dataset]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359c133f",
   "metadata": {},
   "source": [
    "## from privacy_meter.constants import InferenceGame, MetricEnum, NPZ_EXTENSION, SignalSourceEnum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "13036ea2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "\n",
    "########################################################################################################################\n",
    "# ENUM: TYPES OF METRICS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class MetricEnum(Enum):\n",
    "    POPULATION = \"population_metric\"\n",
    "    SHADOW = \"shadow_metric\"\n",
    "    REFERENCE = \"reference_metric\"\n",
    "    GROUPPOPULATION = \"group_population_metric\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "439ee4f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# ENUM: TYPE OF INFERENCE GAME\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class InferenceGame(Enum):\n",
    "    AVG_PRIVACY_LOSS_TRAINING_ALGO = \"Average privacy loss of a training algorithm\"\n",
    "    PRIVACY_LOSS_MODEL = \"Privacy loss of a model\"\n",
    "    PRIVACY_LOSS_SAMPLE = \"Privacy loss of a data record\"\n",
    "    WORST_CASE_PRIVACY_LOSS_TRAINING_ALGO = (\n",
    "        \"Worst-case privacy loss of a training algorithm\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "295bbe9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "NPZ_EXTENSION = \".npz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "0465e574",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# ENUM: SOURCES FOR COMPUTING SIGNALS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class SignalSourceEnum(Enum):\n",
    "    TARGET_MEMBER = \"target_member\"\n",
    "    TARGET_NON_MEMBER = \"target_non_member\"\n",
    "    REFERENCE = \"reference\"\n",
    "    REFERENCE_MEMBER = \"reference_member\"\n",
    "    REFERENCE_NON_MEMBER = \"reference_non_member\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b4ac21",
   "metadata": {},
   "source": [
    "## from privacy_meter.hypothesis_test import linear_itp_threshold_func"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "c9c934ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# HYPOTHESIS TEST: LINEAR INTERPOLATION THRESHOLDING\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "def linear_itp_threshold_func(\n",
    "    distribution: List[float],\n",
    "    alpha: List[float],\n",
    "    **kwargs,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Function that returns the threshold as the alpha quantile of\n",
    "    a linear interpolation curve fit over the provided distribution.\n",
    "    Args:\n",
    "        distribution: Sequence of values that form the distribution from which\n",
    "        the threshold is computed. (Here we only consider positive signal values.)\n",
    "        alpha: Quantile value that will be used to obtain the threshold from the\n",
    "            distribution.\n",
    "    Returns:\n",
    "        threshold: alpha quantile of the provided distribution.\n",
    "    \"\"\"\n",
    "\n",
    "    if len(distribution.shape) > 1:\n",
    "        # for reference attacks\n",
    "        threshold = np.quantile(\n",
    "            distribution, q=alpha[1:-1], method=\"linear\", axis=1, **kwargs\n",
    "        )\n",
    "        threshold = np.concatenate(\n",
    "            [\n",
    "                threshold,\n",
    "                np.repeat(distribution.max() + 1e-4, distribution.shape[0]).reshape(\n",
    "                    1, -1\n",
    "                ),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "        threshold = np.concatenate(\n",
    "            [\n",
    "                np.repeat(distribution.min() - 1e-4, distribution.shape[0]).reshape(\n",
    "                    1, -1\n",
    "                ),\n",
    "                threshold,\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    else:\n",
    "        threshold = np.quantile(distribution, q=alpha[1:-1], method=\"linear\", **kwargs)\n",
    "        threshold = np.concatenate(\n",
    "            [\n",
    "                np.array(distribution.min() - 1e-4).reshape(-1),\n",
    "                threshold,\n",
    "                np.array(distribution.max() + 1e-4).reshape(-1),\n",
    "            ],\n",
    "            axis=0,\n",
    "        )\n",
    "\n",
    "    return threshold"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9798d631",
   "metadata": {},
   "source": [
    "## from privacy_meter.information_source_signal import ModelLoss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8e3ea701",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# MODEL_LOSS CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class ModelLoss(Signal):\n",
    "    \"\"\"\n",
    "    Inherits from the Signal class, used to represent any type of signal that can be obtained from a Model and/or a\n",
    "    Dataset.\n",
    "    This particular class is used to get the loss of a model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(\n",
    "        self,\n",
    "        models: List[Model],\n",
    "        datasets: List[Dataset],\n",
    "        model_to_split_mapping: List[Tuple[int, str, str, str]],\n",
    "        extra: dict,\n",
    "    ):\n",
    "        \"\"\"Built-in call method.\n",
    "\n",
    "        Args:\n",
    "            models: List of models that can be queried.\n",
    "            datasets: List of datasets that can be queried.\n",
    "            model_to_split_mapping: List of tuples, indicating how each model should query the dataset.\n",
    "                More specifically, for model #i:\n",
    "                model_to_split_mapping[i][0] contains the index of the dataset in the list,\n",
    "                model_to_split_mapping[i][1] contains the name of the split,\n",
    "                model_to_split_mapping[i][2] contains the name of the input feature,\n",
    "                model_to_split_mapping[i][3] contains the name of the output feature.\n",
    "                This can also be provided once and for all at the instantiation of InformationSource, through the\n",
    "                default_model_to_split_mapping argument.\n",
    "            extra: Dictionary containing any additional parameter that should be passed to the signal object.\n",
    "\n",
    "        Returns:\n",
    "            The signal value.\n",
    "        \"\"\"\n",
    "\n",
    "        results = []\n",
    "        # Compute the signal for each model\n",
    "        for k, model in enumerate(models):\n",
    "            # Extract the features to be used\n",
    "            (\n",
    "                dataset_index,\n",
    "                split_name,\n",
    "                input_feature,\n",
    "                output_feature,\n",
    "            ) = model_to_split_mapping[k]\n",
    "            x = datasets[dataset_index].get_feature(split_name, input_feature)\n",
    "\n",
    "            # Check if output feature has been provided, else pass None\n",
    "            if output_feature is not None:\n",
    "                y = datasets[dataset_index].get_feature(split_name, output_feature)\n",
    "            else:\n",
    "                y = None\n",
    "\n",
    "            # Compute the signal for each sample\n",
    "            results.append(model.get_loss(x, y))\n",
    "        return results"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab1e1a86",
   "metadata": {},
   "source": [
    "## from privacy_meter.metric_result import CombinedMetricResult, MetricResult"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "de1bb6b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score,\n",
    "    auc,\n",
    "    confusion_matrix,\n",
    "    roc_auc_score,\n",
    "    roc_curve,\n",
    ")\n",
    "\n",
    "########################################################################################################################\n",
    "# METRIC_RESULT CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class MetricResult:\n",
    "    \"\"\"\n",
    "    Contains results related to the performance of the metric.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metric_id: str,\n",
    "        predicted_labels: list,\n",
    "        true_labels: list,\n",
    "        predictions_proba: List[List[float]] = None,\n",
    "        signal_values=None,\n",
    "        threshold: float = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Computes and stores the accuracy, ROC AUC score, and the confusion matrix for a metric.\n",
    "\n",
    "        Args:\n",
    "            metric_id: ID of the metric that was used (c.f. the report_files/explanations.json file).\n",
    "            predicted_labels: Membership predictions of the metric.\n",
    "            true_labels: True membership labels used to evaluate the metric.\n",
    "            predictions_proba: Continuous version of the predicted_labels.\n",
    "            signal_values: Values of the signal used by the metric.\n",
    "            threshold: Threshold computed by the metric.\n",
    "        \"\"\"\n",
    "        self.metric_id = metric_id\n",
    "        self.predicted_labels = predicted_labels\n",
    "        self.true_labels = true_labels\n",
    "        self.predictions_proba = predictions_proba\n",
    "        self.signal_values = signal_values\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.accuracy = accuracy_score(y_true=true_labels, y_pred=predicted_labels)\n",
    "\n",
    "        if self.predictions_proba is None:\n",
    "            self.roc = roc_curve(y_true=true_labels, y_score=predicted_labels)\n",
    "        else:\n",
    "            self.roc = roc_curve(y_true=true_labels, y_score=predictions_proba)\n",
    "\n",
    "        if self.predictions_proba is None:\n",
    "            self.roc_auc = roc_auc_score(y_true=true_labels, y_score=predicted_labels)\n",
    "        else:\n",
    "            self.roc_auc = roc_auc_score(y_true=true_labels, y_score=predictions_proba)\n",
    "\n",
    "        self.tn, self.fp, self.fn, self.tp = confusion_matrix(\n",
    "            y_true=true_labels, y_pred=predicted_labels\n",
    "        ).ravel()\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string describing the metric result.\n",
    "        \"\"\"\n",
    "        txt = [\n",
    "            f'{\" METRIC RESULT OBJECT \":=^48}',\n",
    "            f\"Accuracy          = {self.accuracy}\",\n",
    "            f\"ROC AUC Score     = {self.roc_auc}\",\n",
    "            f\"FPR               = {self.fp / (self.fp + self.tn)}\",\n",
    "            f\"TN, FP, FN, TP    = {self.tn, self.fp, self.fn, self.tp}\",\n",
    "        ]\n",
    "        return \"\\n\".join(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "8224b2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CombinedMetricResult:\n",
    "    \"\"\"\n",
    "    Contains results related to the performance of the metric. It contains the results for multiple fpr.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metric_id: str,\n",
    "        predicted_labels: list,\n",
    "        true_labels: list,\n",
    "        predictions_proba=None,\n",
    "        signal_values=None,\n",
    "        threshold: float = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor.\n",
    "        Computes and stores the accuracy, ROC AUC score, and the confusion matrix for a metric.\n",
    "\n",
    "        Args:\n",
    "            metric_id: ID of the metric that was used (c.f. the report_files/explanations.json file).\n",
    "            predicted_labels: Membership predictions of the metric.\n",
    "            true_labels: True membership labels used to evaluate the metric.\n",
    "            predictions_proba: Continuous version of the predicted_labels.\n",
    "            signal_values: Values of the signal used by the metric.\n",
    "            threshold: Threshold computed by the metric.\n",
    "        \"\"\"\n",
    "        self.metric_id = metric_id\n",
    "        self.predicted_labels = predicted_labels\n",
    "        self.true_labels = true_labels\n",
    "        self.predictions_proba = predictions_proba\n",
    "        self.signal_values = signal_values\n",
    "        self.threshold = threshold\n",
    "\n",
    "        self.accuracy = np.mean(predicted_labels == true_labels, axis=1)\n",
    "        self.tn = np.sum(true_labels == 0) - np.sum(\n",
    "            predicted_labels[:, true_labels == 0], axis=1\n",
    "        )\n",
    "        self.tp = np.sum(predicted_labels[:, true_labels == 1], axis=1)\n",
    "        self.fp = np.sum(predicted_labels[:, true_labels == 0], axis=1)\n",
    "        self.fn = np.sum(true_labels == 1) - np.sum(\n",
    "            predicted_labels[:, true_labels == 1], axis=1\n",
    "        )\n",
    "\n",
    "        self.roc_auc = auc(\n",
    "            self.fp / (np.sum(true_labels == 0)), self.tp / (np.sum(true_labels == 1))\n",
    "        )\n",
    "\n",
    "    def __str__(self):\n",
    "        \"\"\"\n",
    "        Returns a string describing the metric result.\n",
    "        \"\"\"\n",
    "        txt_list = []\n",
    "        for idx in range(len(self.accuracy)):\n",
    "            txt = [\n",
    "                f'{\" METRIC RESULT OBJECT \":=^48}',\n",
    "                f\"Accuracy          = {self.accuracy[idx]}\",\n",
    "                f\"ROC AUC Score     = {self.roc_auc}\",\n",
    "                f\"FPR               = {self.fp[idx] / (self.fp[idx] + self.tn[idx])}\",\n",
    "                f\"TN, FP, FN, TP    = {self.tn[idx], self.fp[idx], self.fn[idx], self.tp[idx]}\",\n",
    "            ]\n",
    "\n",
    "            txt_list.append(\"\\n\".join(txt))\n",
    "        return \"\\n\\n\".join(txt_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a737495",
   "metadata": {},
   "source": [
    "## from privacy_meter.metric import (GroupPopulationMetric, Metric, PopulationMetric, ReferenceMetric, ShadowMetric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "67ded450",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Callable, List, Optional, Tuple, Union\n",
    "\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from privacy_meter.utils import default_quantile, flatten_array\n",
    "\n",
    "########################################################################################################################\n",
    "# METRIC CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class Metric(ABC):\n",
    "    \"\"\"\n",
    "    Interface to construct and perform a membership inference attack on a target model and dataset using auxiliary\n",
    "    information specified by the user. This serves as a guideline for implementing a metric to be used for measuring\n",
    "    the privacy leakage of a target model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_info_source: InformationSource,\n",
    "        reference_info_source: InformationSource,\n",
    "        signals: List[Signal],\n",
    "        hypothesis_test_func: Optional[Callable],\n",
    "        logs_dirname: str,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            target_info_source: InformationSource, containing the Model that the metric will be performed on, and the\n",
    "                corresponding Dataset.\n",
    "            reference_info_source: List of InformationSource(s), containing the Model(s) that the metric will be\n",
    "                fitted on, and their corresponding Dataset.\n",
    "            signals: List of signals to be used.\n",
    "            hypothesis_test_func: Function that will be used for computing attack threshold(s)\n",
    "        \"\"\"\n",
    "\n",
    "        self.target_info_source = target_info_source\n",
    "        self.reference_info_source = reference_info_source\n",
    "        self.signals = signals\n",
    "        self.hypothesis_test_func = hypothesis_test_func\n",
    "        self.logs_dirname = logs_dirname\n",
    "\n",
    "    def _load_or_compute_signal(\n",
    "        self,\n",
    "        signal_source: SignalSourceEnum,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Private helper function to load signals if they have been computed already, or compute and save signals\n",
    "        if they haven't.\n",
    "\n",
    "        Args:\n",
    "            signal_source: Signal source to determine which information source and mapping objects need to be used.\n",
    "\n",
    "        Returns:\n",
    "            Signals computed using the specified information source and mapping object.\n",
    "        \"\"\"\n",
    "        if self.logs_dirname is not None:\n",
    "            print(\"This is logs: \", self.logs_dirname)\n",
    "            signal_filepath = (\n",
    "                f\"{self.logs_dirname}/{type(self).__name__}_{signal_source.value}\"\n",
    "            )\n",
    "            print(\"This is signal_filepath: \", signal_filepath) # un_log/GroupPopulationMetric_target_member\n",
    "        else:\n",
    "            signal_filepath = None\n",
    "\n",
    "        if signal_source == SignalSourceEnum.TARGET_MEMBER:\n",
    "            info_source_obj = self.target_info_source\n",
    "            mapping_obj = self.target_model_to_train_split_mapping\n",
    "        elif signal_source == SignalSourceEnum.TARGET_NON_MEMBER:\n",
    "            info_source_obj = self.target_info_source\n",
    "            mapping_obj = self.target_model_to_test_split_mapping\n",
    "        elif (\n",
    "            signal_source == SignalSourceEnum.REFERENCE_MEMBER\n",
    "            or signal_source == SignalSourceEnum.REFERENCE\n",
    "        ):\n",
    "            info_source_obj = self.reference_info_source\n",
    "            mapping_obj = self.reference_model_to_train_split_mapping\n",
    "        elif signal_source == SignalSourceEnum.REFERENCE_NON_MEMBER:\n",
    "            info_source_obj = self.reference_info_source\n",
    "            mapping_obj = self.reference_model_to_test_split_mapping\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        signals = []\n",
    "\n",
    "        if os.path.isfile(f\"{signal_filepath}{NPZ_EXTENSION}\"): # un_log/GroupPopulationMetric_target_member\n",
    "            print(\"I am in OS path\")\n",
    "            with np.load(\n",
    "                f\"{signal_filepath}{NPZ_EXTENSION}\", allow_pickle=True\n",
    "            ) as data:\n",
    "                signals = np.array(data[\"arr_0\"][()])\n",
    "        else:\n",
    "            # For each signal, compute the response of the model on the dataset according to the mapping\n",
    "            for signal in self.signals:\n",
    "                computed_signal = info_source_obj.get_signal(signal, mapping_obj)\n",
    "                signals.append(info_source_obj.get_signal(computed_signal))\n",
    "                print(f\"Computed signal for {signal_source}: {computed_signal} with shape {np.shape(computed_signal)}\")\n",
    "            if signal_filepath is not None:\n",
    "                print(\"I am in np.savez\")\n",
    "                np.savez(signal_filepath, signals)\n",
    "\n",
    "        print(f\"Loaded or computed signals for {signal_source}: {signals} with shape {np.shape(signals)}\")\n",
    "        \n",
    "        return signals\n",
    "\n",
    "    def _load_or_compute_group_membership(\n",
    "        self,\n",
    "        signal_source: SignalSourceEnum,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Private helper function to compute group membership\n",
    "        Args:\n",
    "            signal_source: Signal source to determine which information source and mapping objects need to be used.\n",
    "\n",
    "        Returns:\n",
    "            Group membership computed using the specified information source and mapping object.\n",
    "        \"\"\"\n",
    "        if signal_source == SignalSourceEnum.TARGET_MEMBER:\n",
    "            info_source_obj = self.target_info_source\n",
    "            mapping_obj = self.target_model_to_train_split_mapping_group\n",
    "        elif signal_source == SignalSourceEnum.TARGET_NON_MEMBER:\n",
    "            info_source_obj = self.target_info_source\n",
    "            mapping_obj = self.target_model_to_test_split_mapping_group\n",
    "        elif (\n",
    "            signal_source == SignalSourceEnum.REFERENCE_MEMBER\n",
    "            or signal_source == SignalSourceEnum.REFERENCE\n",
    "        ):\n",
    "            info_source_obj = self.reference_info_source\n",
    "            mapping_obj = self.reference_model_to_train_split_mapping_group\n",
    "        elif signal_source == SignalSourceEnum.REFERENCE_NON_MEMBER:\n",
    "            info_source_obj = self.reference_info_source\n",
    "            mapping_obj = self.reference_model_to_test_split_mapping_group\n",
    "        else:\n",
    "            raise NotImplementedError\n",
    "\n",
    "        group_membership = info_source_obj.get_signal(GroupInfo(), mapping_obj)\n",
    "        \n",
    "        print(f\"Computed group membership for {signal_source}: {group_membership} with shape {np.shape(group_membership)}\")\n",
    "\n",
    "        return group_membership\n",
    "\n",
    "    def _set_default_mappings(self, unique_dataset: bool):\n",
    "        \"\"\"\n",
    "        Private helper function, to set default values for mappings between models and dataset splits.\n",
    "\n",
    "        Args:\n",
    "            unique_dataset: Boolean indicating if target_info_source and reference_info_source use one same dataset\n",
    "                object.\n",
    "\n",
    "        \"\"\"\n",
    "        if unique_dataset:\n",
    "            if self.target_model_to_train_split_mapping is None:\n",
    "                self.target_model_to_train_split_mapping = [\n",
    "                    (0, \"train000\", \"<default_input>\", \"<default_output>\")\n",
    "                ]\n",
    "            if self.target_model_to_test_split_mapping is None:\n",
    "                self.target_model_to_test_split_mapping = [\n",
    "                    (0, \"test000\", \"<default_input>\", \"<default_output>\")\n",
    "                ]\n",
    "            if self.reference_model_to_train_split_mapping is None:\n",
    "                self.reference_model_to_train_split_mapping = [\n",
    "                    (0, f\"train{k + 1:03d}\", \"<default_input>\", \"<default_output>\")\n",
    "                    for k in range(len(self.reference_info_source.models))\n",
    "                ]\n",
    "            if self.reference_model_to_test_split_mapping is None:\n",
    "                self.reference_model_to_test_split_mapping = [\n",
    "                    (0, f\"test{k + 1:03d}\", \"<default_input>\", \"<default_output>\")\n",
    "                    for k in range(len(self.reference_info_source.models))\n",
    "                ]\n",
    "        else:\n",
    "            if self.target_model_to_train_split_mapping is None:\n",
    "                self.target_model_to_train_split_mapping = [\n",
    "                    (0, \"train\", \"<default_input>\", \"<default_output>\")\n",
    "                ]\n",
    "            if self.target_model_to_test_split_mapping is None:\n",
    "                self.target_model_to_test_split_mapping = [\n",
    "                    (0, \"test\", \"<default_input>\", \"<default_output>\")\n",
    "                ]\n",
    "            if self.reference_model_to_train_split_mapping is None:\n",
    "                self.reference_model_to_train_split_mapping = [\n",
    "                    (k, \"train\", \"<default_input>\", \"<default_output>\")\n",
    "                    for k in range(len(self.reference_info_source.models))\n",
    "                ]\n",
    "            if self.reference_model_to_test_split_mapping is None:\n",
    "                self.reference_model_to_test_split_mapping = [\n",
    "                    (k, \"test\", \"<default_input>\", \"<default_output>\")\n",
    "                    for k in range(len(self.reference_info_source.models))\n",
    "                ]\n",
    "\n",
    "    def _set_default_group_mappings(self, unique_dataset: bool):\n",
    "        \"\"\"\n",
    "        Private helper function, to set default values for mappings between models and dataset splits for groups.\n",
    "        Args:\n",
    "            unique_dataset: Boolean indicating if target_info_source and reference_info_source use one same dataset\n",
    "                object.\n",
    "\n",
    "        \"\"\"\n",
    "        if unique_dataset:\n",
    "            if self.target_model_to_train_split_mapping_group is None:\n",
    "                self.target_model_to_train_split_mapping_group = [\n",
    "                    (0, \"train000\", \"<default_group>\")\n",
    "                ]\n",
    "            if self.target_model_to_test_split_mapping_group is None:\n",
    "                self.target_model_to_test_split_mapping_group = [\n",
    "                    (0, \"test000\", \"<default_group>\")\n",
    "                ]\n",
    "            if self.reference_model_to_train_split_mapping_group is None:\n",
    "                self.reference_model_to_train_split_mapping_group = [\n",
    "                    (0, f\"train{k + 1:03d}\", \"<default_group>\")\n",
    "                    for k in range(len(self.reference_info_source.models))\n",
    "                ]\n",
    "            if self.reference_model_to_test_split_mapping_group is None:\n",
    "                self.reference_model_to_test_split_mapping_group = [\n",
    "                    (0, f\"test{k + 1:03d}\", \"<default_group>\")\n",
    "                    for k in range(len(self.reference_info_source.models))\n",
    "                ]\n",
    "        else:\n",
    "            if self.target_model_to_train_split_mapping_group is None:\n",
    "                self.target_model_to_train_split_mapping_group = [\n",
    "                    (0, \"train\", \"<default_group>\")\n",
    "                ]\n",
    "            if self.target_model_to_test_split_mapping_group is None:\n",
    "                self.target_model_to_test_split_mapping_group = [\n",
    "                    (0, \"test\", \"<default_group>\")\n",
    "                ]\n",
    "            if self.reference_model_to_train_split_mapping_group is None:\n",
    "                self.reference_model_to_train_split_mapping_group = [\n",
    "                    (k, \"train\", \"<default_group>\")\n",
    "                    for k in range(len(self.reference_info_source.models))\n",
    "                ]\n",
    "            if self.reference_model_to_test_split_mapping_group is None:\n",
    "                self.reference_model_to_test_split_mapping_group = [\n",
    "                    (k, \"test\", \"<default_group>\")\n",
    "                    for k in range(len(self.reference_info_source.models))\n",
    "                ]\n",
    "\n",
    "    @abstractmethod\n",
    "    def prepare_metric(self):\n",
    "        \"\"\"\n",
    "        Function to prepare data needed for running the metric on the target model and dataset, using signals computed\n",
    "        on the auxiliary model(s) and dataset.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def run_metric(\n",
    "        self, fpr_tolerance_rate_list=None\n",
    "    ) -> Union[MetricResult, List[MetricResult]]:\n",
    "        \"\"\"\n",
    "        Function to run the metric on the target model and dataset.\n",
    "\n",
    "        Args:\n",
    "            fpr_tolerance_rate_list (optional): List of FPR tolerance values that may be used by the threshold function\n",
    "                to compute the attack threshold for the metric.\n",
    "\n",
    "        Returns:\n",
    "            Result(s) of the metric.\n",
    "        \"\"\"\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "02e59d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# POPULATION_METRIC CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class PopulationMetric(Metric):\n",
    "    \"\"\"\n",
    "    Inherits from the Metric class to perform the population membership inference attack which will be used as a metric\n",
    "    for measuring privacy leakage of a target model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_info_source: InformationSource,\n",
    "        reference_info_source: InformationSource,\n",
    "        signals: List[Signal],\n",
    "        hypothesis_test_func: Optional[Callable],\n",
    "        target_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        target_model_to_test_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        reference_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        unique_dataset: bool = False,\n",
    "        logs_dirname: str = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            target_info_source: InformationSource, containing the Model that the metric will be performed on, and the\n",
    "                corresponding Dataset.\n",
    "            reference_info_source: List of InformationSource(s), containing the Model(s) that the metric will be\n",
    "                fitted on, and their corresponding Dataset.\n",
    "            signals: List of signals to be used.\n",
    "            hypothesis_test_func: Function that will be used for computing attack threshold(s)\n",
    "            target_model_to_train_split_mapping: Mapping from the target model to the train split of the target dataset.\n",
    "                By default, the code will look for a split named \"train\".\n",
    "            target_model_to_test_split_mapping: Mapping from the target model to the test split of the target dataset.\n",
    "                By default, the code will look for a split named \"test\".\n",
    "            reference_model_to_train_split_mapping: Mapping from the reference models to their train splits of the\n",
    "                corresponding reference dataset. By default, the code will look for a split named \"train\" if only one\n",
    "                reference model is provided, else for splits named \"train000\", \"train001\", \"train002\", etc. For the\n",
    "                population metric, at least one reference dataset should be passed.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initializes the parent metric\n",
    "        super().__init__(\n",
    "            target_info_source=target_info_source,\n",
    "            reference_info_source=reference_info_source,\n",
    "            signals=signals,\n",
    "            hypothesis_test_func=hypothesis_test_func,\n",
    "            logs_dirname=logs_dirname,\n",
    "        )\n",
    "\n",
    "        # Useless object, for compatibility purposes only\n",
    "        self.reference_model_to_test_split_mapping = None\n",
    "\n",
    "        # Logs directory\n",
    "        self.logs_dirname = logs_dirname\n",
    "\n",
    "        # Store the model to split mappings\n",
    "        self.target_model_to_train_split_mapping = target_model_to_train_split_mapping\n",
    "        self.target_model_to_test_split_mapping = target_model_to_test_split_mapping\n",
    "        self.reference_model_to_train_split_mapping = (\n",
    "            reference_model_to_train_split_mapping\n",
    "        )\n",
    "        self._set_default_mappings(unique_dataset)\n",
    "\n",
    "        # Variables used in prepare_metric and run_metric\n",
    "        self.member_signals, self.non_member_signals = [], []\n",
    "        self.reference_signals = []\n",
    "\n",
    "    def prepare_metric(self):\n",
    "        \"\"\"\n",
    "        Function to prepare data needed for running the metric on the target model and dataset, using signals computed\n",
    "        on the auxiliary model(s) and dataset. For the population attack, the auxiliary model is the target model\n",
    "        itself, and the auxiliary dataset is a random split from the target model's training data.\n",
    "        \"\"\"\n",
    "        # Load signals if they have been computed already; otherwise, compute and save them\n",
    "        self.member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.TARGET_MEMBER)\n",
    "        )\n",
    "        self.non_member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.TARGET_NON_MEMBER)\n",
    "        )\n",
    "        self.reference_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.REFERENCE)\n",
    "        )\n",
    "\n",
    "    def run_metric(self, fpr_tolerance_rate_list=None) -> List[MetricResult]:\n",
    "        \"\"\"\n",
    "        Function to run the metric on the target model and dataset.\n",
    "\n",
    "        Args:\n",
    "            fpr_tolerance_rate_list (optional): List of FPR tolerance values that may be used by the threshold function\n",
    "                to compute the attack threshold for the metric.\n",
    "\n",
    "        Returns:\n",
    "            A list of MetricResult objects, one per fpr value.\n",
    "        \"\"\"\n",
    "        # map the threshold with the alpha\n",
    "        if fpr_tolerance_rate_list is not None:\n",
    "            self.quantiles = fpr_tolerance_rate_list\n",
    "        else:\n",
    "            self.quantiles = default_quantile()\n",
    "        thresholds = self.hypothesis_test_func(\n",
    "            self.reference_signals, self.quantiles\n",
    "        ).reshape(-1, 1)\n",
    "\n",
    "        num_threshold = len(self.quantiles)\n",
    "        member_signals = self.member_signals.reshape(-1, 1).repeat(num_threshold, 1).T\n",
    "        non_member_signals = (\n",
    "            self.non_member_signals.reshape(-1, 1).repeat(num_threshold, 1).T\n",
    "        )\n",
    "        member_preds = np.less(member_signals, thresholds)\n",
    "        non_member_preds = np.less(non_member_signals, thresholds)\n",
    "\n",
    "        predictions = np.concatenate([member_preds, non_member_preds], axis=1)\n",
    "        true_labels = np.concatenate(\n",
    "            [np.ones(len(self.member_signals)), np.zeros(len(self.non_member_signals))]\n",
    "        )\n",
    "        signal_values = np.concatenate([self.member_signals, self.non_member_signals])\n",
    "        metric_result = CombinedMetricResult(\n",
    "            metric_id=MetricEnum.REFERENCE.value,\n",
    "            predicted_labels=predictions,\n",
    "            true_labels=true_labels,\n",
    "            predictions_proba=None,\n",
    "            signal_values=signal_values,\n",
    "        )\n",
    "        return [metric_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "6958cb98",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# SHADOW_METRIC CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class ShadowMetric(Metric):\n",
    "    \"\"\"\n",
    "    Inherits from the Metric class to perform the shadow membership inference attack which will be used as a metric for\n",
    "    measuring privacy leakage of a target model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_info_source: InformationSource,\n",
    "        reference_info_source: InformationSource,\n",
    "        signals: List[Signal],\n",
    "        hypothesis_test_func: Optional[Callable],\n",
    "        target_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        target_model_to_test_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        reference_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        reference_model_to_test_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        reweight_samples: bool = True,\n",
    "        unique_dataset: bool = False,\n",
    "        logs_dirname: str = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            target_info_source: InformationSource, containing the Model that the metric will be performed on, and the\n",
    "                corresponding Dataset.\n",
    "            reference_info_source: List of InformationSource(s), containing the Model(s) that the metric will be\n",
    "                fitted on, and their corresponding Dataset.\n",
    "            signals: List of signals to be used.\n",
    "            hypothesis_test_func: Function that will be used for computing attack threshold(s)\n",
    "            target_model_to_train_split_mapping: Mapping from the target model to the train split of the target dataset.\n",
    "                By default, the code will look for a split named \"train\"\n",
    "            target_model_to_test_split_mapping: Mapping from the target model to the test split of the target dataset.\n",
    "                By default, the code will look for a split named \"test\"\n",
    "            reference_model_to_train_split_mapping: Mapping from the reference models to their train splits of the\n",
    "                corresponding reference dataset. By default, the code will look for a split named \"train\" if only one\n",
    "                reference model is provided, else for splits named \"train000\", \"train001\", \"train002\", etc.\n",
    "            reference_model_to_test_split_mapping: Mapping from the reference models to their test splits of the\n",
    "                corresponding reference dataset. By default, the code will look for a split named \"test\" if only one\n",
    "                reference model is provided, else for splits named \"test000\", \"test001\", \"test002\", etc.\n",
    "            reweight_samples: Boolean specifying if the metric should account for an unbalance between the number of\n",
    "                members vs non-members.\n",
    "            unique_dataset: Boolean indicating if target_info_source and reference_info_source use one same dataset\n",
    "                object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initializes the parent metric\n",
    "        super().__init__(\n",
    "            target_info_source=target_info_source,\n",
    "            reference_info_source=reference_info_source,\n",
    "            signals=signals,\n",
    "            hypothesis_test_func=hypothesis_test_func,\n",
    "            logs_dirname=logs_dirname,\n",
    "        )\n",
    "\n",
    "        # Logs directory\n",
    "        self.logs_dirname = logs_dirname\n",
    "\n",
    "        self.reweight_samples = reweight_samples\n",
    "\n",
    "        # Store the model to split mappings\n",
    "        self.target_model_to_train_split_mapping = target_model_to_train_split_mapping\n",
    "        self.target_model_to_test_split_mapping = target_model_to_test_split_mapping\n",
    "        self.reference_model_to_train_split_mapping = (\n",
    "            reference_model_to_train_split_mapping\n",
    "        )\n",
    "        self.reference_model_to_test_split_mapping = (\n",
    "            reference_model_to_test_split_mapping\n",
    "        )\n",
    "        self._set_default_mappings(unique_dataset)\n",
    "\n",
    "        # Variables used in prepare_metric and run_metric\n",
    "        self.member_signals, self.non_member_signals = [], []\n",
    "        self.reference_member_signals, self.reference_non_member_signals = [], []\n",
    "\n",
    "    def prepare_metric(self):\n",
    "        \"\"\"\n",
    "        Function to prepare data needed for running the metric on the target model and dataset, using signals computed\n",
    "        on the reference model(s) and dataset. For the shadow attack, the reference models will be a list of shadow\n",
    "        models and the auxiliary dataset will contain the train-test splits of these models.\n",
    "        \"\"\"\n",
    "        # Load signals if they have been computed already; otherwise, compute and save them\n",
    "        self.member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(signal_source=SignalSourceEnum.TARGET_MEMBER)\n",
    "        )\n",
    "        self.non_member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.TARGET_NON_MEMBER)\n",
    "        )\n",
    "        self.reference_member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.REFERENCE_MEMBER)\n",
    "        )\n",
    "        self.reference_non_member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.REFERENCE_NON_MEMBER)\n",
    "        )\n",
    "\n",
    "    def run_metric(self, fpr_tolerance_rate_list=None) -> MetricResult:\n",
    "        \"\"\"\n",
    "        Function to run the metric on the target model and dataset.\n",
    "\n",
    "        Args:\n",
    "            fpr_tolerance_rate_list (optional): List of FPR tolerance values that may be used by the threshold function\n",
    "                to compute the attack threshold for the metric.\n",
    "\n",
    "        Returns:\n",
    "            The result of the metric\n",
    "        \"\"\"\n",
    "\n",
    "        # Create and fit a LogisticRegression object, from the members and non-members of the reference\n",
    "        # InformationSource\n",
    "        clf = LogisticRegression(\n",
    "            class_weight={\n",
    "                0: self.reference_member_signals.shape[0],\n",
    "                1: self.reference_non_member_signals.shape[0],\n",
    "            }\n",
    "            if self.reweight_samples\n",
    "            else None\n",
    "        )\n",
    "        x = np.concatenate(\n",
    "            [self.reference_member_signals, self.reference_non_member_signals]\n",
    "        ).reshape(-1, 1)\n",
    "        y = np.array(\n",
    "            [1] * len(self.reference_member_signals)\n",
    "            + [0] * len(self.reference_non_member_signals)\n",
    "        )\n",
    "        clf.fit(x, y)\n",
    "\n",
    "        signal_space = np.linspace(\n",
    "            np.array(x).ravel().min(), np.array(x).ravel().max(), 200\n",
    "        ).reshape((-1, 1))\n",
    "        i = np.max(\n",
    "            [i if v == 1 else -1 for i, v in enumerate(clf.predict(signal_space))]\n",
    "        )\n",
    "        threshold = signal_space[i : i + 2].mean()\n",
    "\n",
    "        # Predict the membership status of samples in the target InformationSource\n",
    "        predictions_proba = clf.predict_proba(\n",
    "            np.concatenate(\n",
    "                [\n",
    "                    self.member_signals.reshape(-1, 1),\n",
    "                    self.non_member_signals.reshape(-1, 1),\n",
    "                ]\n",
    "            )\n",
    "        )\n",
    "        predictions_label = np.argmax(predictions_proba, axis=1)\n",
    "        predictions_proba = predictions_proba[:, 1]\n",
    "\n",
    "        true_labels = [1] * len(self.member_signals) + [0] * len(\n",
    "            self.non_member_signals\n",
    "        )\n",
    "        signal_values = np.concatenate([self.member_signals, self.non_member_signals])\n",
    "\n",
    "        # Evaluate the power of this inference and display the result\n",
    "        metric_result = MetricResult(\n",
    "            metric_id=MetricEnum.SHADOW.value,\n",
    "            predictions_proba=predictions_proba,\n",
    "            predicted_labels=predictions_label,\n",
    "            true_labels=true_labels,\n",
    "            signal_values=signal_values,\n",
    "            threshold=threshold,\n",
    "        )\n",
    "\n",
    "        return metric_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "9c7ec877",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# REFERENCE_METRIC CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class ReferenceMetric(Metric):\n",
    "    \"\"\"\n",
    "    Inherits from the Metric class to perform the reference membership inference attack which will be used as a metric\n",
    "    for measuring privacy leakage of a target model.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_info_source: InformationSource,\n",
    "        reference_info_source: InformationSource,\n",
    "        signals: List[Signal],\n",
    "        hypothesis_test_func: Optional[Callable],\n",
    "        target_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        target_model_to_test_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        reference_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        reference_model_to_test_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        unique_dataset: bool = False,\n",
    "        logs_dirname: str = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            target_info_source: InformationSource, containing the Model that the metric will be performed on, and the\n",
    "                corresponding Dataset.\n",
    "            reference_info_source: List of InformationSource(s), containing the Model(s) that the metric will be\n",
    "                fitted on, and their corresponding Dataset.\n",
    "            signals: List of signals to be used.\n",
    "            hypothesis_test_func: Function that will be used for computing attack threshold(s)\n",
    "            target_model_to_train_split_mapping: Mapping from the target model to the train split of the target dataset.\n",
    "                By default, the code will look for a split named \"train\"\n",
    "            target_model_to_test_split_mapping: Mapping from the target model to the test split of the target dataset.\n",
    "                By default, the code will look for a split named \"test\"\n",
    "            reference_model_to_train_split_mapping: Mapping from the reference models to their train splits of the\n",
    "                corresponding reference dataset. By default, the code will look for a split named \"train\"\n",
    "            reference_model_to_test_split_mapping: Mapping from the reference models to their test splits of the\n",
    "                corresponding reference dataset. By default, the code will look for a split named \"test\"\n",
    "            unique_dataset: Boolean indicating if target_info_source and target_info_source use one same dataset object.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initializes the parent metric\n",
    "        super().__init__(\n",
    "            target_info_source=target_info_source,\n",
    "            reference_info_source=reference_info_source,\n",
    "            signals=signals,\n",
    "            hypothesis_test_func=hypothesis_test_func,\n",
    "            logs_dirname=logs_dirname,\n",
    "        )\n",
    "\n",
    "        # Logs directory\n",
    "        self.logs_dirname = logs_dirname\n",
    "\n",
    "        # Store the model to split mappings\n",
    "        self.target_model_to_train_split_mapping = target_model_to_train_split_mapping\n",
    "        self.target_model_to_test_split_mapping = target_model_to_test_split_mapping\n",
    "\n",
    "        # Custom default mapping for the reference metric\n",
    "        if reference_model_to_train_split_mapping is None:\n",
    "            self.reference_model_to_train_split_mapping = [\n",
    "                (0, \"train\", \"<default_input>\", \"<default_output>\")\n",
    "            ] * len(self.reference_info_source.models)\n",
    "        if reference_model_to_test_split_mapping is None:\n",
    "            self.reference_model_to_test_split_mapping = [\n",
    "                (0, \"test\", \"<default_input>\", \"<default_output>\")\n",
    "            ] * len(self.reference_info_source.models)\n",
    "\n",
    "        self._set_default_mappings(unique_dataset)\n",
    "\n",
    "        # Variables used in prepare_metric and run_metric\n",
    "        self.member_signals, self.non_member_signals = [], []\n",
    "        self.reference_member_signals, self.reference_non_member_signals = [], []\n",
    "        self.pointwise_member_thresholds, self.pointwise_non_member_thresholds = [], []\n",
    "\n",
    "    def prepare_metric(self):\n",
    "        \"\"\"\n",
    "        Function to prepare data needed for running the metric on the target model and dataset, using signals computed\n",
    "        on the reference model(s) and dataset. For the reference attack, the reference models will be a list of models\n",
    "        trained on data from the same distribution, and the reference dataset will be the target model's train-test\n",
    "        split.\n",
    "        \"\"\"\n",
    "        # Load signals if they have been computed already; otherwise, compute and save them\n",
    "        self.member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.TARGET_MEMBER)\n",
    "        )\n",
    "        self.non_member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.TARGET_NON_MEMBER)\n",
    "        )\n",
    "        self.reference_member_signals = np.array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.REFERENCE_MEMBER)[0]\n",
    "        ).transpose()\n",
    "        self.reference_non_member_signals = np.array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.REFERENCE_NON_MEMBER)[0]\n",
    "        ).transpose()\n",
    "\n",
    "    def run_metric(self, fpr_tolerance_rate_list=None) -> List[MetricResult]:\n",
    "        \"\"\"\n",
    "        Function to run the metric on the target model and dataset.\n",
    "\n",
    "        Args:\n",
    "            fpr_tolerance_rate_list (optional): List of FPR tolerance values that may be used by the threshold function\n",
    "                to compute the attack threshold for the metric.\n",
    "\n",
    "        Returns:\n",
    "            A list of MetricResult objects, one per fpr value.\n",
    "        \"\"\"\n",
    "\n",
    "        if fpr_tolerance_rate_list is None:\n",
    "            self.quantiles = default_quantile()\n",
    "        else:\n",
    "            self.quantiles = np.array(fpr_tolerance_rate_list)\n",
    "        reference_member_threshold = self.hypothesis_test_func(\n",
    "            self.reference_member_signals, self.quantiles\n",
    "        )\n",
    "        reference_non_member_threshold = self.hypothesis_test_func(\n",
    "            self.reference_non_member_signals, self.quantiles\n",
    "        )\n",
    "\n",
    "        num_threshold = len(self.quantiles)\n",
    "        member_signals = self.member_signals.reshape(-1, 1).repeat(num_threshold, 1).T\n",
    "        non_member_signals = (\n",
    "            self.non_member_signals.reshape(-1, 1).repeat(num_threshold, 1).T\n",
    "        )\n",
    "        member_preds = np.less(member_signals, reference_member_threshold)\n",
    "        non_member_preds = np.less(non_member_signals, reference_non_member_threshold)\n",
    "\n",
    "        predictions = np.concatenate([member_preds, non_member_preds], axis=1)\n",
    "        true_labels = np.concatenate(\n",
    "            [np.ones(len(self.member_signals)), np.zeros(len(self.non_member_signals))]\n",
    "        )\n",
    "\n",
    "        signal_values = np.concatenate([self.member_signals, self.non_member_signals])\n",
    "\n",
    "        metric_result = CombinedMetricResult(\n",
    "            metric_id=MetricEnum.REFERENCE.value,\n",
    "            predicted_labels=predictions,\n",
    "            true_labels=true_labels,\n",
    "            predictions_proba=None,\n",
    "            signal_values=signal_values,\n",
    "        )\n",
    "\n",
    "        return [metric_result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "68b32b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "########################################################################################################################\n",
    "# GroupPopulationMetric CLASS\n",
    "########################################################################################################################\n",
    "\n",
    "\n",
    "class GroupPopulationMetric(Metric):\n",
    "    \"\"\"\n",
    "    Inherits from the Metric class to perform the population membership inference attack which will be used as a metric\n",
    "    for measuring privacy leakage of a target model. Compared to PopulationMetric, this new metric is designed to compute the threshold per class.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        target_info_source: InformationSource,\n",
    "        reference_info_source: InformationSource,\n",
    "        signals: List[Signal],\n",
    "        hypothesis_test_func: Optional[Callable],\n",
    "        target_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        target_model_to_test_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        reference_model_to_train_split_mapping: List[Tuple[int, str, str, str]] = None,\n",
    "        target_model_to_train_split_mapping_group: List[Tuple[int, str, str]] = None,\n",
    "        target_model_to_test_split_mapping_group: List[Tuple[int, str, str]] = None,\n",
    "        reference_model_to_train_split_mapping_group: List[Tuple[int, str, str]] = None,\n",
    "        unique_dataset: bool = False,\n",
    "        logs_dirname: str = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            target_info_source: InformationSource, containing the Model that the metric will be performed on, and the\n",
    "                corresponding Dataset.\n",
    "            reference_info_source: List of InformationSource(s), containing the Model(s) that the metric will be\n",
    "                fitted on, and their corresponding Dataset.\n",
    "            signals: List of signals to be used.\n",
    "            hypothesis_test_func: Function that will be used for computing attack threshold(s)\n",
    "            target_model_to_train_split_mapping: Mapping from the target model to the train split of the target dataset.\n",
    "                By default, the code will look for a split named \"train\".\n",
    "            target_model_to_test_split_mapping: Mapping from the target model to the test split of the target dataset.\n",
    "                By default, the code will look for a split named \"test\".\n",
    "            reference_model_to_train_split_mapping: Mapping from the reference models to their train splits of the\n",
    "                corresponding reference dataset. By default, the code will look for a split named \"train\" if only one\n",
    "                reference model is provided, else for splits named \"train000\", \"train001\", \"train002\", etc. For the\n",
    "                population metric, at least one reference dataset should be passed.\n",
    "            target_model_to_train_split_mapping_group: Mapping from the target model to the train split of the target dataset with respect to group information.\n",
    "                By default, the code will look for a split named \"train\".\n",
    "            target_model_to_test_split_mapping_group: Mapping from the target model to the test split of the target dataset with respect to group information.\n",
    "                By default, the code will look for a split named \"test\".\n",
    "            reference_model_to_train_split_mapping_group: Mapping from the reference models to their train splits of the\n",
    "                corresponding reference dataset  with respect to group information.. By default, the code will look for a split named \"train\" if only one\n",
    "                reference model is provided, else for splits named \"train000\", \"train001\", \"train002\", etc. For the\n",
    "                population metric, at least one reference dataset should be passed.\n",
    "        \"\"\"\n",
    "\n",
    "        # Initializes the parent metric\n",
    "        super().__init__(\n",
    "            target_info_source=target_info_source,\n",
    "            reference_info_source=reference_info_source,\n",
    "            signals=signals,\n",
    "            hypothesis_test_func=hypothesis_test_func,\n",
    "            logs_dirname=logs_dirname,\n",
    "        )\n",
    "\n",
    "        # Useless object, for compatibility purposes only\n",
    "        self.reference_model_to_test_split_mapping = None\n",
    "        self.reference_model_to_test_split_mapping_group = None\n",
    "\n",
    "        # Logs directory\n",
    "        self.logs_dirname = logs_dirname\n",
    "\n",
    "        # Store the model to split mappings\n",
    "        self.target_model_to_train_split_mapping = target_model_to_train_split_mapping\n",
    "        self.target_model_to_test_split_mapping = target_model_to_test_split_mapping\n",
    "        self.reference_model_to_train_split_mapping = (\n",
    "            reference_model_to_train_split_mapping\n",
    "        )\n",
    "        self.target_model_to_train_split_mapping_group = (\n",
    "            target_model_to_train_split_mapping_group\n",
    "        )\n",
    "        self.target_model_to_test_split_mapping_group = (\n",
    "            target_model_to_test_split_mapping_group\n",
    "        )\n",
    "        self.reference_model_to_train_split_mapping_group = (\n",
    "            reference_model_to_train_split_mapping_group\n",
    "        )\n",
    "\n",
    "        # get the mapping for the groups\n",
    "        self._set_default_group_mappings(unique_dataset)\n",
    "        self._set_default_mappings(unique_dataset)\n",
    "\n",
    "        # Variables used in prepare_metric and run_metric\n",
    "        self.member_signals, self.non_member_signals = [], []\n",
    "        self.reference_signals = []\n",
    "        self.member_groups, self.non_member_groups = [], []\n",
    "        self.reference_groups = []\n",
    "\n",
    "    def prepare_metric(self):\n",
    "        \"\"\"\n",
    "        Function to prepare data needed for running the metric on the target model and dataset, using signals computed\n",
    "        on the auxiliary model(s) and dataset. For the population attack, the auxiliary model is the target model\n",
    "        itself, and the auxiliary dataset is a random split from the target model's training data.\n",
    "        \"\"\"\n",
    "        # Load signals if they have been computed already; otherwise, compute and save them\n",
    "        self.member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.TARGET_MEMBER)\n",
    "        )\n",
    "        self.non_member_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.TARGET_NON_MEMBER)\n",
    "        )\n",
    "        self.reference_signals = flatten_array(\n",
    "            self._load_or_compute_signal(SignalSourceEnum.REFERENCE)\n",
    "        )\n",
    "        self.reference_groups = flatten_array(\n",
    "            self._load_or_compute_group_membership(SignalSourceEnum.REFERENCE)\n",
    "        )\n",
    "        self.non_member_groups = flatten_array(\n",
    "            self._load_or_compute_group_membership(SignalSourceEnum.TARGET_NON_MEMBER)\n",
    "        )\n",
    "        self.member_groups = flatten_array(\n",
    "            self._load_or_compute_group_membership(SignalSourceEnum.TARGET_MEMBER)\n",
    "        )\n",
    "        \n",
    "        print(\"2###########################\")\n",
    "        print(\"Inside GroupPopulationMetric/prepare_metric:\")\n",
    "        print(\"member_signals:\", self.member_signals, \"shape:\", self.member_signals.shape)\n",
    "        print(\"non_member_signals:\", self.non_member_signals, \"shape:\", self.non_member_signals.shape)\n",
    "        print(\"reference_signals:\", self.reference_signals, \"shape:\", self.reference_signals.shape)\n",
    "        print(\"reference_groups:\", self.reference_groups, \"shape:\", self.reference_groups.shape)\n",
    "        print(\"non_member_groups:\", self.non_member_groups, \"shape:\", self.non_member_groups.shape)\n",
    "        print(\"member_groups:\", self.member_groups, \"shape:\", self.member_groups.shape)\n",
    "        \n",
    "    def run_metric(self, fpr_tolerance_rate_list=None) -> List[MetricResult]:\n",
    "        \"\"\"\n",
    "        Function to run the metric on the target model and dataset.\n",
    "\n",
    "        Args:\n",
    "            fpr_tolerance_rate_list (optional): List of FPR tolerance values that may be used by the threshold function\n",
    "                to compute the attack threshold for the metric.\n",
    "\n",
    "        Returns:\n",
    "            A list of MetricResult objects, one per fpr value.\n",
    "        \"\"\"\n",
    "        print(\"3###########################\")\n",
    "        \n",
    "        if fpr_tolerance_rate_list is None:\n",
    "            self.quantiles = default_quantile()\n",
    "        else:\n",
    "            self.quantiles = fpr_tolerance_rate_list\n",
    "\n",
    "        num_threshold = len(self.quantiles)\n",
    "        member_preds = []\n",
    "        non_member_preds = []\n",
    "        reference_thresholds = {}\n",
    "        member_signal_list = []\n",
    "        non_member_signal_list = []\n",
    "\n",
    "        print(\"Debugging run_metric:\")\n",
    "        print(f\"Quantiles: {self.quantiles}\")\n",
    "        print(f\"Number of thresholds: {num_threshold}\")\n",
    "\n",
    "        for g in np.unique(self.reference_groups):\n",
    "            print(f\"Processing group: {g}\")\n",
    "            group_reference_signals = self.reference_signals[self.reference_groups == g]\n",
    "            print(f\"Reference signals for group {g}: {group_reference_signals}\")\n",
    "\n",
    "            thresholds = self.hypothesis_test_func(group_reference_signals, self.quantiles).reshape(-1, 1)\n",
    "            print(f\"Thresholds for group {g}: {thresholds}\")\n",
    "\n",
    "            group_member_signals = self.member_signals[self.member_groups == g]\n",
    "            print(f\"Member signals for group {g}: {group_member_signals}\")\n",
    "            member_signals = (\n",
    "                group_member_signals\n",
    "                .reshape(-1, 1)\n",
    "                .repeat(num_threshold, 1)\n",
    "                .T\n",
    "            )\n",
    "            print(f\"Member signals (reshaped) for group {g}: {member_signals}\")\n",
    "\n",
    "            group_non_member_signals = self.non_member_signals[self.non_member_groups == g]\n",
    "            print(f\"Non-member signals for group {g}: {group_non_member_signals}\")\n",
    "            non_member_signals = (\n",
    "                group_non_member_signals\n",
    "                .reshape(-1, 1)\n",
    "                .repeat(num_threshold, 1)\n",
    "                .T\n",
    "            )\n",
    "            print(f\"Non-member signals (reshaped) for group {g}: {non_member_signals}\")\n",
    "\n",
    "            member_pred = np.less(member_signals, thresholds)\n",
    "            non_member_pred = np.less(non_member_signals, thresholds)\n",
    "\n",
    "            member_signal_list.append(group_member_signals)\n",
    "            non_member_signal_list.append(group_non_member_signals)\n",
    "            member_preds.append(member_pred)\n",
    "            non_member_preds.append(non_member_pred)\n",
    "\n",
    "            reference_thresholds[g] = thresholds\n",
    "\n",
    "        member_preds = np.concatenate(member_preds, axis=1)\n",
    "        non_member_preds = np.concatenate(non_member_preds, axis=1)\n",
    "        predictions = np.concatenate([member_preds, non_member_preds], axis=1)\n",
    "        true_labels = np.concatenate(\n",
    "            [np.ones(len(self.member_signals)), np.zeros(len(self.non_member_signals))]\n",
    "        )\n",
    "\n",
    "        member_signals = np.concatenate(member_signal_list, axis=0)  # reorder based on the groups\n",
    "        non_member_signals = np.concatenate(non_member_signal_list, axis=0)\n",
    "\n",
    "        signal_values = np.concatenate([member_signals, non_member_signals])\n",
    "\n",
    "        metric_result = CombinedMetricResult(\n",
    "            metric_id=MetricEnum.REFERENCE.value,\n",
    "            predicted_labels=predictions,\n",
    "            true_labels=true_labels,\n",
    "            predictions_proba=None,\n",
    "            signal_values=signal_values,\n",
    "            threshold=reference_thresholds,\n",
    "        )\n",
    "        return [metric_result]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ffa5a9",
   "metadata": {},
   "source": [
    "## from privacy_meter.audit import Audit, MetricEnum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "eb6d749d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime\n",
    "from typing import List, Union\n",
    "\n",
    "class Audit:\n",
    "    \"\"\"\n",
    "    This class orchestrates how the Metric objects and the InformationSource objects interact with one\n",
    "    another. The three steps of using this class are 1) initialization 2) audit.prepare() 3) audit.run().\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        metrics: Union[Union[MetricEnum, Metric], List[Union[MetricEnum, Metric]]],\n",
    "        inference_game_type: InferenceGame,\n",
    "        target_info_sources: Union[InformationSource, List[InformationSource]] = None,\n",
    "        reference_info_sources: Union[\n",
    "            InformationSource, List[InformationSource]\n",
    "        ] = None,\n",
    "        fpr_tolerances: Union[float, List[float]] = None,\n",
    "        logs_directory_names: Union[str, List[str]] = None,\n",
    "        save_logs: bool = True,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Constructor\n",
    "\n",
    "        Args:\n",
    "            metrics: Metric object or list of Metric objects to be used for the audit.\n",
    "            inference_game_type: The type of inference game being played: average privacy loss of a training algorithm,\n",
    "                privacy loss of a model, privacy loss of a data record, or worst-case privacy loss of a training\n",
    "                algorithm.\n",
    "            target_info_sources: InformationSource object(s), containing the Model(s) that the metric will be performed\n",
    "                on, and the corresponding Dataset(s).\n",
    "            reference_info_sources: InformationSource object(s), containing the Model(s) that the metric will be fitted\n",
    "                on, and the corresponding Dataset(s).\n",
    "            fpr_tolerances: FPR tolerance value(s) to be used by the audit.\n",
    "            logs_directory_names: Path(s) to logging directory(ies).\n",
    "            save_logs: Whether to save the signal(s).\n",
    "        \"\"\"\n",
    "\n",
    "        self.metrics = metrics\n",
    "        self.inference_game_type = inference_game_type\n",
    "        self.target_info_sources = target_info_sources\n",
    "        self.reference_info_sources = reference_info_sources\n",
    "        self.fpr_tolerances = fpr_tolerances\n",
    "        self.logs_directory_names = logs_directory_names\n",
    "        self.save_logs = save_logs\n",
    "\n",
    "        self.__init_lists()\n",
    "        self.__init_logs_directories()\n",
    "        self.__init_metric_objects()\n",
    "\n",
    "    def __init_logs_directories(self):\n",
    "        \"\"\"\n",
    "        Private function part of the initialization process, to specify default logging directory(ies), and create them\n",
    "        if necessary.\n",
    "        \"\"\"\n",
    "        if self.logs_directory_names is None and self.save_logs is True:\n",
    "            self.logs_directory_names = []\n",
    "            for i in range(len(self.metrics)):\n",
    "                logs_dirname = os.path.join(\n",
    "                    os.getcwd(),\n",
    "                    datetime.now().strftime(f\"log_%Y-%m-%d_%H-%M-%S-{i:03d}\"),\n",
    "                )\n",
    "                os.mkdir(logs_dirname)\n",
    "                self.logs_directory_names.append(logs_dirname)\n",
    "        elif self.save_logs:\n",
    "            for path in self.logs_directory_names:\n",
    "                if not os.path.isdir(path):\n",
    "                    os.mkdir(path)\n",
    "        else:\n",
    "            self.logs_directory_names = None\n",
    "\n",
    "    def __init_metric_objects(self):\n",
    "        \"\"\"\n",
    "        Private function part of the initialization process, to create Metric objects from MetricEnum ones.\n",
    "        \"\"\"\n",
    "        self.metric_objects = []\n",
    "        for k, metric in enumerate(self.metrics):\n",
    "            if type(metric) == MetricEnum:\n",
    "                # If the user wants to use default versions of metrics\n",
    "                if metric == MetricEnum.POPULATION:\n",
    "                    self.metric_objects.append(\n",
    "                        PopulationMetric(\n",
    "                            target_info_source=self.target_info_sources[k],\n",
    "                            reference_info_source=self.reference_info_sources[k],\n",
    "                            signals=[ModelLoss()],\n",
    "                            hypothesis_test_func=linear_itp_threshold_func,\n",
    "                            logs_dirname=self.logs_directory_names[k],\n",
    "                        )\n",
    "                    )\n",
    "                elif metric == MetricEnum.SHADOW:\n",
    "                    self.metric_objects.append(\n",
    "                        ShadowMetric(\n",
    "                            target_info_source=self.target_info_sources[k],\n",
    "                            reference_info_source=self.reference_info_sources[k],\n",
    "                            signals=[ModelLoss()],\n",
    "                            hypothesis_test_func=None,\n",
    "                            logs_dirname=self.logs_directory_names[k],\n",
    "                        )\n",
    "                    )\n",
    "                elif metric == MetricEnum.REFERENCE:\n",
    "                    self.metric_objects.append(\n",
    "                        ReferenceMetric(\n",
    "                            target_info_source=self.target_info_sources[k],\n",
    "                            reference_info_source=self.reference_info_sources[k],\n",
    "                            signals=[ModelLoss()],\n",
    "                            hypothesis_test_func=linear_itp_threshold_func,\n",
    "                            logs_dirname=self.logs_directory_names[k],\n",
    "                        )\n",
    "                    )\n",
    "                elif metric == MetricEnum.GROUPPOPULATION:\n",
    "                    self.metric_objects.append(\n",
    "                        GroupPopulationMetric(\n",
    "                            target_info_source=self.target_info_sources[k],\n",
    "                            reference_info_source=self.reference_info_sources[k],\n",
    "                            signals=[ModelLoss()],\n",
    "                            hypothesis_test_func=linear_itp_threshold_func,\n",
    "                            logs_dirname=self.logs_directory_names[k],\n",
    "                        )\n",
    "                    )\n",
    "\n",
    "            else:\n",
    "                # If the user wants to pass in their custom metric object\n",
    "                if self.save_logs:\n",
    "                    metric.logs_dirname = self.logs_directory_names[k]\n",
    "                self.metric_objects.append(metric)\n",
    "\n",
    "    def __init_lists(self):\n",
    "        \"\"\"\n",
    "        Private function part of the initialization process, to have consistent argument types.\n",
    "        \"\"\"\n",
    "        if not isinstance(self.metrics, list):\n",
    "            self.metrics = [self.metrics]\n",
    "        if not isinstance(self.target_info_sources, list):\n",
    "            self.target_info_sources = [self.target_info_sources]\n",
    "        if not isinstance(self.reference_info_sources, list):\n",
    "            self.reference_info_sources = [self.reference_info_sources]\n",
    "        if (\n",
    "            not isinstance(self.fpr_tolerances, list)\n",
    "            and self.fpr_tolerances is not None\n",
    "        ):\n",
    "            self.fpr_tolerances = [self.fpr_tolerances]\n",
    "        if self.logs_directory_names is not None and not isinstance(\n",
    "            self.logs_directory_names, list\n",
    "        ):\n",
    "            self.logs_directory_names = [self.logs_directory_names]\n",
    "\n",
    "    def prepare(self):\n",
    "        \"\"\"\n",
    "        Core function that should be called after the initialization and before the audit.run() function. Runs the\n",
    "        prepare_metric function of all metric objects, which computes (or loads from memory) the signals required for\n",
    "        the membership inference algorithms.\n",
    "        \"\"\"\n",
    "        for i in range(len(self.metric_objects)):\n",
    "            print(f\"\\nPreparing metric object {i+1}/{len(self.metric_objects)}\")\n",
    "            print(f\"Metric type: {type(self.metric_objects[i]).__name__}\")\n",
    "            print(f\"Target info source: {self.target_info_sources[i]}\")\n",
    "            print(f\"Target info source train dataset: {self.target_info_sources[i].datasets[0].data_dict['train']['x'].shape}\")\n",
    "            print(f\"Target info source test dataset: {self.target_info_sources[i].datasets[0].data_dict['test']['x'].shape}\")\n",
    "            print(f\"Reference info source: {self.reference_info_sources[i]}\")\n",
    "            print(f\"Reference info source JUST train dataset: {self.reference_info_sources[i].datasets[0].data_dict['train']['x'].shape}\")\n",
    "            self.metric_objects[i].prepare_metric()\n",
    "            print(f\"Preparation complete for metric object {i+1}\")\n",
    "\n",
    "    def run(self) -> List[MetricResult]:\n",
    "        \"\"\"\n",
    "        Core function that should be called after the audit.prepare() function. This actually runs the metrics'\n",
    "        membership inference algorithms.\n",
    "\n",
    "        Returns:\n",
    "            A list of MetricResult objects (one per metric)\n",
    "        \"\"\"\n",
    "        print(f\"Results are stored in: {self.logs_directory_names}\")\n",
    "        print(\"1###########################\")\n",
    "        print(\"Self: \", self)\n",
    "        print(\"Self.metric_objects: \", self.metric_objects)\n",
    "        for i in range(len(self.metric_objects)):\n",
    "            print(\"Self.metric_objects[i]: \", self.metric_objects[i])\n",
    "        return [\n",
    "            self.metric_objects[i].run_metric(self.fpr_tolerances)\n",
    "            for i in range(len(self.metric_objects))\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e63583cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Preparing metric object 1/1\n",
      "Metric type: GroupPopulationMetric\n",
      "Target info source: <__main__.InformationSource object at 0x00000212318E3910>\n",
      "Target info source train dataset: (2000, 10)\n",
      "Target info source test dataset: (2000, 10)\n",
      "Reference info source: <__main__.InformationSource object at 0x00000212410E3350>\n",
      "Reference info source JUST train dataset: (30000, 10)\n",
      "This is logs:  un_log\n",
      "This is signal_filepath:  un_log/GroupPopulationMetric_target_member\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "At least one of self.default_model_to_split_mapping and model_to_split_mapping should be specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[40], line 8\u001b[0m\n\u001b[0;32m      1\u001b[0m un_audit_obj \u001b[38;5;241m=\u001b[39m Audit(\n\u001b[0;32m      2\u001b[0m     metrics\u001b[38;5;241m=\u001b[39mMetricEnum\u001b[38;5;241m.\u001b[39mGROUPPOPULATION,\n\u001b[0;32m      3\u001b[0m     inference_game_type\u001b[38;5;241m=\u001b[39mInferenceGame\u001b[38;5;241m.\u001b[39mPRIVACY_LOSS_MODEL,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m      6\u001b[0m     logs_directory_names\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mun_log\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m      7\u001b[0m )\n\u001b[1;32m----> 8\u001b[0m \u001b[43mun_audit_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 162\u001b[0m, in \u001b[0;36mAudit.prepare\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    160\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReference info source: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreference_info_sources[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReference info source JUST train dataset: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreference_info_sources[i]\u001b[38;5;241m.\u001b[39mdatasets[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mdata_dict[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m][\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mx\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 162\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    163\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPreparation complete for metric object \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[38], line 105\u001b[0m, in \u001b[0;36mGroupPopulationMetric.prepare_metric\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     98\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;124;03mFunction to prepare data needed for running the metric on the target model and dataset, using signals computed\u001b[39;00m\n\u001b[0;32m    100\u001b[0m \u001b[38;5;124;03mon the auxiliary model(s) and dataset. For the population attack, the auxiliary model is the target model\u001b[39;00m\n\u001b[0;32m    101\u001b[0m \u001b[38;5;124;03mitself, and the auxiliary dataset is a random split from the target model's training data.\u001b[39;00m\n\u001b[0;32m    102\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;66;03m# Load signals if they have been computed already; otherwise, compute and save them\u001b[39;00m\n\u001b[0;32m    104\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmember_signals \u001b[38;5;241m=\u001b[39m flatten_array(\n\u001b[1;32m--> 105\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_load_or_compute_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mSignalSourceEnum\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTARGET_MEMBER\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    106\u001b[0m )\n\u001b[0;32m    107\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnon_member_signals \u001b[38;5;241m=\u001b[39m flatten_array(\n\u001b[0;32m    108\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_or_compute_signal(SignalSourceEnum\u001b[38;5;241m.\u001b[39mTARGET_NON_MEMBER)\n\u001b[0;32m    109\u001b[0m )\n\u001b[0;32m    110\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreference_signals \u001b[38;5;241m=\u001b[39m flatten_array(\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_load_or_compute_signal(SignalSourceEnum\u001b[38;5;241m.\u001b[39mREFERENCE)\n\u001b[0;32m    112\u001b[0m )\n",
      "Cell \u001b[1;32mIn[33], line 101\u001b[0m, in \u001b[0;36mMetric._load_or_compute_signal\u001b[1;34m(self, signal_source)\u001b[0m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m signal \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msignals:\n\u001b[0;32m    100\u001b[0m     computed_signal \u001b[38;5;241m=\u001b[39m info_source_obj\u001b[38;5;241m.\u001b[39mget_signal(signal, mapping_obj)\n\u001b[1;32m--> 101\u001b[0m     signals\u001b[38;5;241m.\u001b[39mappend(\u001b[43minfo_source_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_signal\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcomputed_signal\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mComputed signal for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msignal_source\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcomputed_signal\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mshape(computed_signal)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m signal_filepath \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[23], line 67\u001b[0m, in \u001b[0;36mInformationSource.get_signal\u001b[1;34m(self, signal, model_to_split_mapping, extra)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;66;03m# If no value of model_to_split_mapping is provided and no default value is set, raise an exception\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m model_to_split_mapping \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[0;32m     68\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAt least one of self.default_model_to_split_mapping and model_to_split_mapping should be specified\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m     )\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# Calls the signal object, and returns the value of the call\u001b[39;00m\n\u001b[0;32m     72\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m signal(\n\u001b[0;32m     73\u001b[0m     models\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodels,\n\u001b[0;32m     74\u001b[0m     datasets\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdatasets,\n\u001b[0;32m     75\u001b[0m     model_to_split_mapping\u001b[38;5;241m=\u001b[39mmodel_to_split_mapping,\n\u001b[0;32m     76\u001b[0m     extra\u001b[38;5;241m=\u001b[39mextra,\n\u001b[0;32m     77\u001b[0m )\n",
      "\u001b[1;31mTypeError\u001b[0m: At least one of self.default_model_to_split_mapping and model_to_split_mapping should be specified"
     ]
    }
   ],
   "source": [
    "un_audit_obj = Audit(\n",
    "    metrics=MetricEnum.GROUPPOPULATION,\n",
    "    inference_game_type=InferenceGame.PRIVACY_LOSS_MODEL,\n",
    "    target_info_sources=un_target_info_source,\n",
    "    reference_info_sources=un_reference_info_source,\n",
    "    logs_directory_names='un_log'\n",
    ")\n",
    "un_audit_obj.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "7244d829",
   "metadata": {},
   "outputs": [],
   "source": [
    "fair_audit_obj = Audit(\n",
    "    metrics=MetricEnum.GROUPPOPULATION,\n",
    "    inference_game_type=InferenceGame.PRIVACY_LOSS_MODEL,\n",
    "    target_info_sources=fair_target_info_source,\n",
    "    reference_info_sources=fair_reference_info_source,\n",
    "    logs_directory_names='fair_log'\n",
    ")\n",
    "fair_audit_obj.prepare()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "006dc1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results are stored in: ['un_log']\n",
      "1###########################\n",
      "Self:  <__main__.Audit object at 0x0000022581D99F10>\n",
      "Self.metric_objects:  [<__main__.GroupPopulationMetric object at 0x0000022587A0AD90>]\n",
      "Self.metric_objects[i]:  <__main__.GroupPopulationMetric object at 0x0000022587A0AD90>\n",
      "3###########################\n",
      "Debugging run_metric:\n",
      "Quantiles: [1.00000000e-05 1.12332403e-05 1.26185688e-05 1.41747416e-05\n",
      " 1.59228279e-05 1.78864953e-05 2.00923300e-05 2.25701972e-05\n",
      " 2.53536449e-05 2.84803587e-05 3.19926714e-05 3.59381366e-05\n",
      " 4.03701726e-05 4.53487851e-05 5.09413801e-05 5.72236766e-05\n",
      " 6.42807312e-05 7.22080902e-05 8.11130831e-05 9.11162756e-05\n",
      " 1.02353102e-04 1.14975700e-04 1.29154967e-04 1.45082878e-04\n",
      " 1.62975083e-04 1.83073828e-04 2.05651231e-04 2.31012970e-04\n",
      " 2.59502421e-04 2.91505306e-04 3.27454916e-04 3.67837977e-04\n",
      " 4.13201240e-04 4.64158883e-04 5.21400829e-04 5.85702082e-04\n",
      " 6.57933225e-04 7.39072203e-04 8.30217568e-04 9.32603347e-04\n",
      " 1.04761575e-03 1.17681195e-03 1.32194115e-03 1.48496826e-03\n",
      " 1.66810054e-03 1.87381742e-03 2.10490414e-03 2.36448941e-03\n",
      " 2.65608778e-03 2.98364724e-03 3.35160265e-03 3.76493581e-03\n",
      " 4.22924287e-03 4.75081016e-03 5.33669923e-03 5.99484250e-03\n",
      " 6.73415066e-03 7.56463328e-03 8.49753436e-03 9.54548457e-03\n",
      " 1.07226722e-02 1.20450354e-02 1.35304777e-02 1.51991108e-02\n",
      " 1.70735265e-02 1.91791026e-02 2.15443469e-02 2.42012826e-02\n",
      " 2.71858824e-02 3.05385551e-02 3.43046929e-02 3.85352859e-02\n",
      " 4.32876128e-02 4.86260158e-02 5.46227722e-02 6.13590727e-02\n",
      " 6.89261210e-02 7.74263683e-02 8.69749003e-02 9.77009957e-02\n",
      " 1.09749877e-01 1.23284674e-01 1.38488637e-01 1.55567614e-01\n",
      " 1.74752840e-01 1.96304065e-01 2.20513074e-01 2.47707636e-01\n",
      " 2.78255940e-01 3.12571585e-01 3.51119173e-01 3.94420606e-01\n",
      " 4.43062146e-01 4.97702356e-01 5.59081018e-01 6.28029144e-01\n",
      " 7.05480231e-01 7.92482898e-01 8.90215085e-01 1.00000000e+00]\n",
      "Number of thresholds: 100\n",
      "Processing group: 2.0\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "boolean index did not match indexed array along dimension 0; dimension is 16836 but corresponding boolean dimension is 30000",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[86], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m un_audit_results \u001b[38;5;241m=\u001b[39m \u001b[43mun_audit_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m un_audit_results:\n\u001b[0;32m      4\u001b[0m   \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMetric ID: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39mmetric_id\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Accuracy: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;241m.\u001b[39maccuracy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[80], line 171\u001b[0m, in \u001b[0;36mAudit.run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_objects)):\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf.metric_objects[i]: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_objects[i])\n\u001b[1;32m--> 171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m[\u001b[49m\n\u001b[0;32m    172\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpr_tolerances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_objects\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    174\u001b[0m \u001b[43m\u001b[49m\u001b[43m]\u001b[49m\n",
      "Cell \u001b[1;32mIn[80], line 172\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    169\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_objects)):\n\u001b[0;32m    170\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSelf.metric_objects[i]: \u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_objects[i])\n\u001b[0;32m    171\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m [\n\u001b[1;32m--> 172\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetric_objects\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_metric\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfpr_tolerances\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    173\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_objects))\n\u001b[0;32m    174\u001b[0m ]\n",
      "Cell \u001b[1;32mIn[84], line 163\u001b[0m, in \u001b[0;36mGroupPopulationMetric.run_metric\u001b[1;34m(self, fpr_tolerance_rate_list)\u001b[0m\n\u001b[0;32m    161\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m g \u001b[38;5;129;01min\u001b[39;00m np\u001b[38;5;241m.\u001b[39munique(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreference_groups):\n\u001b[0;32m    162\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessing group: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 163\u001b[0m     group_reference_signals \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreference_signals\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreference_groups\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mg\u001b[49m\u001b[43m]\u001b[49m\n\u001b[0;32m    164\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mReference signals for group \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mgroup_reference_signals\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    166\u001b[0m     thresholds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhypothesis_test_func(group_reference_signals, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mquantiles)\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[1;31mIndexError\u001b[0m: boolean index did not match indexed array along dimension 0; dimension is 16836 but corresponding boolean dimension is 30000"
     ]
    }
   ],
   "source": [
    "un_audit_results = un_audit_obj.run()[0]\n",
    "\n",
    "for result in un_audit_results:\n",
    "  print(f\"Metric ID: {result.metric_id}, Accuracy: {result.accuracy}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7213308c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
