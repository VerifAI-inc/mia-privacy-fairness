{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from fairlearn.datasets import fetch_acs_income\n",
    "from sklearn import preprocessing\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from fairlearn.metrics import equalized_odds_difference\n",
    "import numpy as np\n",
    "from privacy_meter.audit import Audit, MetricEnum\n",
    "from privacy_meter.dataset import Dataset\n",
    "from privacy_meter.information_source import InformationSource\n",
    "\n",
    "from privacy_meter import audit_report\n",
    "from privacy_meter.audit_report import *\n",
    "\n",
    "import keras as keras\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "# load dataset\n",
    "import pandas as pd\n",
    "from fairlearn.reductions import EqualizedOdds\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from fairlearn.metrics import MetricFrame\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from privacy_meter.audit import Audit, MetricEnum\n",
    "from privacy_meter.dataset import Dataset\n",
    "from privacy_meter.information_source import InformationSource\n",
    "from privacy_meter.model import Fairlearn_Model, Sklearn_Model\n",
    "import os\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from fairtorch import ConstraintLoss, DemographicParityLoss, EqualiedOddsLoss\n",
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from privacy_meter.model import PytorchModelTensor\n",
    "from privacy_meter.dataset import Dataset\n",
    "import scipy\n",
    "def compute_p_value( target_value, observed_distribution):\n",
    "    return scipy.stats.percentileofscore(observed_distribution, target_value)/100\n",
    "\n",
    "# import copy\n",
    "import time\n",
    "def privacy_evaluation(target_model, train_ds, test_ds, population_ds, g_train, g_test, g_population):\n",
    "    target_model = PytorchModelTensor(target_model, nn.BCEWithLogitsLoss(reduction='none'))\n",
    "    target_dataset = Dataset(\n",
    "        data_dict={'train': train_ds, 'test': test_ds},\n",
    "        default_input='x', default_output='y', default_group='g'\n",
    "    )\n",
    "    reference_dataset = Dataset(\n",
    "    data_dict={'train': population_ds},\n",
    "    default_input='x', default_output='y', default_group='g'\n",
    "    )\n",
    "    target_info_source = InformationSource(\n",
    "    models=[target_model], \n",
    "        datasets=[target_dataset]\n",
    "    )\n",
    "\n",
    "    reference_info_source = InformationSource(\n",
    "        models=[target_model],\n",
    "        datasets=[reference_dataset]\n",
    "    )\n",
    "    audit_obj = Audit(\n",
    "    metrics=MetricEnum.POPULATION,\n",
    "    inference_game_type=InferenceGame.PRIVACY_LOSS_MODEL,\n",
    "    target_info_sources=target_info_source,\n",
    "    reference_info_sources=reference_info_source,\n",
    "    logs_directory_names='log_'+str(time.time())\n",
    "    )\n",
    "    audit_obj.prepare()\n",
    "\n",
    "    train_signal = audit_obj.metric_objects[0].member_signals\n",
    "    test_signal = audit_obj.metric_objects[0].non_member_signals\n",
    "    pop_signal = audit_obj.metric_objects[0].reference_signals\n",
    "    \n",
    "\n",
    "    p_results = {}\n",
    "    for idx, g in enumerate(torch.unique(g_train)):\n",
    "        un_pop = pop_signal[g_population==g]\n",
    "        p_train = compute_p_value(train_signal[g_train==g], un_pop)\n",
    "        p_test = compute_p_value(test_signal[g_test==g], un_pop)\n",
    "        p_results[idx] = {}\n",
    "        p_results[idx]['p'] = np.concatenate([p_train, p_test])\n",
    "        p_results[idx]['mem'] = np.concatenate([np.ones(len(p_train)), np.zeros(len(p_test))])\n",
    "        \n",
    "    return p_results\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "\n",
    "\n",
    "seed_everything(2020)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, output_dim):\n",
    "        super(Model, self).__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.fc1(x)\n",
    "        out = self.relu1(out)\n",
    "        out = self.fc2(out)\n",
    "        out = self.relu2(out)\n",
    "        out = self.fc3(out)\n",
    "        return out\n",
    "    \n",
    "def get_data(N):\n",
    "    p00 = 0.02\n",
    "    p10 = 0.4\n",
    "    p11 = 0.4\n",
    "    p01 = 0.18\n",
    "\n",
    "    X00 = np.random.multivariate_normal([0, -1], [[7, 1], [1, 7]], int(N*p00))\n",
    "    X01 = np.random.multivariate_normal([-5, 0], [[5, 1], [1, 5]], int(N*p01))\n",
    "    X10 = np.random.multivariate_normal([1, 2], [[5, 2], [2, 5]], int(N*p10))\n",
    "    X11 = np.random.multivariate_normal([2, 3], [[10, 1], [1, 4]], int(N*p11))\n",
    "    X = np.concatenate((X00, X01, X10, X11), axis=0)\n",
    "    y_true = np.concatenate((np.zeros(int(N*p00)), np.ones(int(N*p01)), np.zeros(int(N*p10)), np.ones(int(N*p11))), axis=0)\n",
    "    sex = np.concatenate((np.ones(int(N*p00)), np.ones(int(N*p01)), np.full(int(N*p10), 2), np.full(int(N*p11), 2)), axis=0)\n",
    "    return X, y_true, sex\n",
    "\n",
    "def inference(model,X,y,sensi):\n",
    "    y_pred = (torch.sigmoid(model(X)).view(-1) > 0.5 ).float()\n",
    "\n",
    "    expect_by_a={}\n",
    "\n",
    "    # for i in [1, 2]:\n",
    "    #     # print(sensi_test.shape, y_pred.shape)\n",
    "    #     idx = sensi==i\n",
    "    #     expect_by_a[i]=  y_pred[idx].mean().item()\n",
    "    # print(expect_by_a)\n",
    "    # vanilla_gap_dp = np.abs(expect_by_a[1] - expect_by_a[2])\n",
    "    # # print(\"gap: \",vanilla_gap_dp)\n",
    "\n",
    "    expect_by_a_y = {}\n",
    "    expect_by_a_y_list = []\n",
    "    for i in [1, 2]:\n",
    "        expect_by_a_y[i]={}\n",
    "        for j in [0, 1]:\n",
    "            idx= (sensi == i) & (y == j)\n",
    "            expc = y_pred[idx].mean().item()\n",
    "            expect_by_a_y[i][j] =  expc\n",
    "            expect_by_a_y_list.append(expc)\n",
    "    # print(expect_by_a_y)\n",
    "    vanilla_gap_eo = np.max(expect_by_a_y_list)- np.min(expect_by_a_y_list)\n",
    "    print(\"gap: \", vanilla_gap_eo)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Load dataset \n",
    "# data = fetch_acs_income(as_frame=False,states=[\"CA\"])\n",
    "# sex = data.data[:,data['feature_names'].index('SEX')]\n",
    "# X = data.data\n",
    "# y_true = (data.target > 50000) * 1\n",
    "# X = preprocessing.normalize(X, norm='l2')\n",
    "\n",
    "# device = 'cuda:0'\n",
    "\n",
    "# num_train_points = 1000\n",
    "# num_test_points = 1000\n",
    "\n",
    "\n",
    "# num_fair_train_points = 10000\n",
    "# num_population_points = 30000\n",
    "\n",
    "# train_index = np.random.choice(X.shape[0], num_train_points, replace=False)\n",
    "# test_index = np.random.choice(X.shape[0], num_test_points, replace=False)\n",
    "# fair_train_index = np.random.choice(X.shape[0], num_fair_train_points, replace=False)\n",
    "# population_index = np.random.choice(X.shape[0], num_population_points, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda:0'\n",
    "num_train_points = 1000\n",
    "num_test_points = 1000\n",
    "num_population_points = 5000\n",
    "X_train,y_train, g_train = get_data(num_train_points)\n",
    "X_test ,y_test, g_test = get_data(num_test_points)\n",
    "X_population, y_population, g_population = get_data(num_population_points)\n",
    "X_train = torch.from_numpy(X_train).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "y_test = torch.from_numpy(y_test).float()\n",
    "sensi_train = torch.from_numpy(g_train).float()\n",
    "sensi_test = torch.from_numpy(g_test).float()\n",
    "# create the target model's dataset\n",
    "g_train  = y_train+ sensi_train*2\n",
    "g_test  = y_test+ sensi_test*2\n",
    "\n",
    "\n",
    "X_population = torch.from_numpy(X_population).float()\n",
    "y_population = torch.from_numpy(y_population).float()\n",
    "sensi_population = torch.from_numpy(g_population).float()\n",
    "g_population = y_population+ sensi_population*2\n",
    "\n",
    "\n",
    "\n",
    "# Create TensorDatasets\n",
    "train_dataset = TensorDataset(X_train, y_train, sensi_train)\n",
    "test_dataset = TensorDataset(X_test, y_test, sensi_test)\n",
    "\n",
    "# Create DataLoaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=300, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "# X = np.concatenate((X_train, X_test, X_fair_train, X_population), axis=0)\n",
    "# y_true = np.concatenate((y_train, y_test, y_fair_train, y_population), axis=0)\n",
    "# sex = np.concatenate((g_train, g_test, g_fair_train, g_population), axis=0)\n",
    "\n",
    "# train_index = np.arange(num_train_points)\n",
    "# test_index = np.arange(num_train_points, num_train_points+num_test_points)\n",
    "# fair_train_index = np.arange(num_train_points+num_test_points, num_train_points+num_test_points+num_fair_train_points)\n",
    "# population_index = np.arange(num_train_points+num_test_points+num_fair_train_points, num_train_points+num_test_points+num_fair_train_points+num_population_points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the data to tensors\n",
    "num_fair_train_points = 1000\n",
    "X_fair_train, y_fair_train, g_fair_train = get_data(num_fair_train_points)\n",
    "X_fair_train = torch.from_numpy(X_fair_train).float()\n",
    "y_fair_train = torch.from_numpy(y_fair_train).float()\n",
    "sensi_fair = torch.from_numpy(g_fair_train).float()\n",
    "g_fair_train = y_fair_train+ sensi_fair*2\n",
    "\n",
    "fair_train_dataset = TensorDataset(X_fair_train, y_fair_train, sensi_fair)\n",
    "\n",
    "fair_train_loader =  DataLoader(fair_train_dataset, batch_size=len(fair_train_dataset), shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs=1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(input_shape, alpha, coff, device, train_loader,X_train, y_train, X_test, y_test, num_epochs=1000,fair_train_loader=None):\n",
    "    model = Model(input_shape, 1024, 1)\n",
    "    if coff!=0:\n",
    "        dp_loss = EqualiedOddsLoss(sensitive_classes=[1, 2], alpha=alpha)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    model.to(device)\n",
    "    for name, param in model.named_parameters():\n",
    "        if 'weight' in name:\n",
    "            nn.init.xavier_normal_(param)\n",
    "    \n",
    "    model.train()\n",
    "    for epoch in range(num_epochs):\n",
    "        for i, (x, y, s) in enumerate(train_loader):\n",
    "            lossfair = 0\n",
    "            optimizer.zero_grad()  \n",
    "            x, y, s = x.to(device), y.to(device), s.to(device)\n",
    "            logit = model(x)\n",
    "            loss = criterion(logit.view(-1), y)\n",
    "            if coff!=0:\n",
    "                if fair_train_loader is not None:\n",
    "                    for (x_f, y_f, s_f) in fair_train_loader:\n",
    "                        # print(x_f.shape, y_f.shape, s_f.shape)\n",
    "                        x_f, y_f, s_f = x_f.to(device), y_f.to(device), s_f.to(device)\n",
    "                        logit_f = model(x_f)\n",
    "                        lossfair += dp_loss(x_f, logit_f, s_f, y_f)\n",
    "                else:\n",
    "                    lossfair = dp_loss(x, logit, s, y)\n",
    "                if torch.isnan(lossfair):\n",
    "                    loss = loss\n",
    "                else:\n",
    "                    loss = loss + lossfair\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Print progress\n",
    "            if (i+1) % 10 == 0:\n",
    "                print ('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'\n",
    "                        .format(epoch+1, num_epochs, i+1, len(train_loader), loss.item()))\n",
    "\n",
    "    model.to('cpu')\n",
    "    return model\n",
    "\n",
    "\n",
    "def inference(model,X,y,sensi):\n",
    "    y_pred = (torch.sigmoid(model(X)).view(-1) > 0.5 ).float()\n",
    "\n",
    "    expect_by_a={}\n",
    "\n",
    "    # for i in [1, 2]:\n",
    "    #     # print(sensi_test.shape, y_pred.shape)\n",
    "    #     idx = sensi==i\n",
    "    #     expect_by_a[i]=  y_pred[idx].mean().item()\n",
    "    # print(expect_by_a)\n",
    "    # vanilla_gap_dp = np.abs(expect_by_a[1] - expect_by_a[2])\n",
    "    # # print(\"gap: \",vanilla_gap_dp)\n",
    "\n",
    "    expect_by_a_y = {}\n",
    "    expect_by_a_y_list = []\n",
    "    for i in [1, 2]:\n",
    "        expect_by_a_y[i]={}\n",
    "        for j in [0, 1]:\n",
    "            idx= (sensi == i) & (y == j)\n",
    "            expc = y_pred[idx].mean().item()\n",
    "            expect_by_a_y[i][j] =  expc\n",
    "            expect_by_a_y_list.append(expc)\n",
    "    # print(expect_by_a_y)\n",
    "    vanilla_gap_eo = np.max(expect_by_a_y_list)- np.min(expect_by_a_y_list)\n",
    "    acc = ((torch.sigmoid(model(X)).view(-1) > 0.5 ).float()  == y ).float().mean().float().item()\n",
    "    # print(\"acc: \",round(acc,3))\n",
    "    \n",
    "    # print(\"gap: \", round(vanilla_gap_eo,3))\n",
    "    return round(acc,3), round(vanilla_gap_eo,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fair_model2 = train(X_train.shape[1], 50, 1, device, train_loader, X_train, y_train, X_test, y_test,num_epochs=num_epochs,fair_train_loader=fair_train_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = train(X_train.shape[1], 0, 0, device, train_loader, X_train, y_train, X_test, y_test,num_epochs=num_epochs,fair_train_loader=None)\n",
    "fair_model = train(X_train.shape[1], 50, 1, device, train_loader, X_train, y_train, X_test, y_test,num_epochs=num_epochs,fair_train_loader=None)\n",
    "fair_model2 = train(X_train.shape[1], 50, 1, device, train_loader, X_train, y_train, X_test, y_test,num_epochs=num_epochs,fair_train_loader=fair_train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "un_test_acc, un_test_gap = inference(model,X_test,y_test, sensi_test)\n",
    "fair_test_acc, fair_test_gap = inference(fair_model,X_test,y_test, sensi_test)\n",
    "fair_test_acc2, fair_test_gap2 = inference(fair_model2,X_test,y_test, sensi_test)\n",
    "\n",
    "un_train_acc, un_train_gap = inference(model,X_train,y_train, sensi_train)\n",
    "fair_train_acc, fair_train_gap = inference(fair_model,X_train,y_train, sensi_train)\n",
    "fair_train_acc2, fair_train_gap2 = inference(fair_model2,X_train,y_train, sensi_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8 | 0.649 | 0.632 | 0.489 |\n",
      "0.58 | 0.58 | 0.0 | 0.0 |\n",
      "0.702 | 0.62 | 0.433 | 0.452 |\n"
     ]
    }
   ],
   "source": [
    "print(un_train_acc, '|',un_test_acc, '|',un_train_gap, '|',un_test_gap,'|')\n",
    "print(fair_train_acc, '|',fair_test_acc, '|',fair_train_gap, '|',fair_test_gap,'|')\n",
    "print(fair_train_acc2, '|',fair_test_acc2, '|',fair_train_gap2, '|',fair_test_gap2,'|')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# attack fair models and models\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the target model's dataset\n",
    "train_ds = {'x': X_train, 'y': y_train.reshape(-1,1),'g':sensi_train}\n",
    "test_ds = {'x': X_test, 'y': y_test.reshape(-1,1), 'g':sensi_test}\n",
    "population_ds = {'x': X_population, 'y': y_population.reshape(-1,1), 'g': sensi_population}\n",
    "\n",
    "p_results = privacy_evaluation(model,train_ds, test_ds, population_ds, g_train, g_test, g_population)\n",
    "p_fair_results = privacy_evaluation(fair_model, train_ds, test_ds, population_ds, g_train, g_test, g_population)\n",
    "p_fair_results_2 = privacy_evaluation(fair_model2, train_ds, test_ds, population_ds, g_train, g_test, g_population)\n",
    "\n",
    "\n",
    "figs, axes = plt.subplots(1,4, figsize=(12,3), sharey=True, sharex=True)\n",
    "\n",
    "# group = np.unique(g_train)\n",
    "for idx, group in enumerate(np.unique(g_train)):\n",
    "    tpr = [np.mean(p_results[idx]['p'][p_results[idx]['mem']==1]<i) for i in np.linspace(0,1,100)]\n",
    "    fpr = [np.mean(p_results[idx]['p'][p_results[idx]['mem']==0]<i) for i in np.linspace(0,1,100)]\n",
    "\n",
    "    fair_tpr = [np.mean(p_fair_results[idx]['p'][p_fair_results[idx]['mem']==1]<i) for i in np.linspace(0,1,100)]\n",
    "    fair_fpr = [np.mean(p_fair_results[idx]['p'][p_fair_results[idx]['mem']==0]<i) for i in np.linspace(0,1,100)]\n",
    "\n",
    "    fair_tpr2 = [np.mean(p_fair_results_2[idx]['p'][p_fair_results_2[idx]['mem']==1]<i) for i in np.linspace(0,1,100)]\n",
    "    fair_fpr2 = [np.mean(p_fair_results_2[idx]['p'][p_fair_results_2[idx]['mem']==0]<i) for i in np.linspace(0,1,100)]\n",
    "\n",
    "\n",
    "    roc = np.trapz(tpr, fpr)\n",
    "    fair_roc = np.trapz(fair_tpr, fair_fpr)\n",
    "    print(idx, '|', round(max(np.array(tpr) + (1-np.array(fpr)))/2,3), '|',round(max(np.array(fair_tpr) + (1-np.array(fair_fpr)))/2,3), '|',round(max(np.array(fair_tpr2) + (1-np.array(fair_fpr2)))/2,3))\n",
    "    axes[idx].plot([0, 1], [0, 1], 'k--', label='Random Guess', color='grey')\n",
    "    axes[idx].plot(fpr, tpr, label=f'Unfair-ROC:{roc:.3f}', color='blue')\n",
    "    axes[idx].plot(fair_fpr, fair_tpr, label=f'Fair-ROC:{fair_roc:.3f}', ls='--', color='red')\n",
    "    axes[idx].plot(fair_fpr2, fair_tpr2, label=f'Our Fair-ROC:{fair_roc:.3f}', ls='--', color='green')\n",
    "    axes[idx].set_xlabel('False Positive Rate')\n",
    "    axes[idx].set_title(f'Subgroup {idx}')\n",
    "    axes[idx].legend()\n",
    "axes[0].set_ylabel('True Positive Rate')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "privacy_meter_test",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16 | packaged by conda-forge | (main, Feb  1 2023, 21:39:03) \n[GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "af48e0a30d2a0b619bf296aea7d7271ba14c9beb98b0928b3e23c82f12ae949a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
